<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>MOOC on Gejun&#39;s Blog</title>
    <link>https://gejun.name/categories/mooc/</link>
    <description>Recent content in MOOC on Gejun&#39;s Blog</description>
    <image>
      <title>Gejun&#39;s Blog</title>
      <url>https://gejun.name</url>
      <link>https://gejun.name</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gejun.name/categories/mooc/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bigram Character-level Language Model</title>
      <link>https://gejun.name/natural-language-processing/building-makemore/</link>
      <pubDate>Sat, 04 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://gejun.name/natural-language-processing/building-makemore/</guid>
      <description>This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.
In this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let&amp;rsquo;s prepare the data first.</description>
    </item>
    
  </channel>
</rss>
