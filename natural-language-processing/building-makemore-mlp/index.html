<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Multilayer Perceptron (MLP) | Gejun's Blog</title><meta name=keywords content="n-gram,pytorch,embedding"><meta name=description content="In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters."><meta name=author content="Gejun Zhu"><link rel=canonical href=https://gejun.name/natural-language-processing/building-makemore-mlp/><link crossorigin=anonymous href=/assets/css/stylesheet.07b504287c624e47e21667d48ef8c5c81574d5a3594c9eb68922bfeacb71823b.css integrity="sha256-B7UEKHxiTkfiFmfUjvjFyBV01aNZTJ62iSK/6stxgjs=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://gejun.name/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gejun.name/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gejun.name/favicon-32x32.png><link rel=apple-touch-icon href=https://gejun.name/apple-touch-icon.png><link rel=mask-icon href=https://gejun.name/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BEP7FKQVEG"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BEP7FKQVEG")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="Multilayer Perceptron (MLP)"><meta property="og:description" content="In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters."><meta property="og:type" content="article"><meta property="og:url" content="https://gejun.name/natural-language-processing/building-makemore-mlp/"><meta property="og:image" content="https://gejun.name/natural-language-processing/building-makemore-mlp/mlp-feature-vector.png"><meta property="article:section" content="natural-language-processing"><meta property="article:published_time" content="2023-03-13T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-13T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gejun.name/natural-language-processing/building-makemore-mlp/mlp-feature-vector.png"><meta name=twitter:title content="Multilayer Perceptron (MLP)"><meta name=twitter:description content="In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Natural-language-processings","item":"https://gejun.name/natural-language-processing/"},{"@type":"ListItem","position":3,"name":"Multilayer Perceptron (MLP)","item":"https://gejun.name/natural-language-processing/building-makemore-mlp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Multilayer Perceptron (MLP)","name":"Multilayer Perceptron (MLP)","description":"In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters.","keywords":["n-gram","pytorch","embedding"],"articleBody":"In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters. This lack of context can lead to poor performance of bigram models. In this lecture, Andrej shows us how to build a multilayer neural network to improve the model performance.\nUnlike the bigram model we built in the last lecture, our new mode is a multilayer perceptron (MLP) that takes the previous 2 characters to predict the probabilities of the next character. This MLP language model was proposed in the paper A Neural Probabilistic Language Model by Bengio et al. in 2003. As always, the official Jupyter Notebook for this part is here.\nData Preparation First, we build our vocabulary as we did before.\nfrom collections import Counter import torch import string import matplotlib.pyplot as plt import torch.nn.functional as F words = open(\"names.txt\", \"r\").read().splitlines() chars = string.ascii_lowercase stoi = {s: i+1 for i, s in enumerate(chars)} stoi[\".\"] = 0 itos = {i: s for s, i in stoi.items()} print(stoi, itos) {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'} Next, we create the training data. This time, we use the last 2 characters, instead of 1, to predict the next character, which is a 3-gram or trigram model.\nblock_size = 3 X, y = [], [] for word in words: # initialize context context = [0] * block_size for char in word + \".\": idx = stoi[char] X.append(context) y.append(idx) # truncate the first char and add the new char context = context[1:] + [idx] X = torch.tensor(X) y = torch.tensor(y) print(X.shape, y.shape) torch.Size([228146, 3]) torch.Size([228146]) Multilayer Perceptron (MLP) As stated in the name, our neural network model will have multiple hidden layers. Besides this, we will also learn a new way to represent characters.\nFeature Vector The paper proposed that each word would be associated with a feature vector which can be learned as training progresses. In other words, we use feature vectors to represent words in a language model. The number of features or the length of the vector is much smaller than the size of the vocabulary. Since the size of our vocabulary is 27, we will use a vector of length of 2 for now. This feature vector can be considered as word embedding nowadays.\ng = torch.Generator().manual_seed(42) # initialize lookup table C = torch.randn((27, 2), generator=g) print(f\"Vector representation for character a is: {C[stoi['a']]}\") Vector representation for character a is: tensor([ 0.9007, -2.1055]) The code above initializes our lookup table with $27\\times 2$ random numbers using torch.randn function. As we can see that the vector representation for the character a is [ 0.9007, -2.1055].\nNext, we are going to replace the indices in matrix X with vector representations. Since multiplying a one-hot encoding vector having a 1 at index i with the weight matrix W is the same as getting the ith row of W, we will extract the ith row from the embedding matrix directly instead of multiplying one-hot encoding with it.\nAccording to the tensor indexing documentation of PyTorch, we can extract corresponding feature vectors by treating X as an indexing matrix.\nembed = C[X] print(f\"First row of X: {X[0, :]}\") print(f\"First row of embedding: {embed[0,:]}\") print(embed.shape) First row of X: tensor([0, 0, 0]) First row of embedding: tensor([[1.9269, 1.4873], [1.9269, 1.4873], [1.9269, 1.4873]]) torch.Size([228146, 3, 2]) To put it in another way, we transform the matrix X of $228146\\times 3$ to the embedding matrix embed of $228146\\times 3 \\times 2$ because all the indices have been replaced with a vector of $1\\times 2$.\nModel Architecture As highlighted in the picture above, we should have a vector for each trigram after extracting its feature from the lookup table. This way, we can do matrix multiplication like before. However, we have a $3\\times 2$ matrix for each trigram instead. So we need to concatenate all the rows of the matrix into one vector. We can use torch.cat to concatenate the second dimension together, but PyTorch has a more efficient way, the view function(doc), to do so. See this blog post for more details about tensor and PyTorch internals.\nprint(embed[0, :]) print(embed.view(-1, 6)[0, :]) tensor([[1.9269, 1.4873], [1.9269, 1.4873], [1.9269, 1.4873]]) tensor([1.9269, 1.4873, 1.9269, 1.4873, 1.9269, 1.4873]) Building Model Next, we are going to initialize the weights and biases of our first and second hidden layers. Since the input dimension of our first layer is 6 and the number of neurons is 100, we initialize the weight matrix of shape $6\\times 100$ and the bias vector of length 100. The same rule applies to the second layer. The second layer’s input dimension is the first layer’s output dimension, 100. Because the output of the second layer is the probability of all 27 characters, we initialize the weight matrix of shape $100\\times 27$ with the bias vector of length 27.\n# 1st hidden layer W1 = torch.randn((6, 100)) b1 = torch.randn(100) # output for 1st layer h = embed.view(-1, 6) @ W1 + b1 # 2nd hidden layer W2 = torch.randn((100, 27)) b2 = torch.randn(27) # output for 2nd layer logits = h @ W2 + b2 Making Predictions The next step is our first forward pass to obtain the probabilities of the next characters.\n# softmax counts = logits.exp() probs = counts / counts.sum(1, keepdims=True) loss = -probs[torch.arange(X.shape[0]), y].log().mean() print(f\"Overall loss: {loss:.6f}\") Overall loss: nan However, as Andrej mentioned in the video, there is a potential issue with calculating the softmax function traditionally, as we saw above. If the output logits contain a large value, such as 100, applying the exponential function can result in nan values. Therefore, a better way to calculate the loss is to use the built-in cross_entropy function instead.\nloss = F.cross_entropy(logits, y) print(f\"Overall loss: {loss:.6f}\") Overall loss: 78.392731 Put Everything Together Here is the code after we put everything together and enabled backward pass.\ng = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\") Total parameters: 3481 The total number of learnable parameters of our model is 3482. Next, we are going to run the model for 10 epochs and see how loss changes. Notice that we apply the activation function tanh as described in the paper in the code below.\nfor _ in range(10): # forward pass embed = C[X] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y) print(f\"Loss: {loss.item()}\") # backward pass for p in parameters: p.grad = None loss.backward() # update weights lr = 0.1 for p in parameters: p.data += -lr * p.grad Loss: 16.72646713256836 Loss: 14.942943572998047 Loss: 13.863017082214355 Loss: 13.003837585449219 Loss: 12.292213439941406 Loss: 11.732643127441406 Loss: 11.270574569702148 Loss: 10.859720230102539 Loss: 10.479723930358887 Loss: 10.136445045471191 The model’s loss decreases as expected. However, you will notice that the loss comes out slower if you have a larger model with much more parameters. Why? Because we are using the whole dataset as a batch to calculate the loss and update the weights accordingly. In Stochastic Gradient Descent (SGD), the model parameters are updated based on the gradient of the loss function with respect to a randomly selected subset of the training data. Moreover, we can apply this idea to accelerate the training process.\nApplying Mini-batch We pick 32 as the mini-batch size, and the model runs very fast for 1000 epochs.\nfor i in range(1000): # batch_size = 32 idx = torch.randint(0, X.shape[0], (32, )) # forward pass embed = C[X[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 # using the whole dataset as a batch loss = F.cross_entropy(logits, y[idx]) if i % 50 == 0: print(f\"Loss: {loss.item()}\") # backward pass for p in parameters: p.grad = None loss.backward() # update weights lr = 0.1 for p in parameters: p.data += -lr * p.grad embed = C[X] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y) print(f\"Overall loss: {loss.item()}\") Loss: 10.522725105285645 Loss: 4.547809600830078 Loss: 3.9053943157196045 Loss: 3.5418882369995117 Loss: 3.312927722930908 Loss: 3.10072660446167 Loss: 3.188538074493408 Loss: 2.6955881118774414 Loss: 2.9730937480926514 Loss: 2.5453033447265625 Loss: 3.034700870513916 Loss: 2.2029476165771484 Loss: 2.5462143421173096 Loss: 2.6591145992279053 Loss: 2.9640085697174072 Loss: 3.142090082168579 Loss: 2.5031352043151855 Loss: 2.721736431121826 Loss: 2.7801644802093506 Loss: 2.32700252532959 Overall loss: 2.6130335330963135 Learning Rate Selection So how can we determine a suitable learning rate? In our previous training processes, we used a fixed learning rate of 0.1, but how can we know that 0.1 is optimal? Next, we are going to do some experiments to explore how to choose a good learning rate.\ng = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True # logarithm learning rate, base 10 lre = torch.linspace(-3, 0, 1000) # learning rates lrs = 10 ** lre losses = [] for i in range(1000): idx = torch.randint(0, X.shape[0], (32, )) embed = C[X[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y[idx]) if i % 50 == 0: print(f\"Loss: {loss.item()}\") for p in parameters: p.grad = None loss.backward() lr = lrs[i] for p in parameters: p.data += -lr * p.grad losses.append(loss.item()) plt.plot(lre, losses) Loss: 17.930665969848633 Loss: 15.90300464630127 Loss: 14.608807563781738 Loss: 11.146048545837402 Loss: 14.368053436279297 Loss: 10.241884231567383 Loss: 10.7547607421875 Loss: 9.06742000579834 Loss: 6.721671104431152 Loss: 4.959266185760498 Loss: 7.631305694580078 Loss: 6.03385591506958 Loss: 3.6078100204467773 Loss: 3.7624008655548096 Loss: 2.994145393371582 Loss: 2.6852164268493652 Loss: 3.392582893371582 Loss: 3.5405192375183105 Loss: 4.318221569061279 Loss: 5.7273149490356445 Figure 1: A plot for loss on different logarithm of learing rates According to the plot in Figure 1, the optimal logarithmic learning rate is around -1.0, which makes the learning rate 0.1.\nLearning Rate Decay As the training progresses, the loss could encounter a plateau, meaning that it stops decreasing even though the training process is still ongoing. To overcome this, learning rate decay can be applied, which decreases the learning rate over time as the training progresses. The model can escape from plateaus and continue improving its performance.\ng = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True losses = [] epochs = 20000 for i in range(epochs): idx = torch.randint(0, X.shape[0], (32, )) embed = C[X[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y[idx]) for p in parameters: p.grad = None loss.backward() # learning rate decay lr = 0.1 if i \u003c epochs // 2 else 0.001 for p in parameters: p.data += -lr * p.grad losses.append(loss.item()) plt.plot(range(epochs), losses) Train, Validation, and Test Evaluating the model performance on unseen data is important to make sure it generalizes well. It is common practice to split the training data into three parts: 80% for training, 10% for validation, and 10% for testing. The validation set could also be used for early stopping, which means stopping the training process when the performance on the validation set starts to degrade, preventing the model from overfitting to the training set.\ndef build_dataset(words, block_size=3): X, Y = [], [] for w in words: context = [0] * block_size for char in w + \".\": idx = stoi[char] X.append(context) Y.append(idx) context = context[1:] + [idx] X = torch.tensor(X) Y = torch.tensor(Y) print(X.shape, Y.shape) return X, Y import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words)) X_tr, y_tr = build_dataset(words[:n1]) X_va, y_va = build_dataset(words[n1:n2]) X_te, y_te = build_dataset(words[n2:]) g = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True tr_losses = [] va_losses = [] epochs = 20000 for i in range(epochs): idx = torch.randint(0, X_tr.shape[0], (32, )) embed = C[X_tr[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y_tr[idx]) for p in parameters: p.grad = None loss.backward() # learning rate decay lr = 0.1 if i \u003c epochs // 2 else 0.01 for p in parameters: p.data += -lr * p.grad tr_losses.append(loss.item()) val_embed = C[X_va] val_h = torch.tanh(val_embed.view(-1, 6) @ W1 + b1) val_logits = val_h @ W2 + b2 val_loss = F.cross_entropy(val_logits, y_va) va_losses.append(val_loss.item()) plt.plot(range(epochs), tr_losses, label='Training Loss') plt.plot(range(epochs), va_losses, label='Validation Loss') plt.title('Training and Validation Loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend(loc='best') plt.show() torch.Size([182625, 3]) torch.Size([182625]) torch.Size([22655, 3]) torch.Size([22655]) torch.Size([22866, 3]) torch.Size([22866]) Figure 2: Plot for training and validation loss As shown in Figure 2, there is a tiny drop in the validation loss at 10000 epochs, which indicates that our training did encounter a plateau and learning rate decay works very well. Let’s check the loss of the testing data.\ntest_embed = C[X_te] test_h = torch.tanh(test_embed.view(-1, 6) @ W1 + b1) test_logits = test_h @ W2 + b2 test_loss = F.cross_entropy(test_logits, y_te) print(f\"Loss on validation data: {val_loss:.6f}\") print(f\"Loss on testing data: {test_loss:.6f}\") Loss on validation data: 2.375811 Loss on testing data: 2.374066 The losses on validation and testing data are close, indicating we are not overfitting.\nVisualization of Embedding Let’s visualize our embedding matrix.\nplt.figure(figsize=(8,8)) plt.scatter(C[:, 0].data, C[:, 1].data, s = 200) for i in range(C.shape[0]): plt.text(C[i, 0].item(), C[i, 1].item(), itos[i], ha=\"center\", va=\"center\", color=\"white\") plt.grid('minor') plt.show() Figure 3: Visualization of 2D embedding matrix As depicted in Figure 3, the vowels are closely grouped in the left bottom corner of the plot, while the . is situated far away in the top right corner.\nWord Generation The last thing we want to do is word generation.\ng = torch.Generator().manual_seed(420) for _ in range(20): out = [] context = [0] * block_size while True: embed = C[torch.tensor([context])] h = torch.tanh(embed.view(1, -1) @ W1 + b1) logits = h @ W2 + b2 probs = F.softmax(logits, dim=1) idx = torch.multinomial(probs, num_samples=1, generator=g).item() context = context[1:] + [idx] if idx == 0: break out.append(idx) print(''.join(itos[i] for i in out)) rai mal lemistani iua kacyt tan zatlixahnen rarbi zethanli blie mozien nar ameson xaxun koma aedh sarixstah elin dyannili saom The words generated by the multilayer perceptron model make more sense than those from our last model. Still, there are many other ways to improve model performance. For example, train more epochs with learning rate decay, increase the batch size to make the training more stable, and add more data.\n","wordCount":"2600","inLanguage":"en","datePublished":"2023-03-13T00:00:00Z","dateModified":"2023-03-13T00:00:00Z","author":{"@type":"Person","name":"Gejun Zhu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://gejun.name/natural-language-processing/building-makemore-mlp/"},"publisher":{"@type":"Organization","name":"Gejun's Blog","logo":{"@type":"ImageObject","url":"https://gejun.name/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gejun.name accesskey=h title="Gejun's Blog (Alt + H)">Gejun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gejun.name/archives title=Archive><span>Archive</span></a></li><li><a href=https://gejun.name/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://gejun.name/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://gejun.name/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gejun.name>Home</a>&nbsp;»&nbsp;<a href=https://gejun.name/natural-language-processing/>Natural-language-processings</a></div><h1 class=post-title>Multilayer Perceptron (MLP)</h1><div class=post-meta><span title='2023-03-13 00:00:00 +0000 UTC'>March 13, 2023</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Gejun Zhu&nbsp;|&nbsp;<a href=https://github.com/zhugejun/zhugejun.github.io/tree/main/content/natural-language-processing/building-makemore-mlp/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#data-preparation aria-label="Data Preparation">Data Preparation</a></li><li><a href=#multilayer-perceptron-mlp aria-label="Multilayer Perceptron (MLP)">Multilayer Perceptron (MLP)</a><ul><li><a href=#feature-vector aria-label="Feature Vector">Feature Vector</a></li><li><a href=#model-architecture aria-label="Model Architecture">Model Architecture</a></li><li><a href=#building-model aria-label="Building Model">Building Model</a></li><li><a href=#making-predictions aria-label="Making Predictions">Making Predictions</a></li></ul></li><li><a href=#put-everything-together aria-label="Put Everything Together">Put Everything Together</a><ul><li><a href=#applying-mini-batch aria-label="Applying Mini-batch">Applying Mini-batch</a></li><li><a href=#learning-rate-selection aria-label="Learning Rate Selection">Learning Rate Selection</a></li><li><a href=#learning-rate-decay aria-label="Learning Rate Decay">Learning Rate Decay</a></li><li><a href=#train-validation-and-test aria-label="Train, Validation, and Test">Train, Validation, and Test</a></li></ul></li><li><a href=#visualization-of-embedding aria-label="Visualization of Embedding">Visualization of Embedding</a></li><li><a href=#word-generation aria-label="Word Generation">Word Generation</a></li></ul></div></details></div><div class=post-content><p>In <a href=https://gejun.name/natural-language-processing/building-makemore/>Part1</a>, we learned how to build a neural network with one hidden layer to generate words.
The model we built performed fairly well as we got the exact words generated based on counting.
However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character.
Suppose there is only one bigram starting with a particular character.
In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters.
This lack of context can lead to poor performance of bigram models.
In this lecture, Andrej shows us how to build a multilayer neural network to improve the model performance.</p><p>Unlike the bigram model we built in the last lecture, our new mode is a multilayer perceptron (MLP) that takes the previous 2 characters to predict the probabilities of the next character.
This MLP language model was proposed in the paper <a href=https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf>A Neural Probabilistic Language Model</a> by Bengio et al. in 2003.
As always, the official Jupyter Notebook for this part is <a href=https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb>here</a>.</p><h2 id=data-preparation>Data Preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h2><p>First, we build our vocabulary as we did before.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>Counter</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>string</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>words</span> <span class=o>=</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;names.txt&#34;</span><span class=p>,</span> <span class=s2>&#34;r&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>read</span><span class=p>()</span><span class=o>.</span><span class=n>splitlines</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>chars</span> <span class=o>=</span> <span class=n>string</span><span class=o>.</span><span class=n>ascii_lowercase</span>
</span></span><span class=line><span class=cl><span class=n>stoi</span> <span class=o>=</span> <span class=p>{</span><span class=n>s</span><span class=p>:</span> <span class=n>i</span><span class=o>+</span><span class=mi>1</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>s</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>chars</span><span class=p>)}</span>
</span></span><span class=line><span class=cl><span class=n>stoi</span><span class=p>[</span><span class=s2>&#34;.&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>itos</span> <span class=o>=</span> <span class=p>{</span><span class=n>i</span><span class=p>:</span> <span class=n>s</span> <span class=k>for</span> <span class=n>s</span><span class=p>,</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>stoi</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>stoi</span><span class=p>,</span> <span class=n>itos</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}
</code></pre><p>Next, we create the training data.
This time, we use the last 2 characters, instead of 1, to predict the next character, which is a 3-gram or trigram model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>block_size</span> <span class=o>=</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>words</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=c1># initialize context</span>
</span></span><span class=line><span class=cl>  <span class=n>context</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=n>block_size</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>char</span> <span class=ow>in</span> <span class=n>word</span> <span class=o>+</span> <span class=s2>&#34;.&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>idx</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>char</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># truncate the first char and add the new char</span>
</span></span><span class=line><span class=cl>    <span class=n>context</span> <span class=o>=</span> <span class=n>context</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span> <span class=o>+</span> <span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>torch.Size([228146, 3])
torch.Size([228146])
</code></pre><h2 id=multilayer-perceptron-mlp>Multilayer Perceptron (MLP)<a hidden class=anchor aria-hidden=true href=#multilayer-perceptron-mlp>#</a></h2><p>As stated in the name, our neural network model will have multiple hidden layers.
Besides this, we will also learn a new way to represent characters.</p><h3 id=feature-vector>Feature Vector<a hidden class=anchor aria-hidden=true href=#feature-vector>#</a></h3><p><img loading=lazy src=mlp-feature-vector.png alt="feature vector idea"></p><p>The paper proposed that each word would be associated with a feature vector which can be learned as training progresses.
In other words, we use feature vectors to represent words in a language model.
The number of features or the length of the vector is much smaller than the size of the vocabulary.
Since the size of our vocabulary is 27, we will use a vector of length of 2 for now.
This feature vector can be considered as word embedding nowadays.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Generator</span><span class=p>()</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># initialize lookup table</span>
</span></span><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>27</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Vector representation for character a is: </span><span class=si>{</span><span class=n>C</span><span class=p>[</span><span class=n>stoi</span><span class=p>[</span><span class=s1>&#39;a&#39;</span><span class=p>]]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Vector representation for character a is: tensor([ 0.9007, -2.1055])
</code></pre><p>The code above initializes our lookup table with $27\times 2$ random numbers using <code>torch.randn</code> function.
As we can see that the vector representation for the character <code>a</code> is <code>[ 0.9007, -2.1055]</code>.</p><p>Next, we are going to replace the indices in matrix <code>X</code> with vector representations.
Since multiplying a one-hot encoding vector having a 1 at index <code>i</code> with the weight matrix <code>W</code> is the same as getting the <code>ith</code> row of <code>W</code>, we will extract the <code>ith</code> row from the embedding matrix directly instead of multiplying one-hot encoding with it.</p><p><img loading=lazy src=matrix-multiplication.png alt="matrix multiplication"></p><p>According to the <a href=https://pytorch.org/cppdocs/notes/tensor_indexing.html#getter>tensor indexing documentation</a> of PyTorch, we can extract corresponding feature vectors by treating <code>X</code> as an indexing matrix.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>X</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;First row of X: </span><span class=si>{</span><span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=p>:]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;First row of embedding: </span><span class=si>{</span><span class=n>embed</span><span class=p>[</span><span class=mi>0</span><span class=p>,:]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>First row of X: tensor([0, 0, 0])
First row of embedding: tensor([[1.9269, 1.4873],
        [1.9269, 1.4873],
        [1.9269, 1.4873]])
torch.Size([228146, 3, 2])
</code></pre><p>To put it in another way, we transform the matrix <code>X</code> of $228146\times 3$ to the embedding matrix <code>embed</code> of $228146\times 3 \times 2$ because all the indices have been replaced with a vector of $1\times 2$.</p><h3 id=model-architecture>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h3><p><img loading=lazy src=mlp-architecture.png alt="MLP architecture"></p><p>As highlighted in the picture above, we should have a vector for each trigram after extracting its feature from the lookup table.
This way, we can do matrix multiplication like before.
However, we have a $3\times 2$ matrix for each trigram instead.
So we need to concatenate all the rows of the matrix into one vector.
We can use <code>torch.cat</code> to concatenate the second dimension together, but PyTorch has a more efficient way, the <code>view</code> function(<a href=https://pytorch.org/docs/stable/generated/torch.Tensor.view.html>doc</a>), to do so.
See this <a href=http://blog.ezyang.com/2019/05/pytorch-internals/>blog post</a> for more details about tensor and PyTorch internals.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>embed</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=p>:])</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)[</span><span class=mi>0</span><span class=p>,</span> <span class=p>:])</span>
</span></span></code></pre></div><pre><code>tensor([[1.9269, 1.4873],
        [1.9269, 1.4873],
        [1.9269, 1.4873]])
tensor([1.9269, 1.4873, 1.9269, 1.4873, 1.9269, 1.4873])
</code></pre><h3 id=building-model>Building Model<a hidden class=anchor aria-hidden=true href=#building-model>#</a></h3><p>Next, we are going to initialize the weights and biases of our first and second hidden layers.
Since the input dimension of our first layer is 6 and the number of neurons is 100, we initialize the weight matrix of shape $6\times 100$ and the bias vector of length 100.
The same rule applies to the second layer.
The second layer&rsquo;s input dimension is the first layer&rsquo;s output dimension, 100.
Because the output of the second layer is the probability of all 27 characters, we initialize the weight matrix of shape $100\times 27$ with the bias vector of length 27.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># 1st hidden layer</span>
</span></span><span class=line><span class=cl><span class=n>W1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>6</span><span class=p>,</span> <span class=mi>100</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>b1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># output for 1st layer</span>
</span></span><span class=line><span class=cl><span class=n>h</span> <span class=o>=</span> <span class=n>embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2nd hidden layer</span>
</span></span><span class=line><span class=cl><span class=n>W2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>100</span><span class=p>,</span> <span class=mi>27</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>b2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>27</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># output for 2nd layer</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span></code></pre></div><h3 id=making-predictions>Making Predictions<a hidden class=anchor aria-hidden=true href=#making-predictions>#</a></h3><p>The next step is our first forward pass to obtain the probabilities of the next characters.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># softmax</span>
</span></span><span class=line><span class=cl><span class=n>counts</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>probs</span> <span class=o>=</span> <span class=n>counts</span> <span class=o>/</span> <span class=n>counts</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>probs</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=n>y</span><span class=p>]</span><span class=o>.</span><span class=n>log</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Overall loss: </span><span class=si>{</span><span class=n>loss</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Overall loss: nan
</code></pre><p>However, as Andrej mentioned in the video, there is a potential issue with calculating the softmax function traditionally, as we saw above.
If the output logits contain a large value, such as 100, applying the exponential function can result in <code>nan</code> values.
Therefore, a better way to calculate the loss is to use the built-in <code>cross_entropy</code> function instead.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span> 
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Overall loss: </span><span class=si>{</span><span class=n>loss</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Overall loss: 78.392731
</code></pre><h2 id=put-everything-together>Put Everything Together<a hidden class=anchor aria-hidden=true href=#put-everything-together>#</a></h2><p>Here is the code after we put everything together and enabled backward pass.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Generator</span><span class=p>()</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>27</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>6</span><span class=p>,</span> <span class=mi>100</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>100</span><span class=p>,</span> <span class=mi>27</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>27</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>parameters</span> <span class=o>=</span> <span class=p>[</span><span class=n>C</span><span class=p>,</span> <span class=n>W1</span><span class=p>,</span> <span class=n>b1</span><span class=p>,</span> <span class=n>W2</span><span class=p>,</span> <span class=n>b2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Total parameters: </span><span class=si>{</span><span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>nelement</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Total parameters: 3481
</code></pre><p>The total number of learnable parameters of our model is 3482.
Next, we are going to run the model for 10 epochs and see how loss changes.
Notice that we apply the activation function <code>tanh</code> as described in the paper in the code below.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>10</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># forward pass</span>
</span></span><span class=line><span class=cl>  <span class=n>embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>X</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>logits</span> <span class=o>=</span> <span class=n>h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># backward pass</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># update weights</span>
</span></span><span class=line><span class=cl>  <span class=n>lr</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>data</span> <span class=o>+=</span> <span class=o>-</span><span class=n>lr</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span>
</span></span></code></pre></div><pre><code>Loss: 16.72646713256836
Loss: 14.942943572998047
Loss: 13.863017082214355
Loss: 13.003837585449219
Loss: 12.292213439941406
Loss: 11.732643127441406
Loss: 11.270574569702148
Loss: 10.859720230102539
Loss: 10.479723930358887
Loss: 10.136445045471191
</code></pre><p>The model&rsquo;s loss decreases as expected.
However, you will notice that the loss comes out slower if you have a larger model with much more parameters.
Why? Because we are using the whole dataset as a batch to calculate the loss and update the weights accordingly.
In <a href=https://www.wikiwand.com/en/Stochastic_gradient_descent>Stochastic Gradient Descent (SGD)</a>, the model parameters are updated based on the gradient of the loss function with respect to a randomly selected subset of the training data.
Moreover, we can apply this idea to accelerate the training process.</p><h3 id=applying-mini-batch>Applying Mini-batch<a hidden class=anchor aria-hidden=true href=#applying-mini-batch>#</a></h3><p>We pick 32 as the mini-batch size, and the model runs very fast for 1000 epochs.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=c1># batch_size = 32</span>
</span></span><span class=line><span class=cl>  <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># forward pass</span>
</span></span><span class=line><span class=cl>  <span class=n>embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>X</span><span class=p>[</span><span class=n>idx</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>  <span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>logits</span> <span class=o>=</span> <span class=n>h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>  <span class=c1># using the whole dataset as a batch</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>i</span> <span class=o>%</span> <span class=mi>50</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># backward pass</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># update weights</span>
</span></span><span class=line><span class=cl>  <span class=n>lr</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>data</span> <span class=o>+=</span> <span class=o>-</span><span class=n>lr</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>X</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Overall loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Loss: 10.522725105285645
Loss: 4.547809600830078
Loss: 3.9053943157196045
Loss: 3.5418882369995117
Loss: 3.312927722930908
Loss: 3.10072660446167
Loss: 3.188538074493408
Loss: 2.6955881118774414
Loss: 2.9730937480926514
Loss: 2.5453033447265625
Loss: 3.034700870513916
Loss: 2.2029476165771484
Loss: 2.5462143421173096
Loss: 2.6591145992279053
Loss: 2.9640085697174072
Loss: 3.142090082168579
Loss: 2.5031352043151855
Loss: 2.721736431121826
Loss: 2.7801644802093506
Loss: 2.32700252532959
Overall loss: 2.6130335330963135
</code></pre><h3 id=learning-rate-selection>Learning Rate Selection<a hidden class=anchor aria-hidden=true href=#learning-rate-selection>#</a></h3><p>So how can we determine a suitable learning rate?
In our previous training processes, we used a fixed learning rate of 0.1, but how can we know that 0.1 is optimal?
Next, we are going to do some experiments to explore how to choose a good learning rate.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Generator</span><span class=p>()</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>27</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>6</span><span class=p>,</span> <span class=mi>100</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>100</span><span class=p>,</span> <span class=mi>27</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>27</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>parameters</span> <span class=o>=</span> <span class=p>[</span><span class=n>C</span><span class=p>,</span> <span class=n>W1</span><span class=p>,</span> <span class=n>b1</span><span class=p>,</span> <span class=n>W2</span><span class=p>,</span> <span class=n>b2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># logarithm learning rate, base 10</span>
</span></span><span class=line><span class=cl><span class=n>lre</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>3</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1000</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># learning rates</span>
</span></span><span class=line><span class=cl><span class=n>lrs</span> <span class=o>=</span> <span class=mi>10</span> <span class=o>**</span> <span class=n>lre</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=n>embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>X</span><span class=p>[</span><span class=n>idx</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>  <span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>logits</span> <span class=o>=</span> <span class=n>h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>i</span> <span class=o>%</span> <span class=mi>50</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>lr</span> <span class=o>=</span> <span class=n>lrs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>data</span> <span class=o>+=</span> <span class=o>-</span><span class=n>lr</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>  <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>lre</span><span class=p>,</span> <span class=n>losses</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Loss: 17.930665969848633
Loss: 15.90300464630127
Loss: 14.608807563781738
Loss: 11.146048545837402
Loss: 14.368053436279297
Loss: 10.241884231567383
Loss: 10.7547607421875
Loss: 9.06742000579834
Loss: 6.721671104431152
Loss: 4.959266185760498
Loss: 7.631305694580078
Loss: 6.03385591506958
Loss: 3.6078100204467773
Loss: 3.7624008655548096
Loss: 2.994145393371582
Loss: 2.6852164268493652
Loss: 3.392582893371582
Loss: 3.5405192375183105
Loss: 4.318221569061279
Loss: 5.7273149490356445
</code></pre><figure><img src=index_files/figure-markdown_strict/fig-loss-vs-lr-output-4.png id=fig-loss-vs-lr width=653 height=411 alt="Figure 1: A plot for loss on different logarithm of learing rates"><figcaption aria-hidden=true>Figure 1: A plot for loss on different logarithm of learing rates</figcaption></figure><p>According to the plot in <a href=#fig-loss-vs-lr>Figure 1</a>, the optimal logarithmic learning rate is around -1.0, which makes the learning rate 0.1.</p><h3 id=learning-rate-decay>Learning Rate Decay<a hidden class=anchor aria-hidden=true href=#learning-rate-decay>#</a></h3><p>As the training progresses, the loss could encounter a plateau, meaning that it stops decreasing even though the training process is still ongoing.
To overcome this, learning rate decay can be applied, which decreases the learning rate over time as the training progresses.
The model can escape from plateaus and continue improving its performance.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Generator</span><span class=p>()</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>27</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>6</span><span class=p>,</span> <span class=mi>100</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>100</span><span class=p>,</span> <span class=mi>27</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>27</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>parameters</span> <span class=o>=</span> <span class=p>[</span><span class=n>C</span><span class=p>,</span> <span class=n>W1</span><span class=p>,</span> <span class=n>b1</span><span class=p>,</span> <span class=n>W2</span><span class=p>,</span> <span class=n>b2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>20000</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=n>embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>X</span><span class=p>[</span><span class=n>idx</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>  <span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>logits</span> <span class=o>=</span> <span class=n>h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># learning rate decay</span>
</span></span><span class=line><span class=cl>  <span class=n>lr</span> <span class=o>=</span> <span class=mf>0.1</span> <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>epochs</span> <span class=o>//</span> <span class=mi>2</span> <span class=k>else</span> <span class=mf>0.001</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>data</span> <span class=o>+=</span> <span class=o>-</span><span class=n>lr</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>  <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>),</span> <span class=n>losses</span><span class=p>)</span>
</span></span></code></pre></div><img src=index_files/figure-markdown_strict/cell-14-output-1.png width=641 height=411><h3 id=train-validation-and-test>Train, Validation, and Test<a hidden class=anchor aria-hidden=true href=#train-validation-and-test>#</a></h3><p>Evaluating the model performance on unseen data is important to make sure it generalizes well.
It is common practice to split the training data into three parts: 80% for training, 10% for validation, and 10% for testing.
The validation set could also be used for early stopping, which means stopping the training process when the performance on the validation set starts to degrade, preventing the model from overfitting to the training set.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_dataset</span><span class=p>(</span><span class=n>words</span><span class=p>,</span> <span class=n>block_size</span><span class=o>=</span><span class=mi>3</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>X</span><span class=p>,</span> <span class=n>Y</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>w</span> <span class=ow>in</span> <span class=n>words</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>context</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=n>block_size</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>char</span> <span class=ow>in</span> <span class=n>w</span> <span class=o>+</span> <span class=s2>&#34;.&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=n>idx</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>char</span><span class=p>]</span>
</span></span><span class=line><span class=cl>      <span class=n>X</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>Y</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      <span class=n>context</span> <span class=o>=</span> <span class=n>context</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span> <span class=o>+</span> <span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>Y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>Y</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>X</span><span class=p>,</span> <span class=n>Y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>random</span>
</span></span><span class=line><span class=cl><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>random</span><span class=o>.</span><span class=n>shuffle</span><span class=p>(</span><span class=n>words</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>n1</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=mf>0.8</span><span class=o>*</span><span class=nb>len</span><span class=p>(</span><span class=n>words</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>n2</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=mf>0.9</span><span class=o>*</span><span class=nb>len</span><span class=p>(</span><span class=n>words</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X_tr</span><span class=p>,</span> <span class=n>y_tr</span> <span class=o>=</span> <span class=n>build_dataset</span><span class=p>(</span><span class=n>words</span><span class=p>[:</span><span class=n>n1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>X_va</span><span class=p>,</span> <span class=n>y_va</span> <span class=o>=</span> <span class=n>build_dataset</span><span class=p>(</span><span class=n>words</span><span class=p>[</span><span class=n>n1</span><span class=p>:</span><span class=n>n2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>X_te</span><span class=p>,</span> <span class=n>y_te</span> <span class=o>=</span> <span class=n>build_dataset</span><span class=p>(</span><span class=n>words</span><span class=p>[</span><span class=n>n2</span><span class=p>:])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Generator</span><span class=p>()</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>C</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>27</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>6</span><span class=p>,</span> <span class=mi>100</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>100</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>100</span><span class=p>,</span> <span class=mi>27</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>b2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>27</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>parameters</span> <span class=o>=</span> <span class=p>[</span><span class=n>C</span><span class=p>,</span> <span class=n>W1</span><span class=p>,</span> <span class=n>b1</span><span class=p>,</span> <span class=n>W2</span><span class=p>,</span> <span class=n>b2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>p</span><span class=o>.</span><span class=n>requires_grad</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tr_losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>va_losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>20000</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>X_tr</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=p>(</span><span class=mi>32</span><span class=p>,</span> <span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=n>embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>X_tr</span><span class=p>[</span><span class=n>idx</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>  <span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>logits</span> <span class=o>=</span> <span class=n>h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>y_tr</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># learning rate decay</span>
</span></span><span class=line><span class=cl>  <span class=n>lr</span> <span class=o>=</span> <span class=mf>0.1</span> <span class=k>if</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>epochs</span> <span class=o>//</span> <span class=mi>2</span> <span class=k>else</span> <span class=mf>0.01</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>parameters</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span><span class=o>.</span><span class=n>data</span> <span class=o>+=</span> <span class=o>-</span><span class=n>lr</span> <span class=o>*</span> <span class=n>p</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl>  <span class=n>tr_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>val_embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>X_va</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>val_h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>val_embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>val_logits</span> <span class=o>=</span> <span class=n>val_h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>  <span class=n>val_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>val_logits</span><span class=p>,</span> <span class=n>y_va</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>va_losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>val_loss</span><span class=o>.</span><span class=n>item</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>),</span> <span class=n>tr_losses</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Training Loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>),</span> <span class=n>va_losses</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Validation Loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Training and Validation Loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Epochs&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Loss&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>loc</span><span class=o>=</span><span class=s1>&#39;best&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><pre><code>torch.Size([182625, 3]) torch.Size([182625])
torch.Size([22655, 3]) torch.Size([22655])
torch.Size([22866, 3]) torch.Size([22866])
</code></pre><figure><img src=index_files/figure-markdown_strict/fig-loss-plot-output-2.png id=fig-loss-plot width=672 height=449 alt="Figure 2: Plot for training and validation loss"><figcaption aria-hidden=true>Figure 2: Plot for training and validation loss</figcaption></figure><p>As shown in <a href=#fig-loss-plot>Figure 2</a>, there is a tiny drop in the validation loss at 10000 epochs, which indicates that our training did encounter a plateau and learning rate decay works very well.
Let&rsquo;s check the loss of the testing data.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>test_embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>X_te</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>test_h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>test_embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>6</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>test_logits</span> <span class=o>=</span> <span class=n>test_h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl><span class=n>test_loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>test_logits</span><span class=p>,</span> <span class=n>y_te</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loss on validation data: </span><span class=si>{</span><span class=n>val_loss</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Loss on testing data: </span><span class=si>{</span><span class=n>test_loss</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Loss on validation data: 2.375811
Loss on testing data: 2.374066
</code></pre><p>The losses on validation and testing data are close, indicating we are not overfitting.</p><h2 id=visualization-of-embedding>Visualization of Embedding<a hidden class=anchor aria-hidden=true href=#visualization-of-embedding>#</a></h2><p>Let&rsquo;s visualize our embedding matrix.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>8</span><span class=p>,</span><span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>C</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>C</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>data</span><span class=p>,</span> <span class=n>s</span> <span class=o>=</span> <span class=mi>200</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>C</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>  <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>C</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>itos</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>ha</span><span class=o>=</span><span class=s2>&#34;center&#34;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s2>&#34;center&#34;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;white&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=s1>&#39;minor&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><figure><img src=index_files/figure-markdown_strict/fig-embedding-output-1.png id=fig-embedding width=656 height=633 alt="Figure 3: Visualization of 2D embedding matrix"><figcaption aria-hidden=true>Figure 3: Visualization of 2D embedding matrix</figcaption></figure><p>As depicted in <a href=#fig-embedding>Figure 3</a>, the vowels are closely grouped in the left bottom corner of the plot, while the <code>.</code> is situated far away in the top right corner.</p><h2 id=word-generation>Word Generation<a hidden class=anchor aria-hidden=true href=#word-generation>#</a></h2><p>The last thing we want to do is word generation.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Generator</span><span class=p>()</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>420</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>20</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>out</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=n>context</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=n>block_size</span>
</span></span><span class=line><span class=cl>  <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>embed</span> <span class=o>=</span> <span class=n>C</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>context</span><span class=p>])]</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tanh</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=o>@</span> <span class=n>W1</span> <span class=o>+</span> <span class=n>b1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=n>h</span> <span class=o>@</span> <span class=n>W2</span> <span class=o>+</span> <span class=n>b2</span>
</span></span><span class=line><span class=cl>    <span class=n>probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>context</span> <span class=o>=</span> <span class=n>context</span><span class=p>[</span><span class=mi>1</span><span class=p>:]</span> <span class=o>+</span> <span class=p>[</span><span class=n>idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>idx</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=k>break</span>
</span></span><span class=line><span class=cl>    <span class=n>out</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>itos</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>out</span><span class=p>))</span>
</span></span></code></pre></div><pre><code>rai
mal
lemistani
iua
kacyt
tan
zatlixahnen
rarbi
zethanli
blie
mozien
nar
ameson
xaxun
koma
aedh
sarixstah
elin
dyannili
saom
</code></pre><p>The words generated by the multilayer perceptron model make more sense than those from our last model.
Still, there are many other ways to improve model performance.
For example, train more epochs with learning rate decay, increase the batch size to make the training more stable, and add more data.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://gejun.name/tags/n-gram/>n-gram</a></li><li><a href=https://gejun.name/tags/pytorch/>pytorch</a></li><li><a href=https://gejun.name/tags/embedding/>embedding</a></li></ul><nav class=paginav><a class=next href=https://gejun.name/natural-language-processing/understanding-attention-nlp/><span class=title>Next »</span><br><span>Understanding Transformer Architecture by Building GPT</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Multilayer Perceptron (MLP) on twitter" href="https://twitter.com/intent/tweet/?text=Multilayer%20Perceptron%20%28MLP%29&amp;url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore-mlp%2f&amp;hashtags=n-gram%2cpytorch%2cembedding"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Multilayer Perceptron (MLP) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore-mlp%2f&amp;title=Multilayer%20Perceptron%20%28MLP%29&amp;summary=Multilayer%20Perceptron%20%28MLP%29&amp;source=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore-mlp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Multilayer Perceptron (MLP) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore-mlp%2f&title=Multilayer%20Perceptron%20%28MLP%29"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Multilayer Perceptron (MLP) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore-mlp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Multilayer Perceptron (MLP) on whatsapp" href="https://api.whatsapp.com/send?text=Multilayer%20Perceptron%20%28MLP%29%20-%20https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore-mlp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Multilayer Perceptron (MLP) on telegram" href="https://telegram.me/share/url?text=Multilayer%20Perceptron%20%28MLP%29&amp;url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore-mlp%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://giscus.app/client.js data-repo=zhugejun/zhugejun.github.io data-repo-id data-category=Announcements data-category-id data-mapping=pathname data-reactions-enabled=1 data-theme=preferred_color_scheme data-language=en crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the comments powered by giscus.</noscript></article></main><footer class=footer><span>&copy; 2023 <a href=https://gejun.name>Gejun's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>