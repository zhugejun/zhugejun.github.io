<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding Transformer Architecture by Building GPT | Gejun's Blog</title><meta name=keywords content="transformers,encoder,docoder,pytorch,attention"><meta name=description content="In Part2, we constructed a straightforward MLP model to generate characters based on 32k popular names. In this lecture, Andrej guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model. We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.
Data Preparation Let&rsquo;s first import the necessary libraries and get the data ready."><meta name=author content="Gejun Zhu"><link rel=canonical href=https://gejun.name/natural-language-processing/understanding-attention-nlp/><link crossorigin=anonymous href=/assets/css/stylesheet.07b504287c624e47e21667d48ef8c5c81574d5a3594c9eb68922bfeacb71823b.css integrity="sha256-B7UEKHxiTkfiFmfUjvjFyBV01aNZTJ62iSK/6stxgjs=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://gejun.name/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gejun.name/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gejun.name/favicon-32x32.png><link rel=apple-touch-icon href=https://gejun.name/apple-touch-icon.png><link rel=mask-icon href=https://gejun.name/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BEP7FKQVEG"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BEP7FKQVEG")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="Understanding Transformer Architecture by Building GPT"><meta property="og:description" content="In Part2, we constructed a straightforward MLP model to generate characters based on 32k popular names. In this lecture, Andrej guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model. We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.
Data Preparation Let&rsquo;s first import the necessary libraries and get the data ready."><meta property="og:type" content="article"><meta property="og:url" content="https://gejun.name/natural-language-processing/understanding-attention-nlp/"><meta property="og:image" content="https://gejun.name"><meta property="article:section" content="natural-language-processing"><meta property="article:published_time" content="2023-03-15T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-15T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gejun.name"><meta name=twitter:title content="Understanding Transformer Architecture by Building GPT"><meta name=twitter:description content="In Part2, we constructed a straightforward MLP model to generate characters based on 32k popular names. In this lecture, Andrej guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model. We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.
Data Preparation Let&rsquo;s first import the necessary libraries and get the data ready."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Natural-language-processings","item":"https://gejun.name/natural-language-processing/"},{"@type":"ListItem","position":3,"name":"Understanding Transformer Architecture by Building GPT","item":"https://gejun.name/natural-language-processing/understanding-attention-nlp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Transformer Architecture by Building GPT","name":"Understanding Transformer Architecture by Building GPT","description":"In Part2, we constructed a straightforward MLP model to generate characters based on 32k popular names. In this lecture, Andrej guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model. We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.\nData Preparation Let\u0026rsquo;s first import the necessary libraries and get the data ready.","keywords":["transformers","encoder","docoder","pytorch","attention"],"articleBody":" In Part2, we constructed a straightforward MLP model to generate characters based on 32k popular names. In this lecture, Andrej guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model. We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.\nData Preparation Let’s first import the necessary libraries and get the data ready. We will use the tiny shakespeare dataset, featured in Andrej Karpathy’s blog post The Unreasonable Effectiveness of Recurrent Neural Networks.\nimport math import requests import torch from torch import nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt device = 'cuda' if torch.cuda.is_available() else 'cpu' data_url = \"https://t.ly/u1Ax\" text = requests.get(data_url).text # building vocabulary chars = sorted(list(set(text))) vocab_size = len(chars) print(f\"Vocabulary size: {vocab_size}\") print(f\"Vocabulary: {repr(''.join(chars))}\") # mappings stoi = {c: i for i, c in enumerate(chars)} itos = {v: k for k, v in stoi.items()} def encode(s): return [stoi[c] for c in s] def decode(l): return ''.join([itos[i] for i in l]) Vocabulary size: 65 Vocabulary: \"\\n !$\u0026',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\" We have 65 characters, including all lower- and upper-case letters and a few special characters, \\n !$\u0026',-.3:;?. Next, we split the data into two parts: 90% of the dataset for training and 10% for validation.\n# create tensor data = torch.tensor(encode(text), dtype=torch.long) n = int(0.9*len(data)) train_data = data[:n] val_data = data[n:] print(train_data.shape) print(val_data.shape) torch.Size([1003854]) torch.Size([111540]) Training Data Feeding the entire text to the transformer all at once can be computationally expensive and prohibitive. To address this issue, neural network models use batch processing techniques to update the model’s weights and biases. This technique involves dividing the training dataset into smaller subsets, or batches, of size batch_size. The batches are then processed separately by the neural network to update the model’s parameters. For a character generation model, we need a sequence of characters as our training sample, which can be considered a time dimension. For the sample below, the input is [18] and the target is 47 at time 0, and the input is [18, 47] and the target is 56, and so on.\nblock_size = 8 x = train_data[:block_size] y = train_data[1:block_size+1] for t in range(block_size): context = x[:t+1] target = y[t] print(f\"Time: {t}, input: {context}, target: {target}\") Time: 0, input: tensor([18]), target: 47 Time: 1, input: tensor([18, 47]), target: 56 Time: 2, input: tensor([18, 47, 56]), target: 57 Time: 3, input: tensor([18, 47, 56, 57]), target: 58 Time: 4, input: tensor([18, 47, 56, 57, 58]), target: 1 Time: 5, input: tensor([18, 47, 56, 57, 58, 1]), target: 15 Time: 6, input: tensor([18, 47, 56, 57, 58, 1, 15]), target: 47 Time: 7, input: tensor([18, 47, 56, 57, 58, 1, 15, 47]), target: 58 To create our training data, we select a sequence starting from the character of a fixed size block_size in each batch. We then create our input and target along the time dimension inside each sequence, resulting in batch_size time block_size training examples. The example below shows that there are $4\\times 8=32$ training examples in each batch as we have 4 sequences of 8 characters each.\nbatch_size = 4 block_size = 8 def get_batch(split): data = train_data if split == \"train\" else val_data idx = torch.randint(len(data) - block_size, (batch_size,)) x = torch.stack([data[i:i+block_size] for i in idx]) y = torch.stack([data[i+1:i+block_size+1] for i in idx]) x, y = x.to(device), y.to(device) return x, y x_batch, y_batch = get_batch(\"train\") print(x_batch.shape, y_batch.shape) for b in range(batch_size): print(f\"---------- Batch {b} ----------\") for t in range(block_size): context = x_batch[b, :t+1] target = y_batch[b, t] print(f\"Time: {t}, input: {context}, target: {target}\") torch.Size([4, 8]) torch.Size([4, 8]) ---------- Batch 0 ---------- Time: 0, input: tensor([53], device='cuda:0'), target: 56 Time: 1, input: tensor([53, 56], device='cuda:0'), target: 58 Time: 2, input: tensor([53, 56, 58], device='cuda:0'), target: 46 Time: 3, input: tensor([53, 56, 58, 46], device='cuda:0'), target: 11 Time: 4, input: tensor([53, 56, 58, 46, 11], device='cuda:0'), target: 1 Time: 5, input: tensor([53, 56, 58, 46, 11, 1], device='cuda:0'), target: 41 Time: 6, input: tensor([53, 56, 58, 46, 11, 1, 41], device='cuda:0'), target: 53 Time: 7, input: tensor([53, 56, 58, 46, 11, 1, 41, 53], device='cuda:0'), target: 51 ---------- Batch 1 ---------- Time: 0, input: tensor([52], device='cuda:0'), target: 52 Time: 1, input: tensor([52, 52], device='cuda:0'), target: 53 Time: 2, input: tensor([52, 52, 53], device='cuda:0'), target: 58 Time: 3, input: tensor([52, 52, 53, 58], device='cuda:0'), target: 1 Time: 4, input: tensor([52, 52, 53, 58, 1], device='cuda:0'), target: 46 Time: 5, input: tensor([52, 52, 53, 58, 1, 46], device='cuda:0'), target: 47 Time: 6, input: tensor([52, 52, 53, 58, 1, 46, 47], device='cuda:0'), target: 58 Time: 7, input: tensor([52, 52, 53, 58, 1, 46, 47, 58], device='cuda:0'), target: 1 ---------- Batch 2 ---------- Time: 0, input: tensor([35], device='cuda:0'), target: 43 Time: 1, input: tensor([35, 43], device='cuda:0'), target: 56 Time: 2, input: tensor([35, 43, 56], device='cuda:0'), target: 58 Time: 3, input: tensor([35, 43, 56, 58], device='cuda:0'), target: 1 Time: 4, input: tensor([35, 43, 56, 58, 1], device='cuda:0'), target: 58 Time: 5, input: tensor([35, 43, 56, 58, 1, 58], device='cuda:0'), target: 46 Time: 6, input: tensor([35, 43, 56, 58, 1, 58, 46], device='cuda:0'), target: 53 Time: 7, input: tensor([35, 43, 56, 58, 1, 58, 46, 53], device='cuda:0'), target: 59 ---------- Batch 3 ---------- Time: 0, input: tensor([53], device='cuda:0'), target: 59 Time: 1, input: tensor([53, 59], device='cuda:0'), target: 50 Time: 2, input: tensor([53, 59, 50], device='cuda:0'), target: 42 Time: 3, input: tensor([53, 59, 50, 42], device='cuda:0'), target: 1 Time: 4, input: tensor([53, 59, 50, 42, 1], device='cuda:0'), target: 41 Time: 5, input: tensor([53, 59, 50, 42, 1, 41], device='cuda:0'), target: 46 Time: 6, input: tensor([53, 59, 50, 42, 1, 41, 46], device='cuda:0'), target: 53 Time: 7, input: tensor([53, 59, 50, 42, 1, 41, 46, 53], device='cuda:0'), target: 54 BigramLanguageModel Let’s rewrite our previous bigram model. Here is the main part of the model we built in Part 1.\nW = torch.randn((27, 27), requires_grad=True) logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) Base model From Part 2, we learned how to represent a token with a fixed-length, real-valued, and learnable vector, which is known as token embedding. The embedding matrix can be initialized by nn.Embedding where num_embeddings refers to the vocabulary size, and embedding_dim refers to the length of the feature vector. For consistency with the original paper, we will use d_model to represent the feature vector’s length, which will be set to 64 instead of the vocabulary size. As a result, we need to create another linear layer to ensure that the output dimension is the same as the vocabulary size.\nIt’s worth noting that we cannot compute the cross-entropy for a 3-dimensional matrix, as seen from the documentation of cross_entropy function. Therefore, we need to reshape the logits and targets before computing it.\ntorch.manual_seed(42) batch_size = 32 d_model = 64 # B: batch_size # T: time, up to block_size # C: d_model # 65: vocabulary size class BigramLanguageModel(nn.Module): def __init__(self, vocab_size): super().__init__() self.token_embedding_table = nn.Embedding(vocab_size, d_model) # 65, C self.output_linear = nn.Linear(d_model, vocab_size) # C, 65 def forward(self, idx, targets=None): # idx: B, T embedded = self.token_embedding_table(idx) # B, T, C logits = self.output_linear(embedded) # B, T, 65 # there is no target when predicting if targets is None: loss = None else: B, T, C = logits.shape logits = logits.view(B*T, C) # N, C targets = targets.view(B*T) # N loss = F.cross_entropy(logits, targets) return logits, loss def generate(self, idx, max_length): for _ in range(max_length): logits, _ = self(idx) # focus on the char on last time stamp because it's a bigram model logits = logits[:, -1, :] # B, C probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) # concatenate the new generated to the old ones idx = torch.cat((idx, idx_next), dim=1) return idx base_model = BigramLanguageModel(vocab_size).to(device) idx = torch.zeros((1, 1), dtype=torch.long, device=device) print(decode(base_model.generate(idx, max_length=100).squeeze().tolist())) dF3unFC;RnXbzDP'CnT-P.lBuYkUWdXRaRnqDCk,b!:UE$J,uuheZqKPXEPYMYSAxKlRpvwisS.MIwITP$YqrgGRpP.AwYluRWGI Certainly, the 100 characters generated at this point are not meaningful as the model has not been trained yet.\nTraining optimizer = torch.optim.AdamW(base_model.parameters(), lr=1e-3) # training epochs = 10000 for epoch in range(epochs): x_batch, y_batch = get_batch(\"train\") logits, loss = base_model(x_batch, y_batch) optimizer.zero_grad(set_to_none=True) loss.backward() optimizer.step() if epoch % 1000 == 0 or epoch == epochs - 1: print(f\"Epoch {epoch}: {loss.item()}\") # starting with [[0]] idx = torch.zeros((1, 1), dtype=torch.long, device=device) print(decode(base_model.generate(idx, max_length=100).squeeze().tolist())) Epoch 0: 4.440201282501221 Epoch 1000: 2.5844924449920654 Epoch 2000: 2.469000816345215 Epoch 3000: 2.473245859146118 Epoch 4000: 2.4555399417877197 Epoch 5000: 2.5115771293640137 Epoch 6000: 2.3323276042938232 Epoch 7000: 2.331480026245117 Epoch 8000: 2.436919927597046 Epoch 9000: 2.473867893218994 Epoch 9999: 2.636822462081909 Tody inde eve d tlakemang yofowhas Thind. UCESer ur thathapr me machan fl haisu d iere--sthurore ce The generated characters appear more word-like than before, but most are misspelled because the bigram model only generates a new character based on the last generated character. To improve our model’s performance, we need a way to incorporate information from previously generated characters up to block_size. One solution is to use a bag-of-words model to extract features from previously generated characters. In a bag-of-words model, a text is treated as a bag of tokens, disregarding grammar and order. In the next section, we will introduce the transformer architecture from the classic paper, Attention is all you need. We will explain what attention is, how to calculate, and most importantly, how to understand it intuitively. Furthermore, we will implement it step by step and see how it improves our model’s performance.\nTransformer Architecture The transformer model architecture from the paper is shown below.\nLet’s first clarify what an encoder is. According to the paper:\n“The encoder maps an input sequence of symbol representations $(x_1, …, x_n)$ to a sequence of continuous representations $z=(z_1, …, z_n)$. It converts an input sequence of tokens into a sequence of embedding vectors, often called a hidden state. The encoder is composed of a stack of encoder layers, which are used to update the input embeddings to produce representations that encode some contextual information in the sequence.”\nIn the transformer architecture shown above, the encoder is on the left side inside the blue box, and it contains multiple encoder layers. The encoder compresses and extracts important information from the input sequence while discarding the irrelevant information.\nNext, let’s see what a decoder is. The decoder is inside the red box on the right side of the transformer architecture. It is also composed of a stack of decoder layers, which are similar to encoder layers except that they add an extra masked layer in the multi-head attention.\nLast but not least, the state generated from the encoder is passed to the decoder and generates the output sequence, which is referred to as cross-attention. A decoder uses the encoder’s hidden state to iteratively generate an output sequence of tokens, one at a time.\nGPT, which stands for Generative Pretrained Transformer, focuses on the decoder part. Therefore, our model architecture becomes the following.\nIn the next few sections, we will build the model from bottom to top. Since the input embedding stays the same, we will skip the input embedding section and talk about positional embedding.\nPositional Embedding The embedding of input tokens alone does not capture any information about their relative positions within the sequence. Hence a positional embedding is introduced to inject this information. According to the paper, there are multiple ways for positional embeddings, with some being fixed while others are learnable. For our implementation, we will use a learnable positional embedding with the same dimension as the token embedding, which is d_model. The num_embeddings parameter in the nn.Embedding function will be set to block_size since our training sequence has a maximum length of block_size.\nLet’s dive into the dimensions of the input tokens. The input tokens have two dimensions: the batch dimension, which indicates how many independent sequences the model processes in parallel, and the time dimension, which records the current position within the sequence up to a maximum length of block_size. After the input tokens pass through the token and positional embedding layers, they will have an additional channel dimension, which is a convention borrowed from computer vision. For simplicity, we will use B, T, and C to denote the batch, time, and channel dimensions, respectively.\nclass BigramLanguageModel(nn.Module): def __init__(self, vocab_size): super().__init__() self.token_embedding_table = nn.Embedding(vocab_size, d_model) # position embedding table self.position_embedding_table = nn.Embedding(block_size, d_model) self.output_linear = nn.Linear(d_model, vocab_size) def forward(self, idx, targets=None): # idx: B, T B, T = idx.shape token_embed = self.token_embedding_table(idx) # B, T, C posit_embed = self.position_embedding_table(torch.arange(T, device=device)) # T, C # sum of token and positional embeddings x = token_embed + posit_embed # B, T, C logits = self.output_linear(x) # B, T, vocab_size if targets is None: loss = None else: B, T, C = logits.shape logits = logits.view(B*T, C) # (N, C) targets = targets.view(B*T) # (N) loss = F.cross_entropy(logits, targets) return logits, loss base_model = BigramLanguageModel(vocab_size).to(device) optimizer = torch.optim.AdamW(base_model.parameters(), lr=1e-3) epochs = 10000 for epoch in range(epochs): x_batch, y_batch = get_batch(\"train\") logits, loss = base_model(x_batch, y_batch) optimizer.zero_grad(set_to_none=True) loss.backward() optimizer.step() if epoch % 1000 == 0 or epoch == epochs - 1: print(f\"Epoch {epoch}: {loss.item()}\") Epoch 0: 4.435860633850098 Epoch 1000: 2.538156270980835 Epoch 2000: 2.5488555431365967 Epoch 3000: 2.479320764541626 Epoch 4000: 2.3083598613739014 Epoch 5000: 2.472010850906372 Epoch 6000: 2.5080037117004395 Epoch 7000: 2.4842913150787354 Epoch 8000: 2.3710641860961914 Epoch 9000: 2.4978179931640625 Epoch 9999: 2.416473627090454 Attention What is attention?\n“An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.”\nWe can compute the attention score using the following steps as described in the paper.\n$$Attention(Q,K,V)=softmax\\bigl( \\frac{QK^T}{\\sqrt{d_k}}\\bigr) V$$\nTo better understand the attention formula above, it’s helpful to review some linear algebra concepts.\nDot Product The dot product of two Euclidean vectors $\\vec{a}$ and $\\vec{b}$ is defined by\n$$\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^n a_ib_i$$\nwhere $n$ is the length of the vectors.\nGeometrically, the dot product of two vectors is equal to the product of their magnitudes and the cosine of the angle between them. Specifically, if $\\theta$ is the angle between $\\vec{a}$ and $\\vec{b}$, then\n$$\\vec{a} \\cdot \\vec{b} = |a| \\cdot |b| cos(\\theta)$$\nsource\nThe quantity $|a|cos(\\theta)$ is the scalar projection of $\\vec{a}$ onto $\\vec{b}$. The higher the product, the more similar two vectors. Let’s take the learned embedding from our last model and compute the dot products of some tokens from our vocabulary.\nchar1 = 'a' char2 = 'z' char3 = 'e' token_embeddings = base_model.token_embedding_table.weight def calc_dp(char1, char2): with torch.no_grad(): embed1 = token_embeddings[stoi[char1]] embed2 = token_embeddings[stoi[char2]] return sum(embed1 * embed2) print(f\"Dot product of {char1} and {char1}: {calc_dp(char1, char1):.6f}\") print(f\"Dot product of {char1} and {char2}: {calc_dp(char1, char2):.6f}\") print(f\"Dot product of {char1} and {char3}: {calc_dp(char1, char3):.6f}\") Dot product of a and a: 78.494980 Dot product of a and z: -14.060809 Dot product of a and e: 12.071777 The dot product of the feature vectors of a and itself is much higher than with e or z. Also, the results show that a is more similar to e then to z.\nAttention Score Every token in the input sequence generates a query vector and a key vector of the same dimension. This operation is called self-attention because $Q$, $V$, and $T$ are all derived from the same source in GPT. The dot product of the query and key vectors measures their similarity.\nLet $X_{m\\times n}$ and $W$ denote the embedding matrix of the input sequence and the weight of the linear transformation, where $m$ is the number of tokens, $n$ is the token dimension, and $k$ is the output dimension of the linear transformation or the head size of our attention. Each row represents the token embedding for each token in the input. Then, we apply three linear transformations on $X$ to project it onto 3 new vector spaces:\n$X_{m\\times n} \\cdot W^Q_{n\\times k} = Q_{m\\times k}$ to obtain the query space. $X_{m\\times n} \\cdot W^K_{n\\times k} = K_{m\\times k}$ to obtain the key space. $X_{m\\times n} \\cdot W^V_{n\\times k} = V_{m\\times k}$ to obtain the value space. $Q\\cdot K^T$ is the attention score matrix, having a shape of $m \\times m$. The larger the value, the closer the vectors and hence the more attention.\nLet’s take the learned token and positional embeddings from our previous model, apply the query and key transformations, and calculate the attention scores of the sequence sea.\nsequence = \"sea\" # get positional embeddings from model position_embeddings = base_model.position_embedding_table.weight tokens = torch.tensor([stoi [c] for c in sequence]) positions = torch.tensor([i for i in range(len(sequence))]) # final embedding matrix for a given sequence embed = token_embeddings[tokens] + position_embeddings[positions] # query and vector weights d_k = 16 torch.manual_seed(42) q = nn.Linear(embed.shape[1], d_k, bias=False).to(device) k = nn.Linear(embed.shape[1], d_k, bias=False).to(device) # query and key space with torch.no_grad(): Q = q(embed) K = k(embed) # similarity between query and keys score = Q @ K.T print(score) tensor([[ 1.5712, -2.8564, 3.0652], [ 1.6477, 0.1216, -0.4353], [-6.8497, -1.1358, -0.8100]], device='cuda:0') The attention score vector for e is [ 1.6477, 0.1216, -0.4353] However, the dot products may become too large in magnitude when the head size $d_k$ is large, which can result in extremely small gradients after applying the softmax function. To mitigate this issue, the scores are scaled by multiplying with the factor $\\frac{1}{\\sqrt{d_k}}$, as suggested in the paper.\nwith torch.no_grad(): score /= math.sqrt(d_k) score = F.softmax(score, dim=-1) print(score) tensor([[0.3593, 0.1188, 0.5220], [0.4392, 0.2999, 0.2609], [0.1031, 0.4302, 0.4667]], device='cuda:0') After scaling, the attention score vector for token e in sea becomes [0.4392, 0.2999, 0.2609]. This implies that the token s requires more attention than the tokens e and a.\nWait a minute! Why does the token e pay attention to the future token a in a GPT model? It is cheating in this way. How can we preserve the information from the previous tokens while not peeking the future tokens? The masking layer.\nMasking Where exactly do we apply a masking layer? Since we want to use a softmax function to normalize the attention scores until the current position so that the divided attention sums to one, it should be applied after calculating the unscaled attention score and before the softmax layer. In this way, we can exclude the future tokens. To implement this masking, we will use a PyTorch built-in function, torch.tril, which preserves the original values for the lower triangular part of the matrix while setting the upper part to zero. In our case, we replace the scores in the upper triangular part of the matrix with a very small number, such as float(\"-inf\"), so that they will become zeros after applying the softmax function.\nwith torch.no_grad(): mask = torch.tril(torch.ones(embed.shape[0], embed.shape[0])).to(device) score = score.masked_fill(mask == 0, float(\"-inf\")) score = F.softmax(score, dim=-1) print(score) tensor([[1.0000, 0.0000, 0.0000], [0.5348, 0.4652, 0.0000], [0.2614, 0.3626, 0.3760]], device='cuda:0') Now, the scaled attention vector for e becomes [0.5348, 0.4652, 0.0000], indicating that the model pays roughly half of its attention to tokens s and e when it reaches token e while completely ignoring the future token a.\nWeighted Sum Finally, we obtain a new adjusted embedding for each token in the context by multiplying the attention matrix with the value matrix $V$.\nv = nn.Linear(embed.shape[1], d_k, bias=False).to(device) with torch.no_grad(): V = v(embed) new_embed = score @ V print(new_embed) tensor([[ 0.0959, 0.4068, -0.2983, 0.8456, -1.6365, 0.9545, -0.5414, 2.2582, -0.3868, 1.1196, 1.6244, 0.3545, -1.1479, 0.4165, -0.7899, -0.7008], [ 0.1352, -0.2616, -0.4122, 0.1182, -1.2960, 0.5224, -0.3819, 1.3335, -0.1463, 0.2113, 0.8228, -0.0095, -0.8548, 0.0567, -0.5980, -0.3525], [-0.4126, -0.4585, -0.2760, 0.0813, -0.9609, 0.2358, -0.3887, 0.7906, 0.0084, -0.1094, 0.3198, -0.5582, -0.7782, 0.4525, -0.1208, 0.1493]], device='cuda:0') To put it in another way, we force the tokens to look at each other by multiplying the attention scores with the value matrix $V$. This helps to adjust the value matrix to represent the entire sequence better as training progresses.\nDemystifying QKV How do we understand attention from intuition? Here is a great answer from Cross Validated.\nThe key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).\nsource\nHere are the intuitive meaning of these matrices:\nThe query matrix represents a piece of information we are looking for in a query we have. The key matrix is intuitively meant to represent the relevance of each word to our query. And the key matrix represents how important each word is to my overall query. The value matrix intuitively represents the contextless meaning of our input tokens. Imagine that you’re at the supermarket buying all the ingredients you need for your dinner. You have the dish’s recipe, and the ingredients (query) are what you look for in a supermarket. Scanning the shelves, you look at the labels (keys) and check whether they match an ingredient on your list. You are determining the similarity between query and keys. If you have a match, you take the item (value) from the shelf.\nLet’s put the attention layer into a single Head class.\nclass Head(nn.Module): def __init__(self, d_k): super().__init__() self.query = nn.Linear(d_model, d_k, bias=False) # C, d_k self.key = nn.Linear(d_model, d_k, bias=False) # C, d_k self.value = nn.Linear(d_model, d_k, bias=False) # C, d_k # not a model parameter self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # block_size, block_size def forward(self, x): B, T, C = x.shape q = self.query(x) # B, T, d_k k = self.key(x) # B, T, d_k score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(C) # B, T, T score = score.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # B, T, T score = F.softmax(score, dim=-1) # B, T, T v = self.value(x) # B, T, d_k out = score @ v # (B, T, T)@(B, T, d_k) = (B, T, d_k) return out To ensure compatibility with matrix multiplication, we need to set the head size as the embedding dimension, d_model, because we currently only have one head layer. However, we will not train this model at this moment.\nclass BigramLanguageModel(nn.Module): def __init__(self, vocab_size): super().__init__() self.token_embedding_table = nn.Embedding(vocab_size, d_model) self.position_embedding_table = nn.Embedding(block_size, d_model) self.self_attn = Head(d_model) self.output_linear = nn.Linear(d_model, vocab_size) def forward(self, idx, targets=None): B, T = idx.shape token_embed = self.token_embedding_table(idx) posit_embed = self.position_embedding_table(torch.arange(T, device=device)) x = token_embed + posit_embed # apply self attention x = self.self_attn(x) logits = self.output_linear(x) if targets is None: loss = None else: B, T, C = logits.shape logits = logits.view(B*T, C) targets = targets.view(B*T) loss = F.cross_entropy(logits, targets) return logits, loss def generate(self, idx, max_length): for _ in range(max_length): logits, loss = self(idx[:, -block_size:]) logits = logits[:, -1, :] probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx, idx_next), dim=1) return idx Multi-head Attention As an old saying goes, two heads are better than one. By having multiple heads, we can apply multiple transformations to the embeddings. Each projection has its own set of learnable parameters, which enables the self-attention layer to focus on different semantic aspects of the sequence. We will denote the number of heads as h.\nclass MultiHeadAttention(nn.Module): def __init__(self, h, d_k): super.__init__() self.heads = nn.ModuleList([Head(d_k) for _ in range(h)]) def forward(self, x): return torch.cat([head(x) for head in self.heads], dim=-1) # B, T, C Dropout Dropout was proposed in Dropout: A Simple Way to Prevent Neural Networks from Overfitting by Nitish Srivastava et al. in 2014. In this technique, a certain proportion of neurons are randomly dropped out during training to prevent overfitting.\nWe apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\nsource\nWe will apply PyTorch’s built-in function nn.Dropout to our Head and MultiHeadAttention layers.\ndropout = 0.1 class Head(nn.Module): def __init__(self, d_k): super().__init__() self.query = nn.Linear(d_model, d_k, bias=False) # C, d_k self.key = nn.Linear(d_model, d_k, bias=False) # C, d_k self.value = nn.Linear(d_model, d_k, bias=False) # C, d_k # not a model parameter self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # block_size, block_size self.dropout = nn.Dropout(dropout) def forward(self, x): B, T, C = x.shape q = self.query(x) # B, T, d_k k = self.key(x) # B, T, d_k score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(C) # B, T, T score = score.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # B, T, T score = F.softmax(score, dim=-1) # B, T, T score = self.dropout(score) v = self.value(x) # B, T, d_k out = score @ v # (B, T, T)@(B, T, d_k) = (B, T, d_k) return out class MultiHeadAttention(nn.Module): def __init__(self, h, d_k): super.__init__() self.heads = nn.ModuleList([Head(d_k) for _ in range(h)]) self.dropout = nn.Dropout(dropout) def forward(self, x): x = torch.cat([head(x) for head in self.heads], dim=-1) # B, T, C x = self.dropout(x) return x Residual Connection The concept of residual connections was first introduced in 2015 by Kaiming He et al. in their paper Deep Residual Learning for Image Recognition. It allows the network to bypass one or more layers, which helps alleviate the vanishing gradient problem that could occur in very deep neural networks.\nsource\nTo implement residual connections and a projection layer in our multi-head attention module, we modify the MultiHeadAttention class as follows.\nclass MultiHeadAttention(nn.Module): def __init__(self, h, d_k): super().__init__() self.heads = nn.ModuleList([Head(d_k) for _ in range(h)]) self.proj = nn.Linear(d_model, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): x = torch.cat([head(x) for head in self.heads], dim=-1) x = self.proj(x) x = self.dropout(x) return x Feed-Forward As stated in the paper:\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\nThis means that instead of processing the entire sequence of embeddings as a single vector, the feed-forward network applies the same linear transformations to each embedding individually.\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{model} = 512$, and the inner-layer has dimensionality $d_{ff} = 2048$.\nThis implies that our first linear layer in the feed-forward layer has an output dimension of d_model * 4, which serves as the input dimension of the second linear layer. We also apply a dropout layer to the feed-forward layer to avoid overfitting.\nclass FeedForward(nn.Module): def __init__(self): super().__init__() self.net = nn.Sequential( nn.Linear(d_model, d_model * 4), nn.ReLU(), nn.Linear(d_model * 4, d_model), nn.Dropout(dropout) ) def forward(self, x): x = self.net(x) return x Layer Normalization The concept of layer normalization was introduced by Jimmy Lei Ba et al. in their paper Layer Normalization published in 2016. Unlike batch normalization, which normalizes the inputs to a batch of data, layer normalization normalizes the inputs to a single layer of the network. In our implementation, we apply layer normalization before self-attention and feed-forward layers.\nsource\nRefactoring Let’s refactor the code to put multi-head attention and feed-forward layers to a single Block class. Moreover, the head size would be automatically set to d_model/h.\nclass Block(nn.Module): def __init__(self, h): super().__init__() d_k = d_model // h self.attn = MultiHeadAttention(h, d_k) self.ff = FeedForward() self.ln1 = nn.LayerNorm(d_model) self.ln2 = nn.LayerNorm(d_model) def forward(self, x): # attention + residual connection x = x + self.attn(x) # layer normalization x = self.ln1(x) # feed forward x = x + self.ff(x) # layer normalization x = self.ln2(x) return x Put Everything Together Here are the steps to build a GPT with transformer architecture:\nInitialize the token embedding table with the vocabulary size and embedding dimension (vocab_size, d_model). Initialize the positional embedding table with the maximum sequence length and embedding dimension (block_size, d_model). Create N identical decoder layers using the Block class with multi-head attention, feed-forward, and layer normalization layers. The head_size parameter will be automatically set to d_model/h. Add a linear output layer with the output dimension equal to the vocab_size. batch_size = 16 block_size = 32 eval_interval = 1000 eval_iters = 200 learning_rate = 1e-3 epochs = 10000 d_model = 64 # dimension of embedding h = 8 # number of heads N = 6 # number of identical layers dropout = 0.1 # dropout percentage device = 'cuda' if torch.cuda.is_available() else 'cpu' @torch.no_grad() def estimate_loss(): out = {} model.eval() for split in ['train', 'val']: losses = torch.zeros(eval_iters) for k in range(eval_iters): X, Y = get_batch(split) logits, loss = model(X, Y) losses[k] = loss.item() out[split] = losses.mean() model.train() return out class BigramLanguageModel(nn.Module): def __init__(self): super().__init__() self.token_embedding_table = nn.Embedding(vocab_size, d_model) self.position_embedding_table = nn.Embedding(block_size, d_model) self.blocks = nn.Sequential(*[Block(h) for _ in range(N)]) self.output_linear = nn.Linear(d_model, vocab_size) def forward(self, idx, targets=None): B, T = idx.shape token_embed = self.token_embedding_table(idx) posit_embed = self.position_embedding_table(torch.arange(T, device=device)) x = token_embed + posit_embed x = self.blocks(x) logits = self.output_linear(x) if targets is None: loss = None else: B, T, C = logits.shape logits = logits.view(B*T, C) targets = targets.view(B*T) loss = F.cross_entropy(logits, targets) return logits, loss def generate(self, idx, max_length): for _ in range(max_length): logits, _ = self(idx[:, -block_size:]) logits = logits[:, -1, :] probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx, idx_next), dim=1) return idx model = BigramLanguageModel().to(device) print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters') 0.309185 M parameters Retraining optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) for i in range(epochs): if i % eval_interval == 0 or i == epochs - 1: losses = estimate_loss() print(f\"step {i:\u003e6}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") x_batch, y_batch = get_batch('train') logits, loss = model(x_batch, y_batch) optimizer.zero_grad(set_to_none=True) loss.backward() optimizer.step() context = torch.zeros((1, 1), dtype=torch.long, device=device) print(decode(model.generate(context, max_length=2000)[0].tolist())) step 0: train loss 4.4133, val loss 4.4188 step 1000: train loss 2.1523, val loss 2.1733 step 2000: train loss 1.9162, val loss 1.9929 step 3000: train loss 1.8095, val loss 1.9325 step 4000: train loss 1.7424, val loss 1.8743 step 5000: train loss 1.7031, val loss 1.8359 step 6000: train loss 1.6730, val loss 1.8091 step 7000: train loss 1.6381, val loss 1.8015 step 8000: train loss 1.6231, val loss 1.7956 step 9000: train loss 1.6010, val loss 1.7734 step 9999: train loss 1.5991, val loss 1.7507 POLINCE: O-momen, marran, stack the blow. VALUMNIA: TRong it is 'twill o despreng. MARCIUS: She are. COMSARIO: Thraby, the tongue, And lefe to he she this highnoural, Have any but ut your to spuake it. LEONTES: Goot saled shur he wrett. SICIDIUS: Be she done! te. First KINquel: Thy had diul as recat my deasury? Faulter'er mean, on altal, If none banch with to times? York, Vom have yzage; this hight think noble of eye bewill fre, In gring might to jue of knot it the clunter, Were henrey quoring to jurition tone stime to known? Pryity and bear. KING EL.TAh, is leaven. For I would in Ancompers, for comen telms things: I worn apene so Herdow procked love; dime so worder. LORIS: It is here bear of go him. ROMEY: How I Leffater the death? And mearrinad king cans no myselfy that bartt, If you I decdom to be in tellothen, Low ke'es hath s duck, and within kindes, that found als In he house his Of the spine confeive inther his dear to gater: And go agonst Marcito, I'll wid my countery, I way, lientifirn tenving rulby us my follow honour yield stent poon, Jufe the be dared on the kial je: The day my Lounges, be agains in have once as to plating exvage of his tonake That themn were by to the hance, The sold long, po somebn o'er becelds Is this ofseding on this soak? alrick. UMISmed! HO, answer Off Humbledy, that's will forted yial with pring's lord. Forth, Jolsn'd ladib tod But thy shorly be this mine stons. Good you withnlieds think, this mance and thingn blunge his of be be reep steep your intent for thou way, the nober, and visy From the pot of lord? Mast all to be endought: what my loness, Tis is monius and from out of Sunscoa may, And not my the see to all, everstrer. KING RICHARD III: My hidoner, and strangems, honours, Before requick? ELIFFOLLY: O, ce, but and 't: her I near afta humbhal gittled here, O tAlker of off it dispuised here the heam we froens, Wasce, not he rese that dear'd, to, And in stay be I have will am gove his derefy: lade them brooks it in The newly generated text contains more word-like characters and resembles the style of Shakespeare, with a more significant proportion of correctly spelled words.\nRevisiting Attention sequence = \"\"\"MENENIUS:\\nWhat is gra\"\"\" token_embeddings = model.token_embedding_table.weight position_embeddings = model.position_embedding_table.weight tokens = torch.tensor([stoi [c] for c in sequence]) positions = torch.tensor([i for i in range(len(sequence))]) embed = token_embeddings[tokens] + position_embeddings[positions] # query and vector weights for last head of the last block q = model.blocks[5].attn.heads[7].query k = model.blocks[5].attn.heads[7].key v = model.blocks[5].attn.heads[7].value # query and key space with torch.no_grad(): Q = q(embed) K = k(embed) score = Q @ K.T score /= math.sqrt(d_model // h) mask = torch.tril(torch.ones(embed.shape[0], embed.shape[0])).to(device) score = score.masked_fill(mask == 0, float(\"-inf\")) score = F.softmax(score, dim=-1) V = v(embed) new_embed = score @ V print(f\"Attention scores for the sequence:\\n {score[-1, :]}\") print(f\"Adjusted and compressed embeddings for the sequence:\\n {new_embed}\") Attention scores for the sequence: tensor([1.0275e-01, 6.3248e-03, 1.2576e-02, 7.7688e-04, 1.2232e-03, 1.0114e-01, 5.6094e-03, 1.2616e-01, 1.0319e-01, 1.5049e-01, 4.3153e-02, 4.5383e-03, 9.5087e-03, 4.0352e-03, 1.8735e-01, 1.6233e-03, 1.2997e-01, 5.6082e-03, 2.0060e-05, 3.1588e-04, 3.6474e-03], device='cuda:0') Adjusted and compressed embeddings for the sequence: tensor([[ 3.8285e-01, -5.6125e-01, -1.2138e+00, -5.2913e-01, 9.2973e-01, -4.2545e-01, -2.4848e+00, 6.8524e-03], [ 3.6920e-01, -5.4390e-01, -9.2868e-01, -4.8776e-01, 8.3288e-01, -3.3776e-01, -2.2440e+00, 1.1147e-01], [ 4.0217e-01, -4.6048e-01, 8.1029e-01, -1.8336e-01, 2.0483e-01, 2.0015e-01, -5.8833e-01, 7.3810e-01], [ 1.4945e+00, -6.4184e-01, -2.4202e-01, 1.0156e-01, 2.0985e-01, -1.5870e-01, 5.5549e-02, 3.1318e-01], [ 3.5607e-01, -5.5418e-02, 1.5003e+00, -2.7288e-01, -8.2235e-02, 1.1763e-01, -7.0800e-01, 1.2626e+00], [-6.7275e-02, -1.2190e+00, -1.7885e-01, 2.6792e-01, -2.2870e-01, -8.5028e-01, 3.4890e-01, 1.3680e-01], [ 1.7077e+00, -8.5935e-01, -6.5319e-01, 1.4917e-01, 2.7577e-01, -2.7634e-01, 3.0645e-01, 3.7927e-02], [-5.1688e-02, -8.2619e-01, 8.0506e-02, 2.1702e-01, -1.6939e-02, -6.4278e-01, 1.9751e-01, 8.1660e-02], [-7.1723e-02, -5.1926e-01, -2.9651e-01, -5.3577e-02, 1.8432e-01, -4.6867e-01, -7.6557e-01, -1.7440e-01], [ 8.1420e-01, -4.1661e-01, 1.0995e+00, -3.2608e-01, 2.8869e-02, 2.2275e-02, -5.5174e-02, 8.0169e-01], [ 2.9553e-01, -5.1129e-01, 2.6954e-01, -1.0131e-01, 1.6535e-03, -2.3739e-01, 2.4023e-01, 2.7450e-03], [ 9.2807e-01, -5.3834e-01, -4.8175e-01, -1.7232e-02, 1.6207e-01, -1.7096e-01, 3.0736e-01, -9.1554e-02], [ 4.2730e-01, 6.4469e-01, 8.8334e-01, 4.4953e-01, -3.0363e-01, 1.3055e-01, 1.1382e+00, -6.6804e-01], [ 8.4047e-01, -4.7317e-01, -6.5326e-02, -5.7882e-02, 1.3698e-01, -1.0259e-01, 2.5059e-01, 6.1572e-02], [ 2.9731e-01, -8.2256e-01, -2.8259e-02, 3.3942e-01, -2.8240e-01, 1.9379e-01, -9.6743e-02, 2.3589e-01], [ 6.0484e-01, -1.0521e-01, 2.7202e-01, 2.2309e-01, -6.7768e-01, 2.5342e-01, -4.1722e-01, 8.2589e-02], [ 4.1097e-01, 6.0131e-01, 8.3584e-01, 4.4749e-01, -2.8864e-01, 1.3370e-01, 1.0975e+00, -6.3150e-01], [-5.1310e-01, -3.5065e-01, -1.4606e-01, 4.4343e-01, 2.1451e-01, 7.1118e-02, -1.8510e-02, 6.4416e-01], [ 1.3922e-01, -5.7186e-02, -2.0533e-01, -2.0123e-01, -2.3971e-01, 2.8392e-01, -2.8814e-01, 3.0751e-01], [ 3.3605e-01, 5.6808e-01, 8.5728e-01, 3.2310e-01, -3.3082e-01, 1.1003e-01, 1.1402e+00, -6.2344e-01], [ 1.1078e-01, -2.0579e-02, -1.6989e-01, -8.3665e-02, -1.2148e-02, 5.8077e-02, -3.4206e-01, 3.3760e-01]], device='cuda:0') Notes Here are some tiny differences between my code and the code in the video.\nI applied layer normalization after the self-attention layer, while he applied immediately on x before x entered the self-attention and feed-forward layers. class Block(nn.Module): \"\"\" Transformer block: communication followed by computation \"\"\" def __init__(self, n_embd, n_head): # n_embd: embedding dimension, n_head: the number of heads we'd like super().__init__() head_size = n_embd // n_head self.sa = MultiHeadAttention(n_head, head_size) self.ffwd = FeedFoward(n_embd) self.ln1 = nn.LayerNorm(n_embd) self.ln2 = nn.LayerNorm(n_embd) def forward(self, x): x = x + self.sa(self.ln1(x)) x = x + self.ffwd(self.ln2(x)) return x The scaling factor I used was $d_k$ instead of $d_model$ (maybe it’s a typo in his code?). class Head(nn.Module): \"\"\" one head of self-attention \"\"\" def __init__(self, d_k): super().__init__() self.key = nn.Linear(n_embd, head_size, bias=False) self.query = nn.Linear(n_embd, head_size, bias=False) self.value = nn.Linear(n_embd, head_size, bias=False) self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) self.dropout = nn.Dropout(dropout) def forward(self, x): B, T, C = x.shape # batch_size, block_size, n_embd k = self.key(x) # (B,T,C) q = self.query(x) # (B,T,C) # compute attention scores (\"affinities\") wei = q @ k.transpose(-2,-1) * C **-0.5 # (B, T, C) @ (B, C, T) -\u003e (B, T, T) wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) wei = F.softmax(wei, dim=-1) # (B, T, T) wei = self.dropout(wei) # perform the weighted aggregation of the values v = self.value(x) # (B,T,C) out = wei @ v # (B, T, T) @ (B, T, C) -\u003e (B, T, C) return out Other Resources https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html https://jalammar.github.io/illustrated-transformer/ https://www.youtube.com/watch?v=ptuGllU5SQQ\u0026list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ\u0026index=9 https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/ ","wordCount":"5967","inLanguage":"en","datePublished":"2023-03-15T00:00:00Z","dateModified":"2023-03-15T00:00:00Z","author":{"@type":"Person","name":"Gejun Zhu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://gejun.name/natural-language-processing/understanding-attention-nlp/"},"publisher":{"@type":"Organization","name":"Gejun's Blog","logo":{"@type":"ImageObject","url":"https://gejun.name/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gejun.name accesskey=h title="Gejun's Blog (Alt + H)">Gejun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gejun.name/archives title=Archive><span>Archive</span></a></li><li><a href=https://gejun.name/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://gejun.name/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://gejun.name/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gejun.name>Home</a>&nbsp;»&nbsp;<a href=https://gejun.name/natural-language-processing/>Natural-language-processings</a></div><h1 class=post-title>Understanding Transformer Architecture by Building GPT</h1><div class=post-meta><span title='2023-03-15 00:00:00 +0000 UTC'>March 15, 2023</span>&nbsp;·&nbsp;29 min&nbsp;·&nbsp;Gejun Zhu&nbsp;|&nbsp;<a href=https://github.com/zhugejun/zhugejun.github.io/tree/main/content/natural-language-processing/understanding-attention-nlp/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#data-preparation aria-label="Data Preparation">Data Preparation</a><ul><li><a href=#training-data aria-label="Training Data">Training Data</a></li></ul></li><li><a href=#bigramlanguagemodel aria-label=BigramLanguageModel>BigramLanguageModel</a><ul><li><a href=#base-model aria-label="Base model">Base model</a></li><li><a href=#training aria-label=Training>Training</a></li></ul></li><li><a href=#transformer-architecture aria-label="Transformer Architecture">Transformer Architecture</a></li><li><a href=#positional-embedding aria-label="Positional Embedding">Positional Embedding</a></li><li><a href=#attention aria-label=Attention>Attention</a><ul><li><a href=#dot-product aria-label="Dot Product">Dot Product</a></li><li><a href=#attention-score aria-label="Attention Score">Attention Score</a></li><li><a href=#masking aria-label=Masking>Masking</a></li><li><a href=#weighted-sum aria-label="Weighted Sum">Weighted Sum</a></li><li><a href=#demystifying-qkv aria-label="Demystifying QKV">Demystifying QKV</a></li><li><a href=#multi-head-attention aria-label="Multi-head Attention">Multi-head Attention</a></li><li><a href=#dropout aria-label=Dropout>Dropout</a></li><li><a href=#residual-connection aria-label="Residual Connection">Residual Connection</a></li><li><a href=#feed-forward aria-label=Feed-Forward>Feed-Forward</a></li><li><a href=#layer-normalization aria-label="Layer Normalization">Layer Normalization</a></li><li><a href=#refactoring aria-label=Refactoring>Refactoring</a></li></ul></li><li><a href=#put-everything-together aria-label="Put Everything Together">Put Everything Together</a></li><li><a href=#retraining aria-label=Retraining>Retraining</a></li><li><a href=#revisiting-attention aria-label="Revisiting Attention">Revisiting Attention</a></li><li><a href=#notes aria-label=Notes>Notes</a></li><li><a href=#other-resources aria-label="Other Resources">Other Resources</a></li></ul></div></details></div><div class=post-content><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube-nocookie.com/embed/kCc8FmEb1nY style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>In <a href=https://gejun.name/natural-language-processing/building-makemore-mlp/>Part2</a>, we constructed a straightforward MLP model to generate characters based on 32k popular names.
In this lecture, <a href=https://karpathy.ai>Andrej</a> guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model.
We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.</p><h2 id=data-preparation>Data Preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h2><p>Let&rsquo;s first import the necessary libraries and get the data ready.
We will use the tiny shakespeare dataset, featured in Andrej Karpathy&rsquo;s blog post <a href=http://karpathy.github.io/2015/05/21/rnn-effectiveness/>The Unreasonable Effectiveness of Recurrent Neural Networks</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>math</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>data_url</span> <span class=o>=</span> <span class=s2>&#34;https://t.ly/u1Ax&#34;</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>data_url</span><span class=p>)</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># building vocabulary</span>
</span></span><span class=line><span class=cl><span class=n>chars</span> <span class=o>=</span> <span class=nb>sorted</span><span class=p>(</span><span class=nb>list</span><span class=p>(</span><span class=nb>set</span><span class=p>(</span><span class=n>text</span><span class=p>)))</span>
</span></span><span class=line><span class=cl><span class=n>vocab_size</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>chars</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Vocabulary size: </span><span class=si>{</span><span class=n>vocab_size</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Vocabulary: </span><span class=si>{</span><span class=nb>repr</span><span class=p>(</span><span class=s1>&#39;&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>chars</span><span class=p>))</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># mappings</span>
</span></span><span class=line><span class=cl><span class=n>stoi</span> <span class=o>=</span> <span class=p>{</span><span class=n>c</span><span class=p>:</span> <span class=n>i</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>c</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>chars</span><span class=p>)}</span>
</span></span><span class=line><span class=cl><span class=n>itos</span> <span class=o>=</span> <span class=p>{</span><span class=n>v</span><span class=p>:</span> <span class=n>k</span> <span class=k>for</span> <span class=n>k</span><span class=p>,</span> <span class=n>v</span> <span class=ow>in</span> <span class=n>stoi</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>encode</span><span class=p>(</span><span class=n>s</span><span class=p>):</span> <span class=k>return</span> <span class=p>[</span><span class=n>stoi</span><span class=p>[</span><span class=n>c</span><span class=p>]</span> <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>s</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>decode</span><span class=p>(</span><span class=n>l</span><span class=p>):</span> <span class=k>return</span> <span class=s1>&#39;&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>([</span><span class=n>itos</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>l</span><span class=p>])</span>
</span></span></code></pre></div><pre><code>Vocabulary size: 65
Vocabulary: &quot;\n !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&quot;
</code></pre><p>We have 65 characters, including all lower- and upper-case letters and a few special characters, <code>\n !$&',-.3:;?</code>.
Next, we split the data into two parts: 90% of the dataset for training and 10% for validation.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># create tensor</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>encode</span><span class=p>(</span><span class=n>text</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>n</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=mf>0.9</span><span class=o>*</span><span class=nb>len</span><span class=p>(</span><span class=n>data</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>train_data</span> <span class=o>=</span> <span class=n>data</span><span class=p>[:</span><span class=n>n</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>val_data</span> <span class=o>=</span> <span class=n>data</span><span class=p>[</span><span class=n>n</span><span class=p>:]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>train_data</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>val_data</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>torch.Size([1003854])
torch.Size([111540])
</code></pre><h3 id=training-data>Training Data<a hidden class=anchor aria-hidden=true href=#training-data>#</a></h3><p>Feeding the entire text to the transformer all at once can be computationally expensive and prohibitive.
To address this issue, neural network models use batch processing techniques to update the model&rsquo;s weights and biases.
This technique involves dividing the training dataset into smaller subsets, or batches, of size <code>batch_size</code>.
The batches are then processed separately by the neural network to update the model&rsquo;s parameters.
For a character generation model, we need a sequence of characters as our training sample, which can be considered a time dimension.
For the sample below, the input is <code>[18]</code> and the target is <code>47</code> at time 0, and the input is <code>[18, 47]</code> and the target is <code>56</code>, and so on.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>block_size</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=n>x</span> <span class=o>=</span> <span class=n>train_data</span><span class=p>[:</span><span class=n>block_size</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>train_data</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=n>block_size</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>block_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>context</span> <span class=o>=</span> <span class=n>x</span><span class=p>[:</span><span class=n>t</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>target</span> <span class=o>=</span> <span class=n>y</span><span class=p>[</span><span class=n>t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Time: </span><span class=si>{</span><span class=n>t</span><span class=si>}</span><span class=s2>, input: </span><span class=si>{</span><span class=n>context</span><span class=si>}</span><span class=s2>, target: </span><span class=si>{</span><span class=n>target</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Time: 0, input: tensor([18]), target: 47
Time: 1, input: tensor([18, 47]), target: 56
Time: 2, input: tensor([18, 47, 56]), target: 57
Time: 3, input: tensor([18, 47, 56, 57]), target: 58
Time: 4, input: tensor([18, 47, 56, 57, 58]), target: 1
Time: 5, input: tensor([18, 47, 56, 57, 58,  1]), target: 15
Time: 6, input: tensor([18, 47, 56, 57, 58,  1, 15]), target: 47
Time: 7, input: tensor([18, 47, 56, 57, 58,  1, 15, 47]), target: 58
</code></pre><p>To create our training data, we select a sequence starting from the character of a fixed size <code>block_size</code> in each batch.
We then create our input and target along the time dimension inside each sequence, resulting in <code>batch_size</code> time <code>block_size</code> training examples.
The example below shows that there are $4\times 8=32$ training examples in each batch as we have 4 sequences of 8 characters each.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>block_size</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_batch</span><span class=p>(</span><span class=n>split</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>train_data</span> <span class=k>if</span> <span class=n>split</span> <span class=o>==</span> <span class=s2>&#34;train&#34;</span> <span class=k>else</span> <span class=n>val_data</span>
</span></span><span class=line><span class=cl>    <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>data</span><span class=p>)</span> <span class=o>-</span> <span class=n>block_size</span><span class=p>,</span> <span class=p>(</span><span class=n>batch_size</span><span class=p>,))</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>([</span><span class=n>data</span><span class=p>[</span><span class=n>i</span><span class=p>:</span><span class=n>i</span><span class=o>+</span><span class=n>block_size</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>stack</span><span class=p>([</span><span class=n>data</span><span class=p>[</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=p>:</span><span class=n>i</span><span class=o>+</span><span class=n>block_size</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>),</span> <span class=n>y</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=o>=</span> <span class=n>get_batch</span><span class=p>(</span><span class=s2>&#34;train&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>x_batch</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y_batch</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>b</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>batch_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;---------- Batch </span><span class=si>{</span><span class=n>b</span><span class=si>}</span><span class=s2> ----------&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>block_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>context</span> <span class=o>=</span> <span class=n>x_batch</span><span class=p>[</span><span class=n>b</span><span class=p>,</span> <span class=p>:</span><span class=n>t</span><span class=o>+</span><span class=mi>1</span><span class=p>]</span> 
</span></span><span class=line><span class=cl>        <span class=n>target</span> <span class=o>=</span> <span class=n>y_batch</span><span class=p>[</span><span class=n>b</span><span class=p>,</span> <span class=n>t</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Time: </span><span class=si>{</span><span class=n>t</span><span class=si>}</span><span class=s2>, input: </span><span class=si>{</span><span class=n>context</span><span class=si>}</span><span class=s2>, target: </span><span class=si>{</span><span class=n>target</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>torch.Size([4, 8]) torch.Size([4, 8])
---------- Batch 0 ----------
Time: 0, input: tensor([53], device='cuda:0'), target: 56
Time: 1, input: tensor([53, 56], device='cuda:0'), target: 58
Time: 2, input: tensor([53, 56, 58], device='cuda:0'), target: 46
Time: 3, input: tensor([53, 56, 58, 46], device='cuda:0'), target: 11
Time: 4, input: tensor([53, 56, 58, 46, 11], device='cuda:0'), target: 1
Time: 5, input: tensor([53, 56, 58, 46, 11,  1], device='cuda:0'), target: 41
Time: 6, input: tensor([53, 56, 58, 46, 11,  1, 41], device='cuda:0'), target: 53
Time: 7, input: tensor([53, 56, 58, 46, 11,  1, 41, 53], device='cuda:0'), target: 51
---------- Batch 1 ----------
Time: 0, input: tensor([52], device='cuda:0'), target: 52
Time: 1, input: tensor([52, 52], device='cuda:0'), target: 53
Time: 2, input: tensor([52, 52, 53], device='cuda:0'), target: 58
Time: 3, input: tensor([52, 52, 53, 58], device='cuda:0'), target: 1
Time: 4, input: tensor([52, 52, 53, 58,  1], device='cuda:0'), target: 46
Time: 5, input: tensor([52, 52, 53, 58,  1, 46], device='cuda:0'), target: 47
Time: 6, input: tensor([52, 52, 53, 58,  1, 46, 47], device='cuda:0'), target: 58
Time: 7, input: tensor([52, 52, 53, 58,  1, 46, 47, 58], device='cuda:0'), target: 1
---------- Batch 2 ----------
Time: 0, input: tensor([35], device='cuda:0'), target: 43
Time: 1, input: tensor([35, 43], device='cuda:0'), target: 56
Time: 2, input: tensor([35, 43, 56], device='cuda:0'), target: 58
Time: 3, input: tensor([35, 43, 56, 58], device='cuda:0'), target: 1
Time: 4, input: tensor([35, 43, 56, 58,  1], device='cuda:0'), target: 58
Time: 5, input: tensor([35, 43, 56, 58,  1, 58], device='cuda:0'), target: 46
Time: 6, input: tensor([35, 43, 56, 58,  1, 58, 46], device='cuda:0'), target: 53
Time: 7, input: tensor([35, 43, 56, 58,  1, 58, 46, 53], device='cuda:0'), target: 59
---------- Batch 3 ----------
Time: 0, input: tensor([53], device='cuda:0'), target: 59
Time: 1, input: tensor([53, 59], device='cuda:0'), target: 50
Time: 2, input: tensor([53, 59, 50], device='cuda:0'), target: 42
Time: 3, input: tensor([53, 59, 50, 42], device='cuda:0'), target: 1
Time: 4, input: tensor([53, 59, 50, 42,  1], device='cuda:0'), target: 41
Time: 5, input: tensor([53, 59, 50, 42,  1, 41], device='cuda:0'), target: 46
Time: 6, input: tensor([53, 59, 50, 42,  1, 41, 46], device='cuda:0'), target: 53
Time: 7, input: tensor([53, 59, 50, 42,  1, 41, 46, 53], device='cuda:0'), target: 54
</code></pre><h2 id=bigramlanguagemodel>BigramLanguageModel<a hidden class=anchor aria-hidden=true href=#bigramlanguagemodel>#</a></h2><p>Let&rsquo;s rewrite our previous bigram model.
Here is the main part of the model we built in <a href=https://gejun.name/natural-language-processing/building-makemore/>Part 1</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>W</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>27</span><span class=p>,</span> <span class=mi>27</span><span class=p>),</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>xenc</span> <span class=o>@</span> <span class=n>W</span> 
</span></span><span class=line><span class=cl><span class=n>counts</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>probs</span> <span class=o>=</span> <span class=n>counts</span> <span class=o>/</span> <span class=n>counts</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=base-model>Base model<a hidden class=anchor aria-hidden=true href=#base-model>#</a></h3><p>From <a href=https://gejun.name/natural-language-processing/building-makemore-mlp/>Part 2</a>, we learned how to represent a token with a fixed-length, real-valued, and learnable vector, which is known as token embedding.
The embedding matrix can be initialized by <a href=https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html><code>nn.Embedding</code></a> where <code>num_embeddings</code> refers to the vocabulary size, and <code>embedding_dim</code> refers to the length of the feature vector.
For consistency with the original paper, we will use <code>d_model</code> to represent the feature vector&rsquo;s length, which will be set to 64 instead of the vocabulary size.
As a result, we need to create another linear layer to ensure that the output dimension is the same as the vocabulary size.</p><p>It&rsquo;s worth noting that we cannot compute the cross-entropy for a 3-dimensional matrix, as seen from the <a href=https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html>documentation</a> of <code>cross_entropy</code> function.
Therefore, we need to reshape the logits and targets before computing it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># B: batch_size</span>
</span></span><span class=line><span class=cl><span class=c1># T: time, up to block_size</span>
</span></span><span class=line><span class=cl><span class=c1># C: d_model</span>
</span></span><span class=line><span class=cl><span class=c1># 65: vocabulary size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BigramLanguageModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span> <span class=c1># 65, C</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>            <span class=c1># C, 65</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=n>targets</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># idx: B, T</span>
</span></span><span class=line><span class=cl>        <span class=n>embedded</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span> <span class=c1># B, T, C</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span><span class=p>(</span><span class=n>embedded</span><span class=p>)</span>      <span class=c1># B, T, 65</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># there is no target when predicting</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>targets</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=o>*</span><span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span> <span class=c1># N, C</span>
</span></span><span class=line><span class=cl>            <span class=n>targets</span> <span class=o>=</span> <span class=n>targets</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=o>*</span><span class=n>T</span><span class=p>)</span>  <span class=c1># N</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span><span class=p>,</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=n>max_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># focus on the char on last time stamp because it&#39;s a bigram model</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span> <span class=c1># B, C</span>
</span></span><span class=line><span class=cl>            <span class=n>probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>idx_next</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=c1># concatenate the new generated to the old ones</span>
</span></span><span class=line><span class=cl>            <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>idx</span><span class=p>,</span> <span class=n>idx_next</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>idx</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>base_model</span> <span class=o>=</span> <span class=n>BigramLanguageModel</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>decode</span><span class=p>(</span><span class=n>base_model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>idx</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span><span class=o>.</span><span class=n>tolist</span><span class=p>()))</span>
</span></span></code></pre></div><pre><code>dF3unFC;RnXbzDP'CnT-P.lBuYkUWdXRaRnqDCk,b!:UE$J,uuheZqKPXEPYMYSAxKlRpvwisS.MIwITP$YqrgGRpP.AwYluRWGI
</code></pre><p>Certainly, the 100 characters generated at this point are not meaningful as the model has not been trained yet.</p><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>base_model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># training</span>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=o>=</span> <span class=n>get_batch</span><span class=p>(</span><span class=s2>&#34;train&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>,</span> <span class=n>loss</span> <span class=o>=</span> <span class=n>base_model</span><span class=p>(</span><span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>(</span><span class=n>set_to_none</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>epoch</span> <span class=o>%</span> <span class=mi>1000</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>or</span> <span class=n>epoch</span> <span class=o>==</span> <span class=n>epochs</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># starting with [[0]]</span>
</span></span><span class=line><span class=cl><span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>decode</span><span class=p>(</span><span class=n>base_model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>idx</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>100</span><span class=p>)</span><span class=o>.</span><span class=n>squeeze</span><span class=p>()</span><span class=o>.</span><span class=n>tolist</span><span class=p>()))</span>
</span></span></code></pre></div><pre><code>Epoch 0: 4.440201282501221
Epoch 1000: 2.5844924449920654
Epoch 2000: 2.469000816345215
Epoch 3000: 2.473245859146118
Epoch 4000: 2.4555399417877197
Epoch 5000: 2.5115771293640137
Epoch 6000: 2.3323276042938232
Epoch 7000: 2.331480026245117
Epoch 8000: 2.436919927597046
Epoch 9000: 2.473867893218994
Epoch 9999: 2.636822462081909

Tody inde eve d tlakemang yofowhas

Thind.
UCESer ur thathapr me machan fl haisu d iere--sthurore ce
</code></pre><p>The generated characters appear more word-like than before, but most are misspelled because the bigram model only generates a new character based on the last generated character.
To improve our model&rsquo;s performance, we need a way to incorporate information from previously generated characters up to <code>block_size</code>.
One solution is to use a bag-of-words model to extract features from previously generated characters.
In a bag-of-words model, a text is treated as a bag of tokens, disregarding grammar and order.
In the next section, we will introduce the transformer architecture from the classic paper, <a href=https://arxiv.org/pdf/1706.03762.pdf>Attention is all you need</a>.
We will explain what attention is, how to calculate, and most importantly, how to understand it intuitively.
Furthermore, we will implement it step by step and see how it improves our model&rsquo;s performance.</p><h2 id=transformer-architecture>Transformer Architecture<a hidden class=anchor aria-hidden=true href=#transformer-architecture>#</a></h2><p>The transformer model architecture from the paper is shown below.</p><img src=transformer-architecture.jpg class=quarto-discovered-preview-image alt=encoder-decoder-architecture width=50%><p>Let&rsquo;s first clarify what an encoder is.
According to the paper:</p><blockquote><p>&ldquo;The encoder maps an input sequence of symbol representations $(x_1, &mldr;, x_n)$ to a sequence of continuous representations $z=(z_1, &mldr;, z_n)$.
It converts an input sequence of tokens into a sequence of embedding vectors, often called a hidden state.
The encoder is composed of a stack of encoder layers, which are used to update the input embeddings to produce representations that encode some contextual information in the sequence.&rdquo;</p></blockquote><p>In the transformer architecture shown above, the encoder is on the left side inside the blue box, and it contains multiple encoder layers.
The encoder compresses and extracts important information from the input sequence while discarding the irrelevant information.</p><p>Next, let&rsquo;s see what a decoder is.
The decoder is inside the red box on the right side of the transformer architecture.
It is also composed of a stack of decoder layers, which are similar to encoder layers except that they add an extra masked layer in the multi-head attention.</p><p>Last but not least, the state generated from the encoder is passed to the decoder and generates the output sequence, which is referred to as cross-attention.
A decoder uses the encoder&rsquo;s hidden state to iteratively generate an output sequence of tokens, one at a time.</p><p>GPT, which stands for Generative Pretrained Transformer, focuses on the decoder part.
Therefore, our model architecture becomes the following.</p><img src=GPT.jpg class=quarto-discovered-preview-image alt=gpt-architecture width=50%><p>In the next few sections, we will build the model from bottom to top.
Since the input embedding stays the same, we will skip the input embedding section and talk about positional embedding.</p><h2 id=positional-embedding>Positional Embedding<a hidden class=anchor aria-hidden=true href=#positional-embedding>#</a></h2><p>The embedding of input tokens alone does not capture any information about their relative positions within the sequence.
Hence a positional embedding is introduced to inject this information.
According to the paper, there are multiple ways for positional embeddings, with some being fixed while others are learnable.
For our implementation, we will use a learnable positional embedding with the same dimension as the token embedding, which is <code>d_model</code>.
The num_embeddings parameter in the <code>nn.Embedding</code> function will be set to <code>block_size</code> since our training sequence has a maximum length of <code>block_size</code>.</p><p>Let&rsquo;s dive into the dimensions of the input tokens.
The input tokens have two dimensions: the batch dimension, which indicates how many independent sequences the model processes in parallel, and the time dimension, which records the current position within the sequence up to a maximum length of <code>block_size</code>.
After the input tokens pass through the token and positional embedding layers, they will have an additional channel dimension, which is a convention borrowed from computer vision.
For simplicity, we will use <code>B</code>, <code>T</code>, and <code>C</code> to denote the batch, time, and channel dimensions, respectively.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>BigramLanguageModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># position embedding table</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>block_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=n>targets</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># idx: B, T</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>T</span> <span class=o>=</span> <span class=n>idx</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=n>token_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span>     <span class=c1># B, T, C</span>
</span></span><span class=line><span class=cl>        <span class=n>posit_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding_table</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>))</span>  <span class=c1># T, C</span>
</span></span><span class=line><span class=cl>        <span class=c1># sum of token and positional embeddings </span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>token_embed</span> <span class=o>+</span> <span class=n>posit_embed</span>              <span class=c1># B, T, C</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>             <span class=c1># B, T, vocab_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>targets</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=o>*</span><span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span> <span class=c1># (N, C)</span>
</span></span><span class=line><span class=cl>            <span class=n>targets</span> <span class=o>=</span> <span class=n>targets</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=o>*</span><span class=n>T</span><span class=p>)</span>  <span class=c1># (N)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span><span class=p>,</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>base_model</span> <span class=o>=</span> <span class=n>BigramLanguageModel</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>base_model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=mf>1e-3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=o>=</span> <span class=n>get_batch</span><span class=p>(</span><span class=s2>&#34;train&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>,</span> <span class=n>loss</span> <span class=o>=</span> <span class=n>base_model</span><span class=p>(</span><span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>(</span><span class=n>set_to_none</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>epoch</span> <span class=o>%</span> <span class=mi>1000</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>or</span> <span class=n>epoch</span> <span class=o>==</span> <span class=n>epochs</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epoch </span><span class=si>{</span><span class=n>epoch</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Epoch 0: 4.435860633850098
Epoch 1000: 2.538156270980835
Epoch 2000: 2.5488555431365967
Epoch 3000: 2.479320764541626
Epoch 4000: 2.3083598613739014
Epoch 5000: 2.472010850906372
Epoch 6000: 2.5080037117004395
Epoch 7000: 2.4842913150787354
Epoch 8000: 2.3710641860961914
Epoch 9000: 2.4978179931640625
Epoch 9999: 2.416473627090454
</code></pre><h2 id=attention>Attention<a hidden class=anchor aria-hidden=true href=#attention>#</a></h2><p>What is attention?</p><blockquote><p>&ldquo;An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.&rdquo;</p></blockquote><p><img loading=lazy src=attention-multi-head.png alt=attention-multihead></p><p>We can compute the attention score using the following steps as described in the paper.</p><p>$$Attention(Q,K,V)=softmax\bigl( \frac{QK^T}{\sqrt{d_k}}\bigr) V$$</p><p>To better understand the attention formula above, it&rsquo;s helpful to review some linear algebra concepts.</p><h3 id=dot-product>Dot Product<a hidden class=anchor aria-hidden=true href=#dot-product>#</a></h3><p>The <a href=https://www.wikiwand.com/en/Dot_product>dot product</a> of two Euclidean vectors $\vec{a}$ and $\vec{b}$ is defined by</p><p>$$\vec{a} \cdot \vec{b} = \sum_{i=1}^n a_ib_i$$</p><p>where $n$ is the length of the vectors.</p><p>Geometrically, the dot product of two vectors is equal to the product of their magnitudes and the cosine of the angle between them.
Specifically, if $\theta$ is the angle between $\vec{a}$ and $\vec{b}$, then</p><p>$$\vec{a} \cdot \vec{b} = |a| \cdot |b| cos(\theta)$$</p><p><img loading=lazy src=320px-Dot_Product.png alt=dot-product-projection>
<em><a href=https://www.wikiwand.com/en/Dot_product>source</a></em></p><p>The quantity $|a|cos(\theta)$ is the scalar projection of $\vec{a}$ onto $\vec{b}$.
The higher the product, the more similar two vectors.
Let&rsquo;s take the learned embedding from our last model and compute the dot products of some tokens from our vocabulary.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>char1</span> <span class=o>=</span> <span class=s1>&#39;a&#39;</span>
</span></span><span class=line><span class=cl><span class=n>char2</span> <span class=o>=</span> <span class=s1>&#39;z&#39;</span>
</span></span><span class=line><span class=cl><span class=n>char3</span> <span class=o>=</span> <span class=s1>&#39;e&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>token_embeddings</span> <span class=o>=</span> <span class=n>base_model</span><span class=o>.</span><span class=n>token_embedding_table</span><span class=o>.</span><span class=n>weight</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>calc_dp</span><span class=p>(</span><span class=n>char1</span><span class=p>,</span> <span class=n>char2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>        <span class=n>embed1</span> <span class=o>=</span> <span class=n>token_embeddings</span><span class=p>[</span><span class=n>stoi</span><span class=p>[</span><span class=n>char1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>        <span class=n>embed2</span> <span class=o>=</span> <span class=n>token_embeddings</span><span class=p>[</span><span class=n>stoi</span><span class=p>[</span><span class=n>char2</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>sum</span><span class=p>(</span><span class=n>embed1</span> <span class=o>*</span> <span class=n>embed2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Dot product of </span><span class=si>{</span><span class=n>char1</span><span class=si>}</span><span class=s2> and </span><span class=si>{</span><span class=n>char1</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>calc_dp</span><span class=p>(</span><span class=n>char1</span><span class=p>,</span> <span class=n>char1</span><span class=p>)</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Dot product of </span><span class=si>{</span><span class=n>char1</span><span class=si>}</span><span class=s2> and </span><span class=si>{</span><span class=n>char2</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>calc_dp</span><span class=p>(</span><span class=n>char1</span><span class=p>,</span> <span class=n>char2</span><span class=p>)</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Dot product of </span><span class=si>{</span><span class=n>char1</span><span class=si>}</span><span class=s2> and </span><span class=si>{</span><span class=n>char3</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>calc_dp</span><span class=p>(</span><span class=n>char1</span><span class=p>,</span> <span class=n>char3</span><span class=p>)</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Dot product of a and a: 78.494980
Dot product of a and z: -14.060809
Dot product of a and e: 12.071777
</code></pre><p>The dot product of the feature vectors of <code>a</code> and itself is much higher than with <code>e</code> or <code>z</code>.
Also, the results show that <code>a</code> is more similar to <code>e</code> then to <code>z</code>.</p><h3 id=attention-score>Attention Score<a hidden class=anchor aria-hidden=true href=#attention-score>#</a></h3><p>Every token in the input sequence generates a query vector and a key vector of the same dimension.
This operation is called <strong>self-attention</strong> because $Q$, $V$, and $T$ are all derived from the same source in GPT.
The dot product of the query and key vectors measures their similarity.</p><p>Let $X_{m\times n}$ and $W$ denote the embedding matrix of the input sequence and the weight of the linear transformation, where $m$ is the number of tokens, $n$ is the token dimension, and $k$ is the output dimension of the linear transformation or the head size of our attention.
Each row represents the token embedding for each token in the input.
Then, we apply three linear transformations on $X$ to project it onto 3 new vector spaces:</p><ul><li>$X_{m\times n} \cdot W^Q_{n\times k} = Q_{m\times k}$ to obtain the query space.</li><li>$X_{m\times n} \cdot W^K_{n\times k} = K_{m\times k}$ to obtain the key space.</li><li>$X_{m\times n} \cdot W^V_{n\times k} = V_{m\times k}$ to obtain the value space.</li></ul><p>$Q\cdot K^T$ is the attention score matrix, having a shape of $m \times m$.
The larger the value, the closer the vectors and hence the more attention.</p><p>Let&rsquo;s take the learned token and positional embeddings from our previous model, apply the query and key transformations, and calculate the attention scores of the sequence <code>sea</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sequence</span> <span class=o>=</span> <span class=s2>&#34;sea&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># get positional embeddings from model</span>
</span></span><span class=line><span class=cl><span class=n>position_embeddings</span> <span class=o>=</span> <span class=n>base_model</span><span class=o>.</span><span class=n>position_embedding_table</span><span class=o>.</span><span class=n>weight</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>stoi</span> <span class=p>[</span><span class=n>c</span><span class=p>]</span> <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>sequence</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>positions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>sequence</span><span class=p>))])</span>
</span></span><span class=line><span class=cl><span class=c1># final embedding matrix for a given sequence</span>
</span></span><span class=line><span class=cl><span class=n>embed</span> <span class=o>=</span> <span class=n>token_embeddings</span><span class=p>[</span><span class=n>tokens</span><span class=p>]</span> <span class=o>+</span> <span class=n>position_embeddings</span><span class=p>[</span><span class=n>positions</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># query and vector weights</span>
</span></span><span class=line><span class=cl><span class=n>d_k</span> <span class=o>=</span> <span class=mi>16</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>q</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>k</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># query and key space</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>Q</span> <span class=o>=</span> <span class=n>q</span><span class=p>(</span><span class=n>embed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span> <span class=o>=</span> <span class=n>k</span><span class=p>(</span><span class=n>embed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># similarity between query and keys</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>Q</span> <span class=o>@</span> <span class=n>K</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([[ 1.5712, -2.8564,  3.0652],
        [ 1.6477,  0.1216, -0.4353],
        [-6.8497, -1.1358, -0.8100]], device='cuda:0')
</code></pre><p>The attention score vector for <code>e</code> is <code>[ 1.6477, 0.1216, -0.4353]</code>
However, the dot products may become too large in magnitude when the head size $d_k$ is large, which can result in extremely small gradients after applying the softmax function.
To mitigate this issue, the scores are scaled by multiplying with the factor $\frac{1}{\sqrt{d_k}}$, as suggested in the paper.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>/=</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([[0.3593, 0.1188, 0.5220],
        [0.4392, 0.2999, 0.2609],
        [0.1031, 0.4302, 0.4667]], device='cuda:0')
</code></pre><p>After scaling, the attention score vector for token <code>e</code> in <code>sea</code> becomes <code>[0.4392, 0.2999, 0.2609]</code>.
This implies that the token <code>s</code> requires more attention than the tokens <code>e</code> and <code>a</code>.</p><p>Wait a minute!
Why does the token <code>e</code> pay attention to the future token <code>a</code> in a GPT model?
It is cheating in this way.
How can we preserve the information from the previous tokens while not peeking the future tokens?
The masking layer.</p><h3 id=masking>Masking<a hidden class=anchor aria-hidden=true href=#masking>#</a></h3><p>Where exactly do we apply a masking layer?
Since we want to use a softmax function to normalize the attention scores until the current position so that the divided attention sums to one, it should be applied after calculating the unscaled attention score and before the softmax layer.
In this way, we can exclude the future tokens.
To implement this masking, we will use a PyTorch built-in function, <code>torch.tril</code>, which preserves the original values for the lower triangular part of the matrix while setting the upper part to zero.
In our case, we replace the scores in the upper triangular part of the matrix with a very small number, such as <code>float("-inf")</code>, so that they will become zeros after applying the softmax function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>embed</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>score</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;-inf&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([[1.0000, 0.0000, 0.0000],
        [0.5348, 0.4652, 0.0000],
        [0.2614, 0.3626, 0.3760]], device='cuda:0')
</code></pre><p>Now, the scaled attention vector for <code>e</code> becomes <code>[0.5348, 0.4652, 0.0000]</code>, indicating that the model pays roughly half of its attention to tokens <code>s</code> and <code>e</code> when it reaches token <code>e</code> while completely ignoring the future token <code>a</code>.</p><h3 id=weighted-sum>Weighted Sum<a hidden class=anchor aria-hidden=true href=#weighted-sum>#</a></h3><p>Finally, we obtain a new adjusted embedding for each token in the context by multiplying the attention matrix with the value matrix $V$.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>v</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>V</span> <span class=o>=</span> <span class=n>v</span><span class=p>(</span><span class=n>embed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>new_embed</span> <span class=o>=</span> <span class=n>score</span> <span class=o>@</span> <span class=n>V</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>new_embed</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor([[ 0.0959,  0.4068, -0.2983,  0.8456, -1.6365,  0.9545, -0.5414,  2.2582,
         -0.3868,  1.1196,  1.6244,  0.3545, -1.1479,  0.4165, -0.7899, -0.7008],
        [ 0.1352, -0.2616, -0.4122,  0.1182, -1.2960,  0.5224, -0.3819,  1.3335,
         -0.1463,  0.2113,  0.8228, -0.0095, -0.8548,  0.0567, -0.5980, -0.3525],
        [-0.4126, -0.4585, -0.2760,  0.0813, -0.9609,  0.2358, -0.3887,  0.7906,
          0.0084, -0.1094,  0.3198, -0.5582, -0.7782,  0.4525, -0.1208,  0.1493]],
       device='cuda:0')
</code></pre><p>To put it in another way, we force the tokens to look at each other by multiplying the attention scores with the value matrix $V$.
This helps to adjust the value matrix to represent the entire sequence better as training progresses.</p><h3 id=demystifying-qkv>Demystifying QKV<a hidden class=anchor aria-hidden=true href=#demystifying-qkv>#</a></h3><p>How do we understand attention from intuition?
Here is a great answer from <a href=https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms>Cross Validated</a>.</p><blockquote><p>The key/value/query concept is analogous to retrieval systems.
For example, when you search for videos on Youtube, the search engine will map your <strong>query</strong>
(text in the search bar) against a set of <strong>keys</strong> (video title, description, etc.) associated
with candidate videos in their database, then present you the best matched videos (<strong>values</strong>).</p></blockquote><p><img loading=lazy src=youtube-search.png alt=youtube-search>
<em><a href="https://www.youtube.com/watch?v=ySEx_Bqxvvo&amp;ab_channel=AlexanderAmini">source</a></em></p><p>Here are the intuitive meaning of these matrices:</p><ul><li>The query matrix represents a piece of information we are looking for in a query we have.</li><li>The key matrix is intuitively meant to represent the relevance of each word to our query. And the key matrix represents how important each word is to my overall query.</li><li>The value matrix intuitively represents the contextless meaning of our input tokens.</li></ul><p>Imagine that you&rsquo;re at the supermarket buying all the ingredients you need for your dinner.
You have the dish&rsquo;s recipe, and the ingredients (query) are what you look for in a supermarket.
Scanning the shelves, you look at the labels (keys) and check whether they match an ingredient on your list.
You are determining the similarity between query and keys.
If you have a match, you take the item (value) from the shelf.</p><p>Let&rsquo;s put the attention layer into a single <code>Head</code> class.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Head</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>query</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=c1># C, d_k</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>key</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>   <span class=c1># C, d_k</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>value</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=c1># C, d_k</span>
</span></span><span class=line><span class=cl>        <span class=c1># not a model parameter</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s1>&#39;tril&#39;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>block_size</span><span class=p>,</span> <span class=n>block_size</span><span class=p>)))</span>   <span class=c1># block_size, block_size</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># B, T, d_k</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>key</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>   <span class=c1># B, T, d_k</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>C</span><span class=p>)</span>         <span class=c1># B, T, T</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=n>score</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tril</span><span class=p>[:</span><span class=n>T</span><span class=p>,</span> <span class=p>:</span><span class=n>T</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;-inf&#34;</span><span class=p>))</span>    <span class=c1># B, T, T</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>                                    <span class=c1># B, T, T</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>value</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>   <span class=c1># B, T, d_k</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>score</span> <span class=o>@</span> <span class=n>v</span>     <span class=c1># (B, T, T)@(B, T, d_k) = (B, T, d_k)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span></code></pre></div><p>To ensure compatibility with matrix multiplication, we need to set the head size as the embedding dimension, <code>d_model</code>, because we currently only have one head layer.
However, we will not train this model at this moment.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>BigramLanguageModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>block_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span> <span class=o>=</span> <span class=n>Head</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=n>targets</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>T</span> <span class=o>=</span> <span class=n>idx</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=n>token_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        <span class=n>posit_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding_table</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>))</span> 
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>token_embed</span> <span class=o>+</span> <span class=n>posit_embed</span> 
</span></span><span class=line><span class=cl>        <span class=c1># apply self attention</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>self_attn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>targets</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=o>*</span><span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>targets</span> <span class=o>=</span> <span class=n>targets</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=o>*</span><span class=n>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span><span class=p>,</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=n>max_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span><span class=p>,</span> <span class=n>loss</span> <span class=o>=</span> <span class=bp>self</span><span class=p>(</span><span class=n>idx</span><span class=p>[:,</span> <span class=o>-</span><span class=n>block_size</span><span class=p>:])</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>            <span class=n>probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>idx_next</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>idx</span><span class=p>,</span> <span class=n>idx_next</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>idx</span>
</span></span></code></pre></div><h3 id=multi-head-attention>Multi-head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h3><p>As an old saying goes, two heads are better than one.
By having multiple heads, we can apply multiple transformations to the embeddings.
Each projection has its own set of learnable parameters, which enables the self-attention layer to focus on different semantic aspects of the sequence.
We will denote the number of heads as <code>h</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>Head</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>h</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=k>for</span> <span class=n>head</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span><span class=p>],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span> <span class=c1># B, T, C</span>
</span></span></code></pre></div><h3 id=dropout>Dropout<a hidden class=anchor aria-hidden=true href=#dropout>#</a></h3><p>Dropout was proposed in <a href=https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf>Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a> by Nitish Srivastava et al. in 2014.
In this technique, a certain proportion of neurons are randomly dropped out during training to prevent overfitting.</p><blockquote><p>We apply dropout to the output of each sub-layer, before it is added to the
sub-layer input and normalized.</p></blockquote><p><img loading=lazy src=dropout.png alt=dropout>
<em><a href=https://wiki.tum.de/download/attachments/23568252/Selection_532.png>source</a></em></p><p>We will apply PyTorch&rsquo;s built-in function <code>nn.Dropout</code> to our <code>Head</code> and <code>MultiHeadAttention</code> layers.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dropout</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Head</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>query</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=c1># C, d_k</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>key</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>   <span class=c1># C, d_k</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>value</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span> <span class=c1># C, d_k</span>
</span></span><span class=line><span class=cl>        <span class=c1># not a model parameter</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s1>&#39;tril&#39;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>block_size</span><span class=p>,</span> <span class=n>block_size</span><span class=p>)))</span>   <span class=c1># block_size, block_size</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># B, T, d_k</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>key</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>   <span class=c1># B, T, d_k</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>C</span><span class=p>)</span>         <span class=c1># B, T, T</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=n>score</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tril</span><span class=p>[:</span><span class=n>T</span><span class=p>,</span> <span class=p>:</span><span class=n>T</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;-inf&#34;</span><span class=p>))</span>    <span class=c1># B, T, T</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>                                    <span class=c1># B, T, T</span>
</span></span><span class=line><span class=cl>        <span class=n>score</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>score</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>value</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>   <span class=c1># B, T, d_k</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>score</span> <span class=o>@</span> <span class=n>v</span>     <span class=c1># (B, T, T)@(B, T, d_k) = (B, T, d_k)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>Head</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>h</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=k>for</span> <span class=n>head</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span><span class=p>],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span> <span class=c1># B, T, C</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h3 id=residual-connection>Residual Connection<a hidden class=anchor aria-hidden=true href=#residual-connection>#</a></h3><p>The concept of residual connections was first introduced in 2015 by Kaiming He et al. in their paper <a href=https://arxiv.org/pdf/1512.03385.pdf>Deep Residual Learning for Image Recognition</a>.
It allows the network to bypass one or more layers, which helps alleviate the vanishing gradient problem that could occur in very deep neural networks.</p><p><img loading=lazy src=resnet.png alt=resnet-residual-connection>
<em><a href=https://paperswithcode.com/>source</a></em></p><p>To implement residual connections and a projection layer in our multi-head attention module, we modify the <code>MultiHeadAttention</code> class as follows.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>h</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>heads</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ModuleList</span><span class=p>([</span><span class=n>Head</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>h</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>head</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=k>for</span> <span class=n>head</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>heads</span><span class=p>],</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h3 id=feed-forward>Feed-Forward<a hidden class=anchor aria-hidden=true href=#feed-forward>#</a></h3><p>As stated in the paper:</p><blockquote><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically.</p></blockquote><p>This means that instead of processing the entire sequence of embeddings as a single vector, the feed-forward network applies the same linear transformations to each embedding individually.</p><blockquote><p>While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is $d_{model} = 512$, and the inner-layer has dimensionality
$d_{ff} = 2048$.</p></blockquote><p>This implies that our first linear layer in the feed-forward layer has an output dimension of <code>d_model * 4</code>, which serves as the input dimension of the second linear layer.
We also apply a dropout layer to the feed-forward layer to avoid overfitting.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>FeedForward</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>net</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_model</span> <span class=o>*</span> <span class=mi>4</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span> <span class=o>*</span> <span class=mi>4</span><span class=p>,</span> <span class=n>d_model</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>net</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h3 id=layer-normalization>Layer Normalization<a hidden class=anchor aria-hidden=true href=#layer-normalization>#</a></h3><p>The concept of layer normalization was introduced by Jimmy Lei Ba et al. in their paper <a href=https://arxiv.org/pdf/1607.06450.pdf>Layer Normalization</a> published in 2016.
Unlike batch normalization, which normalizes the inputs to a batch of data, layer normalization normalizes the inputs to a single layer of the network.
In our implementation, we apply layer normalization before self-attention and feed-forward layers.</p><p><img loading=lazy src=layer-normalization.png alt=layer-normalization>
<em><a href=https://paperswithcode.com/>source</a></em></p><h3 id=refactoring>Refactoring<a hidden class=anchor aria-hidden=true href=#refactoring>#</a></h3><p>Let&rsquo;s refactor the code to put multi-head attention and feed-forward layers to a single <code>Block</code> class.
Moreover, the head size would be automatically set to <code>d_model/h</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Block</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>h</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>h</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>attn</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>h</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ff</span> <span class=o>=</span> <span class=n>FeedForward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># attention + residual connection</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>attn</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># layer normalization</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ln1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># feed forward</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>ff</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># layer normalization</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>ln2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><h2 id=put-everything-together>Put Everything Together<a hidden class=anchor aria-hidden=true href=#put-everything-together>#</a></h2><p>Here are the steps to build a GPT with transformer architecture:</p><ol><li>Initialize the token embedding table with the vocabulary size and embedding dimension <code>(vocab_size, d_model)</code>.</li><li>Initialize the positional embedding table with the maximum sequence length and embedding dimension <code>(block_size, d_model)</code>.</li><li>Create <code>N</code> identical decoder layers using the <code>Block</code> class with multi-head attention, feed-forward, and layer normalization layers. The <code>head_size</code> parameter will be automatically set to <code>d_model/h</code>.</li><li>Add a linear output layer with the output dimension equal to the <code>vocab_size</code>.</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>batch_size</span> <span class=o>=</span> <span class=mi>16</span> 
</span></span><span class=line><span class=cl><span class=n>block_size</span> <span class=o>=</span> <span class=mi>32</span>
</span></span><span class=line><span class=cl><span class=n>eval_interval</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl><span class=n>eval_iters</span> <span class=o>=</span> <span class=mi>200</span>
</span></span><span class=line><span class=cl><span class=n>learning_rate</span> <span class=o>=</span> <span class=mf>1e-3</span>
</span></span><span class=line><span class=cl><span class=n>epochs</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>64</span>   <span class=c1># dimension of embedding</span>
</span></span><span class=line><span class=cl><span class=n>h</span> <span class=o>=</span> <span class=mi>8</span>          <span class=c1># number of heads</span>
</span></span><span class=line><span class=cl><span class=n>N</span> <span class=o>=</span> <span class=mi>6</span>          <span class=c1># number of identical layers</span>
</span></span><span class=line><span class=cl><span class=n>dropout</span> <span class=o>=</span> <span class=mf>0.1</span>  <span class=c1># dropout percentage</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=s1>&#39;cuda&#39;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s1>&#39;cpu&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@torch.no_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>estimate_loss</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>out</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>split</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>,</span> <span class=s1>&#39;val&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=n>losses</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>eval_iters</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>eval_iters</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>X</span><span class=p>,</span> <span class=n>Y</span> <span class=o>=</span> <span class=n>get_batch</span><span class=p>(</span><span class=n>split</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span><span class=p>,</span> <span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>losses</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span><span class=p>[</span><span class=n>split</span><span class=p>]</span> <span class=o>=</span> <span class=n>losses</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>train</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BigramLanguageModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding_table</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>block_size</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=p>[</span><span class=n>Block</span><span class=p>(</span><span class=n>h</span><span class=p>)</span> <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>N</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=n>targets</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>T</span> <span class=o>=</span> <span class=n>idx</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>token_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding_table</span><span class=p>(</span><span class=n>idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>posit_embed</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>position_embedding_table</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>T</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>token_embed</span> <span class=o>+</span> <span class=n>posit_embed</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>blocks</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>logits</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_linear</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>targets</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=o>*</span><span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>targets</span> <span class=o>=</span> <span class=n>targets</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=o>*</span><span class=n>T</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>cross_entropy</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>targets</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>logits</span><span class=p>,</span> <span class=n>loss</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>idx</span><span class=p>,</span> <span class=n>max_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_length</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=bp>self</span><span class=p>(</span><span class=n>idx</span><span class=p>[:,</span> <span class=o>-</span><span class=n>block_size</span><span class=p>:])</span>
</span></span><span class=line><span class=cl>            <span class=n>logits</span> <span class=o>=</span> <span class=n>logits</span><span class=p>[:,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>            <span class=n>probs</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>logits</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>idx_next</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>idx</span><span class=p>,</span> <span class=n>idx_next</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>idx</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>BigramLanguageModel</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=n>p</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span> <span class=k>for</span> <span class=n>p</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>())</span><span class=o>/</span><span class=mf>1e6</span><span class=p>,</span> <span class=s1>&#39;M parameters&#39;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>0.309185 M parameters
</code></pre><h2 id=retraining>Retraining<a hidden class=anchor aria-hidden=true href=#retraining>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>AdamW</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>i</span> <span class=o>%</span> <span class=n>eval_interval</span> <span class=o>==</span> <span class=mi>0</span> <span class=ow>or</span> <span class=n>i</span> <span class=o>==</span> <span class=n>epochs</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>losses</span> <span class=o>=</span> <span class=n>estimate_loss</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;step </span><span class=si>{</span><span class=n>i</span><span class=si>:</span><span class=s2>&gt;6</span><span class=si>}</span><span class=s2>: train loss </span><span class=si>{</span><span class=n>losses</span><span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>, val loss </span><span class=si>{</span><span class=n>losses</span><span class=p>[</span><span class=s1>&#39;val&#39;</span><span class=p>]</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span> <span class=o>=</span> <span class=n>get_batch</span><span class=p>(</span><span class=s1>&#39;train&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span><span class=p>,</span> <span class=n>loss</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x_batch</span><span class=p>,</span> <span class=n>y_batch</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>(</span><span class=n>set_to_none</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>context</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>long</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>decode</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>context</span><span class=p>,</span> <span class=n>max_length</span><span class=o>=</span><span class=mi>2000</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>tolist</span><span class=p>()))</span>
</span></span></code></pre></div><pre><code>step      0: train loss 4.4133, val loss 4.4188
step   1000: train loss 2.1523, val loss 2.1733
step   2000: train loss 1.9162, val loss 1.9929
step   3000: train loss 1.8095, val loss 1.9325
step   4000: train loss 1.7424, val loss 1.8743
step   5000: train loss 1.7031, val loss 1.8359
step   6000: train loss 1.6730, val loss 1.8091
step   7000: train loss 1.6381, val loss 1.8015
step   8000: train loss 1.6231, val loss 1.7956
step   9000: train loss 1.6010, val loss 1.7734
step   9999: train loss 1.5991, val loss 1.7507


POLINCE:
O-momen, marran, stack the blow.

VALUMNIA:
TRong it is 'twill o despreng.

MARCIUS:
She are.

COMSARIO:
Thraby, the tongue,
And lefe to he she this highnoural,
Have any but ut your to spuake it.

LEONTES:
Goot saled shur he wrett.

SICIDIUS:
Be she done! te.

First KINquel:
Thy had diul as recat my deasury? Faulter'er mean, on altal,
If none banch with to times? York,
Vom have yzage; this hight think noble of eye bewill fre,
In gring might to jue of knot it the clunter,
Were henrey quoring to jurition tone
stime to known? Pryity and bear.

KING EL.TAh, is leaven. For I would in
Ancompers, for comen telms things:
I worn apene so Herdow procked love; dime so worder.

LORIS:
It is here bear of go him.

ROMEY:
How I Leffater the death? And mearrinad
king cans no myselfy that bartt,
If you I decdom to be in tellothen,
Low ke'es hath s duck, and within kindes, that found als
In he house his
Of the spine confeive inther his dear to gater:
And go agonst Marcito, I'll wid my countery,
I way, lientifirn tenving rulby us my follow honour yield stent poon,
Jufe the be dared on the kial je:
The day my Lounges, be agains in have
once as to plating exvage of his tonake
That themn were by to the hance,
The sold long, po somebn o'er becelds
Is this ofseding on this soak? alrick.

UMISmed!
HO, answer
Off Humbledy, that's will forted yial with pring's lord.
Forth, Jolsn'd ladib tod
But thy shorly be this mine stons.
Good you withnlieds think, this mance and thingn blunge his of be be reep steep your intent
for thou way, the nober, and visy
From the pot of lord?
Mast all to be endought: what my loness,
Tis is monius and from out of Sunscoa may,
And not my the see to all, everstrer.

KING RICHARD III:
My hidoner, and strangems, honours,
Before requick?

ELIFFOLLY:
O, ce, but and 't: her I near afta humbhal gittled here,
O
tAlker of off it dispuised here the heam we froens,
Wasce, not he rese that dear'd, to,
And in stay be I have will am gove his derefy:
lade them brooks it in
</code></pre><p>The newly generated text contains more word-like characters and resembles the style of Shakespeare, with a more significant proportion of correctly spelled words.</p><h2 id=revisiting-attention>Revisiting Attention<a hidden class=anchor aria-hidden=true href=#revisiting-attention>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sequence</span> <span class=o>=</span> <span class=s2>&#34;&#34;&#34;MENENIUS:</span><span class=se>\n</span><span class=s2>What is gra&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl><span class=n>token_embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>token_embedding_table</span><span class=o>.</span><span class=n>weight</span>
</span></span><span class=line><span class=cl><span class=n>position_embeddings</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>position_embedding_table</span><span class=o>.</span><span class=n>weight</span>
</span></span><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>stoi</span> <span class=p>[</span><span class=n>c</span><span class=p>]</span> <span class=k>for</span> <span class=n>c</span> <span class=ow>in</span> <span class=n>sequence</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>positions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>i</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>sequence</span><span class=p>))])</span>
</span></span><span class=line><span class=cl><span class=n>embed</span> <span class=o>=</span> <span class=n>token_embeddings</span><span class=p>[</span><span class=n>tokens</span><span class=p>]</span> <span class=o>+</span> <span class=n>position_embeddings</span><span class=p>[</span><span class=n>positions</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># query and vector weights for last head of the last block</span>
</span></span><span class=line><span class=cl><span class=n>q</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>blocks</span><span class=p>[</span><span class=mi>5</span><span class=p>]</span><span class=o>.</span><span class=n>attn</span><span class=o>.</span><span class=n>heads</span><span class=p>[</span><span class=mi>7</span><span class=p>]</span><span class=o>.</span><span class=n>query</span>
</span></span><span class=line><span class=cl><span class=n>k</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>blocks</span><span class=p>[</span><span class=mi>5</span><span class=p>]</span><span class=o>.</span><span class=n>attn</span><span class=o>.</span><span class=n>heads</span><span class=p>[</span><span class=mi>7</span><span class=p>]</span><span class=o>.</span><span class=n>key</span>
</span></span><span class=line><span class=cl><span class=n>v</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>blocks</span><span class=p>[</span><span class=mi>5</span><span class=p>]</span><span class=o>.</span><span class=n>attn</span><span class=o>.</span><span class=n>heads</span><span class=p>[</span><span class=mi>7</span><span class=p>]</span><span class=o>.</span><span class=n>value</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># query and key space</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>Q</span> <span class=o>=</span> <span class=n>q</span><span class=p>(</span><span class=n>embed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>K</span> <span class=o>=</span> <span class=n>k</span><span class=p>(</span><span class=n>embed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>Q</span> <span class=o>@</span> <span class=n>K</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>/=</span> <span class=n>math</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>d_model</span> <span class=o>//</span> <span class=n>h</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>mask</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>embed</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>embed</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]))</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>score</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s2>&#34;-inf&#34;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>score</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>score</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>V</span> <span class=o>=</span> <span class=n>v</span><span class=p>(</span><span class=n>embed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>new_embed</span> <span class=o>=</span> <span class=n>score</span> <span class=o>@</span> <span class=n>V</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Attention scores for the sequence:</span><span class=se>\n</span><span class=s2> </span><span class=si>{</span><span class=n>score</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=p>:]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Adjusted and compressed embeddings for the sequence:</span><span class=se>\n</span><span class=s2> </span><span class=si>{</span><span class=n>new_embed</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Attention scores for the sequence:
 tensor([1.0275e-01, 6.3248e-03, 1.2576e-02, 7.7688e-04, 1.2232e-03, 1.0114e-01,
        5.6094e-03, 1.2616e-01, 1.0319e-01, 1.5049e-01, 4.3153e-02, 4.5383e-03,
        9.5087e-03, 4.0352e-03, 1.8735e-01, 1.6233e-03, 1.2997e-01, 5.6082e-03,
        2.0060e-05, 3.1588e-04, 3.6474e-03], device='cuda:0')
Adjusted and compressed embeddings for the sequence:
 tensor([[ 3.8285e-01, -5.6125e-01, -1.2138e+00, -5.2913e-01,  9.2973e-01,
         -4.2545e-01, -2.4848e+00,  6.8524e-03],
        [ 3.6920e-01, -5.4390e-01, -9.2868e-01, -4.8776e-01,  8.3288e-01,
         -3.3776e-01, -2.2440e+00,  1.1147e-01],
        [ 4.0217e-01, -4.6048e-01,  8.1029e-01, -1.8336e-01,  2.0483e-01,
          2.0015e-01, -5.8833e-01,  7.3810e-01],
        [ 1.4945e+00, -6.4184e-01, -2.4202e-01,  1.0156e-01,  2.0985e-01,
         -1.5870e-01,  5.5549e-02,  3.1318e-01],
        [ 3.5607e-01, -5.5418e-02,  1.5003e+00, -2.7288e-01, -8.2235e-02,
          1.1763e-01, -7.0800e-01,  1.2626e+00],
        [-6.7275e-02, -1.2190e+00, -1.7885e-01,  2.6792e-01, -2.2870e-01,
         -8.5028e-01,  3.4890e-01,  1.3680e-01],
        [ 1.7077e+00, -8.5935e-01, -6.5319e-01,  1.4917e-01,  2.7577e-01,
         -2.7634e-01,  3.0645e-01,  3.7927e-02],
        [-5.1688e-02, -8.2619e-01,  8.0506e-02,  2.1702e-01, -1.6939e-02,
         -6.4278e-01,  1.9751e-01,  8.1660e-02],
        [-7.1723e-02, -5.1926e-01, -2.9651e-01, -5.3577e-02,  1.8432e-01,
         -4.6867e-01, -7.6557e-01, -1.7440e-01],
        [ 8.1420e-01, -4.1661e-01,  1.0995e+00, -3.2608e-01,  2.8869e-02,
          2.2275e-02, -5.5174e-02,  8.0169e-01],
        [ 2.9553e-01, -5.1129e-01,  2.6954e-01, -1.0131e-01,  1.6535e-03,
         -2.3739e-01,  2.4023e-01,  2.7450e-03],
        [ 9.2807e-01, -5.3834e-01, -4.8175e-01, -1.7232e-02,  1.6207e-01,
         -1.7096e-01,  3.0736e-01, -9.1554e-02],
        [ 4.2730e-01,  6.4469e-01,  8.8334e-01,  4.4953e-01, -3.0363e-01,
          1.3055e-01,  1.1382e+00, -6.6804e-01],
        [ 8.4047e-01, -4.7317e-01, -6.5326e-02, -5.7882e-02,  1.3698e-01,
         -1.0259e-01,  2.5059e-01,  6.1572e-02],
        [ 2.9731e-01, -8.2256e-01, -2.8259e-02,  3.3942e-01, -2.8240e-01,
          1.9379e-01, -9.6743e-02,  2.3589e-01],
        [ 6.0484e-01, -1.0521e-01,  2.7202e-01,  2.2309e-01, -6.7768e-01,
          2.5342e-01, -4.1722e-01,  8.2589e-02],
        [ 4.1097e-01,  6.0131e-01,  8.3584e-01,  4.4749e-01, -2.8864e-01,
          1.3370e-01,  1.0975e+00, -6.3150e-01],
        [-5.1310e-01, -3.5065e-01, -1.4606e-01,  4.4343e-01,  2.1451e-01,
          7.1118e-02, -1.8510e-02,  6.4416e-01],
        [ 1.3922e-01, -5.7186e-02, -2.0533e-01, -2.0123e-01, -2.3971e-01,
          2.8392e-01, -2.8814e-01,  3.0751e-01],
        [ 3.3605e-01,  5.6808e-01,  8.5728e-01,  3.2310e-01, -3.3082e-01,
          1.1003e-01,  1.1402e+00, -6.2344e-01],
        [ 1.1078e-01, -2.0579e-02, -1.6989e-01, -8.3665e-02, -1.2148e-02,
          5.8077e-02, -3.4206e-01,  3.3760e-01]], device='cuda:0')
</code></pre><h2 id=notes>Notes<a hidden class=anchor aria-hidden=true href=#notes>#</a></h2><p>Here are some tiny differences between my code and the code in the video.</p><ol><li>I applied layer normalization after the self-attention layer, while he applied immediately on <code>x</code> before <code>x</code> entered the self-attention and feed-forward layers.</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Block</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34; Transformer block: communication followed by computation &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_embd</span><span class=p>,</span> <span class=n>n_head</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># n_embd: embedding dimension, n_head: the number of heads we&#39;d like</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>head_size</span> <span class=o>=</span> <span class=n>n_embd</span> <span class=o>//</span> <span class=n>n_head</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sa</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>n_head</span><span class=p>,</span> <span class=n>head_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ffwd</span> <span class=o>=</span> <span class=n>FeedFoward</span><span class=p>(</span><span class=n>n_embd</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln1</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>n_embd</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>ln2</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>n_embd</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>sa</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ln1</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>ffwd</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>ln2</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></div><ol><li>The scaling factor I used was $d_k$ instead of $d_model$ (maybe it&rsquo;s a typo in his code?).</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>Head</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34; one head of self-attention &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_k</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>key</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>n_embd</span><span class=p>,</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>query</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>n_embd</span><span class=p>,</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>value</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>n_embd</span><span class=p>,</span> <span class=n>head_size</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>register_buffer</span><span class=p>(</span><span class=s1>&#39;tril&#39;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>tril</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>block_size</span><span class=p>,</span> <span class=n>block_size</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Dropout</span><span class=p>(</span><span class=n>dropout</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>  <span class=c1># batch_size, block_size, n_embd</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>key</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>    <span class=c1># (B,T,C)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>query</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># (B,T,C)</span>
</span></span><span class=line><span class=cl>        <span class=c1># compute attention scores (&#34;affinities&#34;)</span>
</span></span><span class=line><span class=cl>        <span class=n>wei</span> <span class=o>=</span> <span class=n>q</span> <span class=o>@</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>C</span> <span class=o>**-</span><span class=mf>0.5</span> <span class=c1># (B, T, C) @ (B, C, T) -&gt; (B, T, T)</span>
</span></span><span class=line><span class=cl>        <span class=n>wei</span> <span class=o>=</span> <span class=n>wei</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tril</span><span class=p>[:</span><span class=n>T</span><span class=p>,</span> <span class=p>:</span><span class=n>T</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span> <span class=c1># (B, T, T)</span>
</span></span><span class=line><span class=cl>        <span class=n>wei</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>wei</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span> <span class=c1># (B, T, T)</span>
</span></span><span class=line><span class=cl>        <span class=n>wei</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>dropout</span><span class=p>(</span><span class=n>wei</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># perform the weighted aggregation of the values</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>value</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=c1># (B,T,C)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=n>wei</span> <span class=o>@</span> <span class=n>v</span> <span class=c1># (B, T, T) @ (B, T, C) -&gt; (B, T, C)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span></code></pre></div><h2 id=other-resources>Other Resources<a hidden class=anchor aria-hidden=true href=#other-resources>#</a></h2><ul><li><a href=https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html>https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html</a></li><li><a href=https://jalammar.github.io/illustrated-transformer/>https://jalammar.github.io/illustrated-transformer/</a></li><li><a href="https://www.youtube.com/watch?v=ptuGllU5SQQ&amp;list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&amp;index=9">https://www.youtube.com/watch?v=ptuGllU5SQQ&amp;list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&amp;index=9</a></li><li><a href=https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms>https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms</a></li><li><a href=https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf>https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf</a></li><li><a href=https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/>https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://gejun.name/tags/transformers/>transformers</a></li><li><a href=https://gejun.name/tags/encoder/>encoder</a></li><li><a href=https://gejun.name/tags/docoder/>docoder</a></li><li><a href=https://gejun.name/tags/pytorch/>pytorch</a></li><li><a href=https://gejun.name/tags/attention/>attention</a></li></ul><nav class=paginav><a class=next href=https://gejun.name/natural-language-processing/building-makemore-mlp/><span class=title>Next »</span><br><span>Multilayer Perceptron (MLP)</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture by Building GPT on twitter" href="https://twitter.com/intent/tweet/?text=Understanding%20Transformer%20Architecture%20by%20Building%20GPT&amp;url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2funderstanding-attention-nlp%2f&amp;hashtags=transformers%2cencoder%2cdocoder%2cpytorch%2cattention"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture by Building GPT on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2funderstanding-attention-nlp%2f&amp;title=Understanding%20Transformer%20Architecture%20by%20Building%20GPT&amp;summary=Understanding%20Transformer%20Architecture%20by%20Building%20GPT&amp;source=https%3a%2f%2fgejun.name%2fnatural-language-processing%2funderstanding-attention-nlp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture by Building GPT on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2funderstanding-attention-nlp%2f&title=Understanding%20Transformer%20Architecture%20by%20Building%20GPT"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture by Building GPT on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgejun.name%2fnatural-language-processing%2funderstanding-attention-nlp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture by Building GPT on whatsapp" href="https://api.whatsapp.com/send?text=Understanding%20Transformer%20Architecture%20by%20Building%20GPT%20-%20https%3a%2f%2fgejun.name%2fnatural-language-processing%2funderstanding-attention-nlp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Understanding Transformer Architecture by Building GPT on telegram" href="https://telegram.me/share/url?text=Understanding%20Transformer%20Architecture%20by%20Building%20GPT&amp;url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2funderstanding-attention-nlp%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://giscus.app/client.js data-repo=zhugejun/zhugejun.github.io data-repo-id data-category=Announcements data-category-id data-mapping=pathname data-reactions-enabled=1 data-theme=preferred_color_scheme data-language=en crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the comments powered by giscus.</noscript></article></main><footer class=footer><span>&copy; 2023 <a href=https://gejun.name>Gejun's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>