<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Bigram Character-level Language Model | Gejun's Blog</title><meta name=keywords content="bigram,pytorch"><meta name=description content="This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.
In this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let&rsquo;s prepare the data first."><meta name=author content="Gejun Zhu"><link rel=canonical href=https://gejun.name/natural-language-processing/building-makemore/><link crossorigin=anonymous href=/assets/css/stylesheet.07b504287c624e47e21667d48ef8c5c81574d5a3594c9eb68922bfeacb71823b.css integrity="sha256-B7UEKHxiTkfiFmfUjvjFyBV01aNZTJ62iSK/6stxgjs=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://gejun.name/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gejun.name/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gejun.name/favicon-32x32.png><link rel=apple-touch-icon href=https://gejun.name/apple-touch-icon.png><link rel=mask-icon href=https://gejun.name/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-BEP7FKQVEG"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-BEP7FKQVEG")</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><meta property="og:title" content="Bigram Character-level Language Model"><meta property="og:description" content="This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.
In this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let&rsquo;s prepare the data first."><meta property="og:type" content="article"><meta property="og:url" content="https://gejun.name/natural-language-processing/building-makemore/"><meta property="og:image" content="https://gejun.name"><meta property="article:section" content="natural-language-processing"><meta property="article:published_time" content="2023-03-04T00:00:00+00:00"><meta property="article:modified_time" content="2023-03-04T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gejun.name"><meta name=twitter:title content="Bigram Character-level Language Model"><meta name=twitter:description content="This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.
In this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let&rsquo;s prepare the data first."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Natural-language-processings","item":"https://gejun.name/natural-language-processing/"},{"@type":"ListItem","position":3,"name":"Bigram Character-level Language Model","item":"https://gejun.name/natural-language-processing/building-makemore/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bigram Character-level Language Model","name":"Bigram Character-level Language Model","description":"This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.\nIn this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let\u0026rsquo;s prepare the data first.","keywords":["bigram","pytorch"],"articleBody":"This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.\nIn this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let’s prepare the data first.\nData Preparation Load data We are using the most common 32k names of 2018 from ssa.gov website as our data source. First, we apply the code below to obtain each bigram’s frequency. If you don’t know what a bigram is, a bigram is a sequence of two adjacent words or characters in a text. We also add a special character, “.”, to the name’s beginning and end to indicate its start and end, respectively. As can be seen that the top 5 common bigrams in the data are n., a., an, .a, and e..\nfrom collections import Counter words = open(\"names.txt\", \"r\").read().splitlines() counter = Counter() for word in words: chs = list(\".\" + word + \".\") for c1, c2 in zip(chs, chs[1:]): bigram = (c1, c2) counter[bigram] += 1 for bigram, frequency in counter.most_common(5): print(f\"Frequency of {''.join(bigram)}: {frequency}\") Frequency of n.: 6763 Frequency of a.: 6640 Frequency of an: 5438 Frequency of .a: 4410 Frequency of e.: 3983 Numericallization As is known that computers are good at processing numerical data; however, they may not be efficient in dealing with text. So our second step is to create two mappings: string to index and index to string. These mappings are used to represent words or characters numerically. This process is sometimes called numericalization.\nimport torch import string import matplotlib.pyplot as plt chars = string.ascii_lowercase stoi = {s: i+1 for i, s in enumerate(chars)} stoi[\".\"] = 0 itos = {i: s for s, i in stoi.items()} print(stoi, itos) {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'} Counting Approach Frequency Our first step is to obtain the frequencies of the bigrams. Since we have a vocabulary of 27 characters-26 letters in lowercase plus 1 special character, we need a $27\\times 27$ matrix to store the frequencies of all possible bigrams. Figure 1 is a heatmap of the calculated frequencies. The darker the color, the higher the frequency of the bigram.\nN = torch.zeros((27, 27), dtype=torch.int32) for (c1, c2), freq in counter.items(): idx1 = stoi[c1] idx2 = stoi[c2] N[idx1, idx2] = freq plt.figure(figsize=(16, 16)) plt.imshow(N, cmap=\"Blues\") for i in range(27): for j in range(27): chstr = itos[i] + itos[j] plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\") plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\") plt.axis(\"off\") plt.show() Figure 1: A heatmap plot for frequencies of bigrams Probability To get the probability of each bigram, we want to normalize the matrix N by row. Why? Because we want to know the probability of the character given the current character we have in the process of character generation, i.e., $P(next\\ char | current\\ char)$. To avoid calculating $log0$ later on, we add 1 to the frequency of each bigram.\nP = (N + 1).float() P /= P.sum(1, keepdims=True) Maximum Likelihood Maximum likelihood is a statistical method to estimate the parameters of a probability distribution based on observed data. The goal of maximum likelihood is to find the values of the distribution’s parameters that make the observed data most likely to have been generated by that distribution. In our case, we want the next generated character comes from the probability distribution as much as possible. How do we calculate the likelihood? It is the product of the probability of each bigram in a word. $$ L(\\theta) = P(X_1=x_1, X_2=x_2, …, X_n=x_n) = \\Pi_i^n P(X_i=x_i)$$ For example, the likelihood of the word good is calculated as $$Likelihood= P(\".g\") * P(“go”) * P(“oo”) * P(“od”) * P(“d.”) $$ $$ = 0.0209*0.0430*0.0146*0.0240*0.0936=2.9399e-8$$\ndef calc_likelihood(word, verbose=False): word = list(\".\" + word + \".\") likelihood = 1.0 for c1, c2 in zip(word, word[1:]): idx1 = stoi[c1] idx2 = stoi[c2] prob = P[idx1, idx2] if verbose: print(f\"probability for {''.join((c1, c2))}: {prob:.4f}\") likelihood *= prob return likelihood prob = calc_likelihood(\"good\", verbose=True) print(f\"Likelihood for good is: {prob:.4e}\") probability for .g: 0.0209 probability for go: 0.0430 probability for oo: 0.0146 probability for od: 0.0240 probability for d.: 0.0936 Likelihood for good is: 2.9399e-08 Let’s generate some words by randomly picking the bigram according to its probability using torch.multinomial function and calculate their likelihoods.\ng = torch.Generator().manual_seed(420) generated_words = [] for i in range(5): out = [] ix = 0 while True: p = P[ix] ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() if ix == 0: break out.append(itos[ix]) generated_words.append((\"\".join(out), calc_likelihood(\"\".join(out)).item())) generated_words.sort(key=lambda x: -x[1]) for gw, lh in generated_words: print(f\"Likelihood for {gw}: {lh}\") Likelihood for jen: 0.0005491252522915602 Likelihood for jor: 0.0001786774955689907 Likelihood for she: 0.00017446796118747443 Likelihood for tais: 3.90183367926511e-06 Likelihood for anuir: 2.335933579900029e-08 It turns out jen which has the maximum likelihood 0.000549 is the winner in these 5 randomly generated words. Remember that our goal is to maximize the likelihood of the word the model generates because the higher the likelihood, the better the model. However, notice that the likelihoods for the generated words are too small, so applying a log function to each probability would make it easier to work with. $$ logL(\\theta) = log\\Pi_i^n P(X_i=x_i)=\\Sigma_i^nlogP(X_i=x_i)$$\nAdditionally, maximizing the likelihood is the same as maximizing the log-likelihood because the logarithm function is a monotonic increasing function, which is the same as minimizing the negative log-likelihood. We prefer minimization to maximization in any optimization problem. Let’s calculate the average negative log-likelihood of our name dataset, which is 2.454679.\ndef calc_nll(word): word = list(\".\" + word + \".\") log_likelihood = 0.0 for c1, c2 in zip(word, word[1:]): idx1 = stoi[c1] idx2 = stoi[c2] prob = P[idx1, idx2] log_prob = torch.log(prob) log_likelihood += log_prob return -log_likelihood nlls = [calc_nll(w) for w in words] ns = [len(w) + 1 for w in words] print(f\"Average negative log-likelihood: {sum(nlls)/sum(ns):.6f}\") Average negative log-likelihood: 2.454579 Neural Network How does a neural network model fit in the character generation? Think of it in this way: given the last generated character, we want the model to output a probability distribution for the next character, in which we can find the most likely character to follow it. In other words, our task is to use the model to estimate the probability distribution based on the dataset rather than relying on counting the occurrences of each bigram. As always, let’s prepare the data in the first step.\nTraining Data Preparation The training data is created using bigrams, where the first character is the input feature, and the second character is used as the target of the model. Since feeding integers into a neural network and multiplying them with weights does not make sense, we need to transform them into a different format. The most common method is one-hot encoding, which transforms each integer into a vector with all 0s except for a 1 at the index corresponding to the integer. PyTorch provides a built-in torch.nn.functional.one_hot function for one-hot encoding.\nimport torch.nn.functional as F xs, ys = [], [] for word in words: chs = list(\".\" + word + \".\") for c1, c2 in zip(chs, chs[1:]): idx1 = stoi[c1] idx2 = stoi[c2] xs.append(idx1) ys.append(idx2) # tensor function returns the same type as its original xs = torch.tensor(xs) ys = torch.tensor(ys) xenc = F.one_hot(xs, num_classes=27).float() print(xenc.shape) torch.Size([228146, 27]) After applying one-hot encoding, we have a tensor xenc of shape $228146\\times 27$.\nUnderstanding Weights The weight matrix of our model has the same shape as the matrix N above but is initialized with random values. PyTorch’s built-in function torch.randn gives us random numbers from a normal distribution with mean 0 and standard deviation 1, resulting in positive and negative values. After multiplying the one-hot encoding matrix with weights, we obtain the output of the first layer, which may contain negative values. However, we want the output to represent the probability of the next character, as we calculated above. To achieve this, we can treat the output as the logarithm of the frequencies and apply the exponential function to obtain the positive values, which can be interpreted as the frequencies of the bigrams starting with the input feature. Why? Because multiplying a one-hot encoding vector having a 1 at index i, with the weight matrix W is the same as getting the ith row of W. And we want this frequency matrix to be close to the matrix N as close as possible. If we further normalize the output over the rows, we can obtain the probability distribution of bigrams. In fact, the last two steps, applying exponential function and normalization, of calculation are known as the softmax function.\ng = torch.Generator().manual_seed(420) W = torch.randn((27, 27), generator=g, requires_grad=True) # log-counts logits = xenc @ W # (228146, 27) x (27, 27) # counts counts = logits.exp() # probability probs = counts / counts.sum(1, keepdim=True) print(probs.shape) print(sum(probs[1,:])) torch.Size([228146, 27]) tensor(1.0000, grad_fn=) Optimization Remember that our goal is to approach the actual probabilities from the training data using maximum likelihood estimation. As the training progresses, the model adjusts the weights in such a way that the predicted probabilities for the next character in a word are as close to the actual probabilities of the training data. By minimizing the negative log-likelihood, we effectively minimize the distance between predicted and actual probabilities. Let’s take the first word, emma, as an example and see how the neural network calculates its loss. This step is also called forward pass. The first bigram is .e with the input . (index 0) and actual label e (index 5). The one-hot encoding for . is [1, 0, ..., 0], and the output probability for e is 0.0246. Applying log and negation, we have the loss as 3.7050. The same calculation applies to em, mm, ma, and a.. Finally, we get the loss for emma is 3.6985.\nnlls = torch.zeros(5) for i in range(5): x = xs[i].item() y = ys[i].item() print('-' * 50) print(f'bigram example {i+1}: {itos[x]} {itos[y]} (indexes {x}, {y})') print(f'input to the neural network: {x}') print(f'output probbabilities from the nn: {probs[i]}') print(f'label (actual next character): {y}') p = probs[i, y] print(f'probability assigned by the nn to the correct character: {p.item()}') logp = torch.log(p) print(f'log-likelihood: {logp.item()}') nll = -logp print(f'negative log likelihood: {nll}') nlls[i] = nll print(f\"Average negative log-likelihood, i.e., loss={nlls.mean().item()}\") -------------------------------------------------- bigram example 1: . e (indexes 0, 5) input to the neural network: 0 output probbabilities from the nn: tensor([0.0167, 0.0278, 0.0328, 0.0114, 0.0173, 0.0246, 0.0100, 0.0341, 0.1024, 0.0259, 0.2364, 0.0219, 0.0422, 0.0108, 0.1262, 0.0647, 0.0130, 0.0162, 0.0157, 0.0093, 0.0184, 0.0022, 0.0482, 0.0090, 0.0069, 0.0195, 0.0362], grad_fn=) label (actual next character): 5 probability assigned by the nn to the correct character: 0.024598384276032448 log-likelihood: -3.7050745487213135 negative log likelihood: 3.7050745487213135 -------------------------------------------------- bigram example 2: e m (indexes 5, 13) input to the neural network: 5 output probbabilities from the nn: tensor([0.1219, 0.0087, 0.0157, 0.0546, 0.0067, 0.0149, 0.0185, 0.0338, 0.0110, 0.0030, 0.0060, 0.0697, 0.0211, 0.0579, 0.0061, 0.0043, 0.0746, 0.0416, 0.0264, 0.0611, 0.0823, 0.0124, 0.0179, 0.0129, 0.0374, 0.1633, 0.0162], grad_fn=) label (actual next character): 13 probability assigned by the nn to the correct character: 0.057898372411727905 log-likelihood: -2.8490660190582275 negative log likelihood: 2.8490660190582275 -------------------------------------------------- bigram example 3: m m (indexes 13, 13) input to the neural network: 13 output probbabilities from the nn: tensor([0.3351, 0.0126, 0.0370, 0.0075, 0.0302, 0.0635, 0.0042, 0.0339, 0.0155, 0.0512, 0.0080, 0.0283, 0.0557, 0.0171, 0.0388, 0.0103, 0.0507, 0.0398, 0.0191, 0.0074, 0.0174, 0.0132, 0.0121, 0.0245, 0.0307, 0.0219, 0.0142], grad_fn=) label (actual next character): 13 probability assigned by the nn to the correct character: 0.017136109992861748 log-likelihood: -4.066567420959473 negative log likelihood: 4.066567420959473 -------------------------------------------------- bigram example 4: m a (indexes 13, 1) input to the neural network: 13 output probbabilities from the nn: tensor([0.3351, 0.0126, 0.0370, 0.0075, 0.0302, 0.0635, 0.0042, 0.0339, 0.0155, 0.0512, 0.0080, 0.0283, 0.0557, 0.0171, 0.0388, 0.0103, 0.0507, 0.0398, 0.0191, 0.0074, 0.0174, 0.0132, 0.0121, 0.0245, 0.0307, 0.0219, 0.0142], grad_fn=) label (actual next character): 1 probability assigned by the nn to the correct character: 0.012621787376701832 log-likelihood: -4.372330665588379 negative log likelihood: 4.372330665588379 -------------------------------------------------- bigram example 5: a . (indexes 1, 0) input to the neural network: 1 output probbabilities from the nn: tensor([0.0302, 0.0788, 0.0096, 0.0099, 0.0151, 0.1021, 0.0146, 0.0253, 0.0076, 0.0107, 0.0429, 0.0286, 0.0371, 0.0437, 0.0168, 0.0133, 0.0129, 0.0075, 0.0038, 0.0199, 0.0854, 0.0875, 0.1194, 0.1119, 0.0151, 0.0325, 0.0177], grad_fn=) label (actual next character): 0 probability assigned by the nn to the correct character: 0.030220769345760345 log-likelihood: -3.4992258548736572 negative log likelihood: 3.4992258548736572 Average negative log-likelihood, i.e., loss=3.6984527111053467 So how do we calculate the loss efficiently? It turns out that we can pass all these row and column indices to the matrix, and then take log and mean afterwards. The loss for the forward pass is 3.6374.\nloss = -probs[torch.arange(len(xs)), ys].log().mean() print(f\"Overall loss: {loss.item()}\") Overall loss: 3.637367010116577 After obtaining the average loss from the forward pass, we need a backward pass to update the weights. To do this, we need to make sure that the parameter requires_grad is set to True for the weight matrix W. Next, we zero out all gradients to avoid the accumulation of gradients across batches. We then call loss.backward() to compute the gradient of the oss with regard to each weight. The gradient of a weight indicates how much that increasing that weight will affect the loss. If it is positive, increasing the weight will increase the loss too. Conversely, increasing the weight will decrease the loss if the gradient is negative. For example, W.grad[0, 0]=0.002339 means that W[0, 0] has a positive effect on the loss.\n# set the gradient to zero W.grad = None # backward pass loss.backward() The next step is to update the weights and recalculate the averge loss.\nlr = 0.1 W.data += -lr * W.grad # forward pass logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) loss = -probs[torch.arange(len(xs)), ys].log().mean() print(f\"Overall loss: {loss.item()}\") Overall loss: 3.6365137100219727 The overall loss is now 3.6365, which is slightly lower than before. We can keep doing this gradient descent step until the model performance is good enough. As we train more epochs, the overall loss is getting closer to the actual overall loss, which is 2.454579.\nlr = 50 for i in range(301): logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) loss = -probs[torch.arange(len(xs)), ys].log().mean() if i % 50 == 0: print(f\"Epochs: {i}, loss: {loss.item()}\") W.grad = None loss.backward() W.data += -lr * W.grad Epochs: 0, loss: 3.6365137100219727 Epochs: 50, loss: 2.4955990314483643 Epochs: 100, loss: 2.4727954864501953 Epochs: 150, loss: 2.4657769203186035 Epochs: 200, loss: 2.4625258445739746 Epochs: 250, loss: 2.4606406688690186 Epochs: 300, loss: 2.459399700164795 Note that our current neural network only has one hidden layer. We can add more hidden layers to improve the model performance. Additionally, we can add a regularization item (e.g., the mean of the square of all weights) in the loss function to prevent overfitting.\nloss = -probs[torch.arange(len(xs)), ys].log().mean() + 0.01 * (W ** 2).mean() print(loss) tensor(2.4834, grad_fn=) In this case, the optimization has two components–average negative log-likelihood and mean of the square of weights. The regularization item works like a force to squeeze the weights and make them close to zeros as much as possible. The last step is to sample characters from the neural network model.\ng = torch.Generator().manual_seed(420) nn_generated_words = [] for _ in range(5): out = [] idx = 0 while True: xenc = F.one_hot(torch.tensor([idx]), num_classes=27).float() logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item() if idx == 0: break out.append(itos[idx]) nn_generated_words.append((\"\".join(out), calc_likelihood(\"\".join(out)).item())) nn_generated_words.sort(key=lambda x: -x[1]) for gw, lh in nn_generated_words: print(f\"Likelihood for {gw}: {lh}\") Likelihood for jen: 0.0005491252522915602 Likelihood for jor: 0.0001786774955689907 Likelihood for she: 0.00017446796118747443 Likelihood for tais: 3.90183367926511e-06 Likelihood for anuir: 2.335933579900029e-08 We are using the same seed for the generator. The words generated from the neural network are exactly the same as those generated from the probability table above, which is what we want to see.\n","wordCount":"2751","inLanguage":"en","datePublished":"2023-03-04T00:00:00Z","dateModified":"2023-03-04T00:00:00Z","author":{"@type":"Person","name":"Gejun Zhu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://gejun.name/natural-language-processing/building-makemore/"},"publisher":{"@type":"Organization","name":"Gejun's Blog","logo":{"@type":"ImageObject","url":"https://gejun.name/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gejun.name accesskey=h title="Gejun's Blog (Alt + H)">Gejun's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gejun.name/archives title=Archive><span>Archive</span></a></li><li><a href=https://gejun.name/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://gejun.name/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://gejun.name/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gejun.name>Home</a>&nbsp;»&nbsp;<a href=https://gejun.name/natural-language-processing/>Natural-language-processings</a></div><h1 class=post-title>Bigram Character-level Language Model</h1><div class=post-meta><span title='2023-03-04 00:00:00 +0000 UTC'>March 4, 2023</span>&nbsp;·&nbsp;13 min&nbsp;·&nbsp;Gejun Zhu&nbsp;|&nbsp;<a href=https://github.com/zhugejun/zhugejun.github.io/tree/main/content/natural-language-processing/building-makemore/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#data-preparation aria-label="Data Preparation">Data Preparation</a><ul><li><a href=#load-data aria-label="Load data">Load data</a></li><li><a href=#numericallization aria-label=Numericallization>Numericallization</a></li></ul></li><li><a href=#counting-approach aria-label="Counting Approach">Counting Approach</a><ul><li><a href=#frequency aria-label=Frequency>Frequency</a></li><li><a href=#probability aria-label=Probability>Probability</a></li><li><a href=#maximum-likelihood aria-label="Maximum Likelihood">Maximum Likelihood</a></li></ul></li><li><a href=#neural-network aria-label="Neural Network">Neural Network</a><ul><li><a href=#training-data-preparation aria-label="Training Data Preparation">Training Data Preparation</a></li><li><a href=#understanding-weights aria-label="Understanding Weights">Understanding Weights</a></li><li><a href=#optimization aria-label=Optimization>Optimization</a></li></ul></li></ul></div></details></div><div class=post-content><p>This is a series of learning notes for the excellent online course <a href=https://karpathy.ai/zero-to-hero.html>Neural Networks: Zero to Hero</a> created by <a href=https://karpathy.ai/>Andrej Karpathy</a>. The official Jupyter Notebook for this lecture is <a href=https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part1_bigrams.ipynb>here</a>.</p><p>In this lecture, Andrej shows us two different approaches to generating characters.
The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch.
Before we can generate characters using either approach, let&rsquo;s prepare the data first.</p><h2 id=data-preparation>Data Preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h2><h3 id=load-data>Load data<a hidden class=anchor aria-hidden=true href=#load-data>#</a></h3><p>We are using the most common 32k names of 2018 from ssa.gov website as our data source.
First, we apply the code below to obtain each bigram&rsquo;s frequency.
If you don&rsquo;t know what a bigram is, a bigram is a sequence of two adjacent words or characters in a text.
We also add a special character, &ldquo;.&rdquo;, to the name&rsquo;s beginning and end to indicate its start and end, respectively.
As can be seen that the top 5 common bigrams in the data are <code>n.</code>, <code>a.</code>, <code>an</code>, <code>.a</code>, and <code>e.</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>Counter</span>
</span></span><span class=line><span class=cl><span class=n>words</span> <span class=o>=</span> <span class=nb>open</span><span class=p>(</span><span class=s2>&#34;names.txt&#34;</span><span class=p>,</span> <span class=s2>&#34;r&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>read</span><span class=p>()</span><span class=o>.</span><span class=n>splitlines</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>counter</span> <span class=o>=</span> <span class=n>Counter</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>words</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>chs</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=s2>&#34;.&#34;</span> <span class=o>+</span> <span class=n>word</span> <span class=o>+</span> <span class=s2>&#34;.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>c1</span><span class=p>,</span> <span class=n>c2</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>chs</span><span class=p>,</span> <span class=n>chs</span><span class=p>[</span><span class=mi>1</span><span class=p>:]):</span>
</span></span><span class=line><span class=cl>    <span class=n>bigram</span> <span class=o>=</span> <span class=p>(</span><span class=n>c1</span><span class=p>,</span> <span class=n>c2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>counter</span><span class=p>[</span><span class=n>bigram</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>bigram</span><span class=p>,</span> <span class=n>frequency</span> <span class=ow>in</span> <span class=n>counter</span><span class=o>.</span><span class=n>most_common</span><span class=p>(</span><span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Frequency of </span><span class=si>{</span><span class=s1>&#39;&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>bigram</span><span class=p>)</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>frequency</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Frequency of n.: 6763
Frequency of a.: 6640
Frequency of an: 5438
Frequency of .a: 4410
Frequency of e.: 3983
</code></pre><h3 id=numericallization>Numericallization<a hidden class=anchor aria-hidden=true href=#numericallization>#</a></h3><p>As is known that computers are good at processing numerical data; however, they may not be efficient in dealing with text.
So our second step is to create two mappings: string to index and index to string.
These mappings are used to represent words or characters numerically.
This process is sometimes called numericalization.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>string</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>chars</span> <span class=o>=</span> <span class=n>string</span><span class=o>.</span><span class=n>ascii_lowercase</span>
</span></span><span class=line><span class=cl><span class=n>stoi</span> <span class=o>=</span> <span class=p>{</span><span class=n>s</span><span class=p>:</span> <span class=n>i</span><span class=o>+</span><span class=mi>1</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>s</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>chars</span><span class=p>)}</span>
</span></span><span class=line><span class=cl><span class=n>stoi</span><span class=p>[</span><span class=s2>&#34;.&#34;</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl><span class=n>itos</span> <span class=o>=</span> <span class=p>{</span><span class=n>i</span><span class=p>:</span> <span class=n>s</span> <span class=k>for</span> <span class=n>s</span><span class=p>,</span> <span class=n>i</span> <span class=ow>in</span> <span class=n>stoi</span><span class=o>.</span><span class=n>items</span><span class=p>()}</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>stoi</span><span class=p>,</span> <span class=n>itos</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}
</code></pre><h2 id=counting-approach>Counting Approach<a hidden class=anchor aria-hidden=true href=#counting-approach>#</a></h2><h3 id=frequency>Frequency<a hidden class=anchor aria-hidden=true href=#frequency>#</a></h3><p>Our first step is to obtain the frequencies of the bigrams.
Since we have a vocabulary of 27 characters-26 letters in lowercase plus 1 special character, we need a $27\times 27$ matrix to store the frequencies of all possible bigrams.
<a href=#fig-heatmap>Figure 1</a> is a heatmap of the calculated frequencies.
The darker the color, the higher the frequency of the bigram.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>N</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>27</span><span class=p>,</span> <span class=mi>27</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>int32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=p>(</span><span class=n>c1</span><span class=p>,</span> <span class=n>c2</span><span class=p>),</span> <span class=n>freq</span> <span class=ow>in</span> <span class=n>counter</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>  <span class=n>idx1</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>c1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>idx2</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>c2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=n>N</span><span class=p>[</span><span class=n>idx1</span><span class=p>,</span> <span class=n>idx2</span><span class=p>]</span> <span class=o>=</span> <span class=n>freq</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>16</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>N</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;Blues&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>27</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>27</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>chstr</span> <span class=o>=</span> <span class=n>itos</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+</span> <span class=n>itos</span><span class=p>[</span><span class=n>j</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>j</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>chstr</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s2>&#34;center&#34;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s2>&#34;bottom&#34;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;gray&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>j</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=n>N</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=n>ha</span><span class=o>=</span><span class=s2>&#34;center&#34;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s2>&#34;top&#34;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;gray&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>axis</span><span class=p>(</span><span class=s2>&#34;off&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><figure><img src=index_files/figure-markdown_strict/fig-heatmap-output-1.png id=fig-heatmap width=720 height=720 alt="Figure 1: A heatmap plot for frequencies of bigrams"><figcaption aria-hidden=true>Figure 1: A heatmap plot for frequencies of bigrams</figcaption></figure><h3 id=probability>Probability<a hidden class=anchor aria-hidden=true href=#probability>#</a></h3><p>To get the probability of each bigram, we want to normalize the matrix <code>N</code> by row.
Why? Because we want to know the probability of the character given the current character we have in the process of character generation, i.e., $P(next\ char | current\ char)$.
To avoid calculating $log0$ later on, we add 1 to the frequency of each bigram.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>P</span> <span class=o>=</span> <span class=p>(</span><span class=n>N</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>P</span> <span class=o>/=</span> <span class=n>P</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=maximum-likelihood>Maximum Likelihood<a hidden class=anchor aria-hidden=true href=#maximum-likelihood>#</a></h3><p><a href=https://www.wikiwand.com/en/Maximum_likelihood_estimation>Maximum likelihood</a> is a statistical method to estimate the parameters of a probability distribution based on observed data.
The goal of maximum likelihood is to find the values of the distribution&rsquo;s parameters that make the observed data most likely to have been generated by that distribution.
In our case, we want the next generated character comes from the probability distribution as much as possible.
How do we calculate the likelihood? It is the product of the probability of each bigram in a word.
$$ L(\theta) = P(X_1=x_1, X_2=x_2, &mldr;, X_n=x_n) = \Pi_i^n P(X_i=x_i)$$
For example, the likelihood of the word <em>good</em> is calculated as
$$Likelihood= P(".g") * P(&ldquo;go&rdquo;) * P(&ldquo;oo&rdquo;) * P(&ldquo;od&rdquo;) * P(&ldquo;d.&rdquo;) $$
$$ = 0.0209*0.0430*0.0146*0.0240*0.0936=2.9399e-8$$</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>calc_likelihood</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>False</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>word</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=s2>&#34;.&#34;</span> <span class=o>+</span> <span class=n>word</span> <span class=o>+</span> <span class=s2>&#34;.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>likelihood</span> <span class=o>=</span> <span class=mf>1.0</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>c1</span><span class=p>,</span> <span class=n>c2</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>word</span><span class=p>[</span><span class=mi>1</span><span class=p>:]):</span>
</span></span><span class=line><span class=cl>    <span class=n>idx1</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>c1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>idx2</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>c2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>prob</span> <span class=o>=</span> <span class=n>P</span><span class=p>[</span><span class=n>idx1</span><span class=p>,</span> <span class=n>idx2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>verbose</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;probability for </span><span class=si>{</span><span class=s1>&#39;&#39;</span><span class=o>.</span><span class=n>join</span><span class=p>((</span><span class=n>c1</span><span class=p>,</span> <span class=n>c2</span><span class=p>))</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>prob</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>likelihood</span> <span class=o>*=</span> <span class=n>prob</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>likelihood</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prob</span> <span class=o>=</span> <span class=n>calc_likelihood</span><span class=p>(</span><span class=s2>&#34;good&#34;</span><span class=p>,</span> <span class=n>verbose</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Likelihood for good is: </span><span class=si>{</span><span class=n>prob</span><span class=si>:</span><span class=s2>.4e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>probability for .g: 0.0209
probability for go: 0.0430
probability for oo: 0.0146
probability for od: 0.0240
probability for d.: 0.0936
Likelihood for good is: 2.9399e-08
</code></pre><p>Let&rsquo;s generate some words by randomly picking the bigram according to its probability using <code>torch.multinomial</code> function and calculate their likelihoods.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Generator</span><span class=p>()</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>420</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_words</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>out</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=n>ix</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>  <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=n>P</span><span class=p>[</span><span class=n>ix</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>ix</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>replacement</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>ix</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=k>break</span>
</span></span><span class=line><span class=cl>    <span class=n>out</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>itos</span><span class=p>[</span><span class=n>ix</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>generated_words</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>out</span><span class=p>),</span> <span class=n>calc_likelihood</span><span class=p>(</span><span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>out</span><span class=p>))</span><span class=o>.</span><span class=n>item</span><span class=p>()))</span>
</span></span><span class=line><span class=cl><span class=n>generated_words</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=o>-</span><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>gw</span><span class=p>,</span> <span class=n>lh</span> <span class=ow>in</span> <span class=n>generated_words</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Likelihood for </span><span class=si>{</span><span class=n>gw</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>lh</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Likelihood for jen: 0.0005491252522915602
Likelihood for jor: 0.0001786774955689907
Likelihood for she: 0.00017446796118747443
Likelihood for tais: 3.90183367926511e-06
Likelihood for anuir: 2.335933579900029e-08
</code></pre><p>It turns out <code>jen</code> which has the maximum likelihood 0.000549 is the winner in these 5 randomly generated words.
Remember that our goal is to maximize the likelihood of the word the model generates because the higher the likelihood, the better the model.
However, notice that the likelihoods for the generated words are too small, so applying a log function to each probability would make it easier to work with.
$$ logL(\theta) = log\Pi_i^n P(X_i=x_i)=\Sigma_i^nlogP(X_i=x_i)$$</p><p>Additionally, maximizing the likelihood is the same as maximizing the log-likelihood because the logarithm function is a monotonic increasing function, which is the same as minimizing the negative log-likelihood.
We prefer minimization to maximization in any optimization problem.
Let&rsquo;s calculate the average negative log-likelihood of our name dataset, which is 2.454679.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>calc_nll</span><span class=p>(</span><span class=n>word</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>word</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=s2>&#34;.&#34;</span> <span class=o>+</span> <span class=n>word</span> <span class=o>+</span> <span class=s2>&#34;.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>log_likelihood</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>c1</span><span class=p>,</span> <span class=n>c2</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>word</span><span class=p>[</span><span class=mi>1</span><span class=p>:]):</span>
</span></span><span class=line><span class=cl>    <span class=n>idx1</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>c1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>idx2</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>c2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>prob</span> <span class=o>=</span> <span class=n>P</span><span class=p>[</span><span class=n>idx1</span><span class=p>,</span> <span class=n>idx2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>log_prob</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>prob</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>log_likelihood</span> <span class=o>+=</span> <span class=n>log_prob</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=o>-</span><span class=n>log_likelihood</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>nlls</span> <span class=o>=</span> <span class=p>[</span><span class=n>calc_nll</span><span class=p>(</span><span class=n>w</span><span class=p>)</span> <span class=k>for</span> <span class=n>w</span> <span class=ow>in</span> <span class=n>words</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>ns</span> <span class=o>=</span> <span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>w</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span> <span class=k>for</span> <span class=n>w</span> <span class=ow>in</span> <span class=n>words</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Average negative log-likelihood: </span><span class=si>{</span><span class=nb>sum</span><span class=p>(</span><span class=n>nlls</span><span class=p>)</span><span class=o>/</span><span class=nb>sum</span><span class=p>(</span><span class=n>ns</span><span class=p>)</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Average negative log-likelihood: 2.454579
</code></pre><h2 id=neural-network>Neural Network<a hidden class=anchor aria-hidden=true href=#neural-network>#</a></h2><p>How does a neural network model fit in the character generation?
Think of it in this way: given the last generated character, we want the model to output a probability distribution for the next character, in which we can find the most likely character to follow it.
In other words, our task is to use the model to estimate the probability distribution based on the dataset rather than relying on counting the occurrences of each bigram.
As always, let&rsquo;s prepare the data in the first step.</p><h3 id=training-data-preparation>Training Data Preparation<a hidden class=anchor aria-hidden=true href=#training-data-preparation>#</a></h3><p>The training data is created using bigrams, where the first character is the input feature, and the second character is used as the target of the model.
Since feeding integers into a neural network and multiplying them with weights does not make sense, we need to transform them into a different format.
The most common method is one-hot encoding, which transforms each integer into a vector with all 0s except for a 1 at the index corresponding to the integer. PyTorch provides a built-in <code>torch.nn.functional.one_hot</code> function for one-hot encoding.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl><span class=n>xs</span><span class=p>,</span> <span class=n>ys</span> <span class=o>=</span> <span class=p>[],</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>words</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=n>chs</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=s2>&#34;.&#34;</span> <span class=o>+</span> <span class=n>word</span> <span class=o>+</span> <span class=s2>&#34;.&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=n>c1</span><span class=p>,</span> <span class=n>c2</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>chs</span><span class=p>,</span> <span class=n>chs</span><span class=p>[</span><span class=mi>1</span><span class=p>:]):</span>
</span></span><span class=line><span class=cl>    <span class=n>idx1</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>c1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>idx2</span> <span class=o>=</span> <span class=n>stoi</span><span class=p>[</span><span class=n>c2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>xs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>idx1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ys</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>idx2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># tensor function returns the same type as its original</span>
</span></span><span class=line><span class=cl><span class=n>xs</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>xs</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ys</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>ys</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>xenc</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>one_hot</span><span class=p>(</span><span class=n>xs</span><span class=p>,</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>27</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>xenc</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>torch.Size([228146, 27])
</code></pre><p>After applying one-hot encoding, we have a tensor <code>xenc</code> of shape $228146\times 27$.</p><h3 id=understanding-weights>Understanding Weights<a hidden class=anchor aria-hidden=true href=#understanding-weights>#</a></h3><p>The weight matrix of our model has the same shape as the matrix <code>N</code> above but is initialized with random values.
PyTorch&rsquo;s built-in function <code>torch.randn</code> gives us random numbers from a normal distribution with mean 0 and standard deviation 1, resulting in positive and negative values.
After multiplying the one-hot encoding matrix with weights, we obtain the output of the first layer, which may contain negative values.
However, we want the output to represent the probability of the next character, as we calculated above.
To achieve this, we can treat the output as the logarithm of the frequencies and apply the exponential function to obtain the positive values, which can be interpreted as the frequencies of the bigrams starting with the input feature.
Why? Because multiplying a one-hot encoding vector having a 1 at index <code>i</code>, with the weight matrix <code>W</code> is the same as getting the <code>ith</code> row of <code>W</code>.
And we want this frequency matrix to be close to the matrix <code>N</code> as close as possible.
If we further normalize the output over the rows, we can obtain the probability distribution of bigrams.
In fact, the last two steps, applying exponential function and normalization, of calculation are known as the <strong>softmax</strong> function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Generator</span><span class=p>()</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>420</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>W</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>27</span><span class=p>,</span> <span class=mi>27</span><span class=p>),</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>,</span> <span class=n>requires_grad</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># log-counts</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>xenc</span> <span class=o>@</span> <span class=n>W</span> <span class=c1># (228146, 27) x (27, 27)</span>
</span></span><span class=line><span class=cl><span class=c1># counts</span>
</span></span><span class=line><span class=cl><span class=n>counts</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># probability</span>
</span></span><span class=line><span class=cl><span class=n>probs</span> <span class=o>=</span> <span class=n>counts</span> <span class=o>/</span> <span class=n>counts</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>probs</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=nb>sum</span><span class=p>(</span><span class=n>probs</span><span class=p>[</span><span class=mi>1</span><span class=p>,:]))</span>
</span></span></code></pre></div><pre><code>torch.Size([228146, 27])
tensor(1.0000, grad_fn=&lt;AddBackward0&gt;)
</code></pre><h3 id=optimization>Optimization<a hidden class=anchor aria-hidden=true href=#optimization>#</a></h3><p>Remember that our goal is to approach the actual probabilities from the training data using maximum likelihood estimation.
As the training progresses, the model adjusts the weights in such a way that the predicted probabilities for the next character in a word are as close to the actual probabilities of the training data.
By minimizing the negative log-likelihood, we effectively minimize the distance between predicted and actual probabilities.
Let&rsquo;s take the first word, <code>emma</code>, as an example and see how the neural network calculates its loss.
This step is also called <strong>forward pass</strong>.
The first bigram is <code>.e</code> with the input <code>.</code> (index 0) and actual label <code>e</code> (index 5).
The one-hot encoding for <code>.</code> is <code>[1, 0, ..., 0]</code>, and the output probability for <code>e</code> is 0.0246.
Applying log and negation, we have the loss as 3.7050.
The same calculation applies to <code>em</code>, <code>mm</code>, <code>ma</code>, and <code>a.</code>.
Finally, we get the loss for <code>emma</code> is 3.6985.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>nlls</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>x</span> <span class=o>=</span> <span class=n>xs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=n>y</span> <span class=o>=</span> <span class=n>ys</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;-&#39;</span> <span class=o>*</span> <span class=mi>50</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;bigram example </span><span class=si>{</span><span class=n>i</span><span class=o>+</span><span class=mi>1</span><span class=si>}</span><span class=s1>: </span><span class=si>{</span><span class=n>itos</span><span class=p>[</span><span class=n>x</span><span class=p>]</span><span class=si>}</span><span class=s1> </span><span class=si>{</span><span class=n>itos</span><span class=p>[</span><span class=n>y</span><span class=p>]</span><span class=si>}</span><span class=s1> (indexes </span><span class=si>{</span><span class=n>x</span><span class=si>}</span><span class=s1>, </span><span class=si>{</span><span class=n>y</span><span class=si>}</span><span class=s1>)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;input to the neural network: </span><span class=si>{</span><span class=n>x</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;output probbabilities from the nn: </span><span class=si>{</span><span class=n>probs</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;label (actual next character): </span><span class=si>{</span><span class=n>y</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>p</span> <span class=o>=</span> <span class=n>probs</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>y</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;probability assigned by the nn to the correct character: </span><span class=si>{</span><span class=n>p</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>logp</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>p</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;log-likelihood: </span><span class=si>{</span><span class=n>logp</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>nll</span> <span class=o>=</span> <span class=o>-</span><span class=n>logp</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s1>&#39;negative log likelihood: </span><span class=si>{</span><span class=n>nll</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>nlls</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>nll</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Average negative log-likelihood, i.e., loss=</span><span class=si>{</span><span class=n>nlls</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>--------------------------------------------------
bigram example 1: . e (indexes 0, 5)
input to the neural network: 0
output probbabilities from the nn: tensor([0.0167, 0.0278, 0.0328, 0.0114, 0.0173, 0.0246, 0.0100, 0.0341, 0.1024,
        0.0259, 0.2364, 0.0219, 0.0422, 0.0108, 0.1262, 0.0647, 0.0130, 0.0162,
        0.0157, 0.0093, 0.0184, 0.0022, 0.0482, 0.0090, 0.0069, 0.0195, 0.0362],
       grad_fn=&lt;SelectBackward0&gt;)
label (actual next character): 5
probability assigned by the nn to the correct character: 0.024598384276032448
log-likelihood: -3.7050745487213135
negative log likelihood: 3.7050745487213135
--------------------------------------------------
bigram example 2: e m (indexes 5, 13)
input to the neural network: 5
output probbabilities from the nn: tensor([0.1219, 0.0087, 0.0157, 0.0546, 0.0067, 0.0149, 0.0185, 0.0338, 0.0110,
        0.0030, 0.0060, 0.0697, 0.0211, 0.0579, 0.0061, 0.0043, 0.0746, 0.0416,
        0.0264, 0.0611, 0.0823, 0.0124, 0.0179, 0.0129, 0.0374, 0.1633, 0.0162],
       grad_fn=&lt;SelectBackward0&gt;)
label (actual next character): 13
probability assigned by the nn to the correct character: 0.057898372411727905
log-likelihood: -2.8490660190582275
negative log likelihood: 2.8490660190582275
--------------------------------------------------
bigram example 3: m m (indexes 13, 13)
input to the neural network: 13
output probbabilities from the nn: tensor([0.3351, 0.0126, 0.0370, 0.0075, 0.0302, 0.0635, 0.0042, 0.0339, 0.0155,
        0.0512, 0.0080, 0.0283, 0.0557, 0.0171, 0.0388, 0.0103, 0.0507, 0.0398,
        0.0191, 0.0074, 0.0174, 0.0132, 0.0121, 0.0245, 0.0307, 0.0219, 0.0142],
       grad_fn=&lt;SelectBackward0&gt;)
label (actual next character): 13
probability assigned by the nn to the correct character: 0.017136109992861748
log-likelihood: -4.066567420959473
negative log likelihood: 4.066567420959473
--------------------------------------------------
bigram example 4: m a (indexes 13, 1)
input to the neural network: 13
output probbabilities from the nn: tensor([0.3351, 0.0126, 0.0370, 0.0075, 0.0302, 0.0635, 0.0042, 0.0339, 0.0155,
        0.0512, 0.0080, 0.0283, 0.0557, 0.0171, 0.0388, 0.0103, 0.0507, 0.0398,
        0.0191, 0.0074, 0.0174, 0.0132, 0.0121, 0.0245, 0.0307, 0.0219, 0.0142],
       grad_fn=&lt;SelectBackward0&gt;)
label (actual next character): 1
probability assigned by the nn to the correct character: 0.012621787376701832
log-likelihood: -4.372330665588379
negative log likelihood: 4.372330665588379
--------------------------------------------------
bigram example 5: a . (indexes 1, 0)
input to the neural network: 1
output probbabilities from the nn: tensor([0.0302, 0.0788, 0.0096, 0.0099, 0.0151, 0.1021, 0.0146, 0.0253, 0.0076,
        0.0107, 0.0429, 0.0286, 0.0371, 0.0437, 0.0168, 0.0133, 0.0129, 0.0075,
        0.0038, 0.0199, 0.0854, 0.0875, 0.1194, 0.1119, 0.0151, 0.0325, 0.0177],
       grad_fn=&lt;SelectBackward0&gt;)
label (actual next character): 0
probability assigned by the nn to the correct character: 0.030220769345760345
log-likelihood: -3.4992258548736572
negative log likelihood: 3.4992258548736572
Average negative log-likelihood, i.e., loss=3.6984527111053467
</code></pre><p>So how do we calculate the loss efficiently?
It turns out that we can pass all these row and column indices to the matrix, and then take log and mean afterwards.
The loss for the forward pass is 3.6374.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>probs</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>xs</span><span class=p>)),</span> <span class=n>ys</span><span class=p>]</span><span class=o>.</span><span class=n>log</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Overall loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Overall loss: 3.637367010116577
</code></pre><p>After obtaining the average loss from the forward pass, we need a backward pass to update the weights.
To do this, we need to make sure that the parameter <code>requires_grad</code> is set to <code>True</code> for the weight matrix <code>W</code>.
Next, we zero out all gradients to avoid the accumulation of gradients across batches.
We then call <code>loss.backward()</code> to compute the gradient of the oss with regard to each weight.
The gradient of a weight indicates how much that increasing that weight will affect the loss.
If it is positive, increasing the weight will increase the loss too.
Conversely, increasing the weight will decrease the loss if the gradient is negative.
For example, <code>W.grad[0, 0]=0.002339</code> means that <code>W[0, 0]</code> has a positive effect on the loss.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># set the gradient to zero</span>
</span></span><span class=line><span class=cl><span class=n>W</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl><span class=c1># backward pass</span>
</span></span><span class=line><span class=cl><span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span></code></pre></div><p>The next step is to update the weights and recalculate the averge loss.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>lr</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=n>W</span><span class=o>.</span><span class=n>data</span> <span class=o>+=</span> <span class=o>-</span><span class=n>lr</span> <span class=o>*</span> <span class=n>W</span><span class=o>.</span><span class=n>grad</span>
</span></span><span class=line><span class=cl><span class=c1># forward pass</span>
</span></span><span class=line><span class=cl><span class=n>logits</span> <span class=o>=</span> <span class=n>xenc</span> <span class=o>@</span> <span class=n>W</span>
</span></span><span class=line><span class=cl><span class=n>counts</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>probs</span> <span class=o>=</span> <span class=n>counts</span> <span class=o>/</span> <span class=n>counts</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>probs</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>xs</span><span class=p>)),</span> <span class=n>ys</span><span class=p>]</span><span class=o>.</span><span class=n>log</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Overall loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Overall loss: 3.6365137100219727
</code></pre><p>The overall loss is now 3.6365, which is slightly lower than before.
We can keep doing this <strong>gradient descent</strong> step until the model performance is good enough.
As we train more epochs, the overall loss is getting closer to the actual overall loss, which is 2.454579.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>lr</span> <span class=o>=</span> <span class=mi>50</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>301</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>logits</span> <span class=o>=</span> <span class=n>xenc</span> <span class=o>@</span> <span class=n>W</span>
</span></span><span class=line><span class=cl>  <span class=n>counts</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=n>probs</span> <span class=o>=</span> <span class=n>counts</span> <span class=o>/</span> <span class=n>counts</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>probs</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>xs</span><span class=p>)),</span> <span class=n>ys</span><span class=p>]</span><span class=o>.</span><span class=n>log</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=n>i</span> <span class=o>%</span> <span class=mi>50</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Epochs: </span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>, loss: </span><span class=si>{</span><span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>W</span><span class=o>.</span><span class=n>grad</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>  <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=n>W</span><span class=o>.</span><span class=n>data</span> <span class=o>+=</span> <span class=o>-</span><span class=n>lr</span> <span class=o>*</span> <span class=n>W</span><span class=o>.</span><span class=n>grad</span>
</span></span></code></pre></div><pre><code>Epochs: 0, loss: 3.6365137100219727
Epochs: 50, loss: 2.4955990314483643
Epochs: 100, loss: 2.4727954864501953
Epochs: 150, loss: 2.4657769203186035
Epochs: 200, loss: 2.4625258445739746
Epochs: 250, loss: 2.4606406688690186
Epochs: 300, loss: 2.459399700164795
</code></pre><p>Note that our current neural network only has one hidden layer.
We can add more hidden layers to improve the model performance.
Additionally, we can add a <strong>regularization</strong> item (e.g., the mean of the square of all weights) in the loss function to prevent overfitting.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=o>-</span><span class=n>probs</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>xs</span><span class=p>)),</span> <span class=n>ys</span><span class=p>]</span><span class=o>.</span><span class=n>log</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span> <span class=o>+</span> <span class=mf>0.01</span> <span class=o>*</span> <span class=p>(</span><span class=n>W</span> <span class=o>**</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>tensor(2.4834, grad_fn=&lt;AddBackward0&gt;)
</code></pre><p>In this case, the optimization has two components&ndash;average negative log-likelihood and mean of the square of weights.
The regularization item works like a force to squeeze the weights and make them close to zeros as much as possible.
The last step is to sample characters from the neural network model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>g</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>Generator</span><span class=p>()</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>420</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>nn_generated_words</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>5</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=n>out</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=n>idx</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>  <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>xenc</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>one_hot</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=n>idx</span><span class=p>]),</span> <span class=n>num_classes</span><span class=o>=</span><span class=mi>27</span><span class=p>)</span><span class=o>.</span><span class=n>float</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>logits</span> <span class=o>=</span> <span class=n>xenc</span> <span class=o>@</span> <span class=n>W</span>
</span></span><span class=line><span class=cl>    <span class=n>counts</span> <span class=o>=</span> <span class=n>logits</span><span class=o>.</span><span class=n>exp</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>probs</span> <span class=o>=</span> <span class=n>counts</span> <span class=o>/</span> <span class=n>counts</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdim</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>multinomial</span><span class=p>(</span><span class=n>probs</span><span class=p>,</span> <span class=n>num_samples</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>replacement</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>generator</span><span class=o>=</span><span class=n>g</span><span class=p>)</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>idx</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=k>break</span>
</span></span><span class=line><span class=cl>    <span class=n>out</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>itos</span><span class=p>[</span><span class=n>idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>  <span class=n>nn_generated_words</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>out</span><span class=p>),</span> <span class=n>calc_likelihood</span><span class=p>(</span><span class=s2>&#34;&#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>out</span><span class=p>))</span><span class=o>.</span><span class=n>item</span><span class=p>()))</span>
</span></span><span class=line><span class=cl><span class=n>nn_generated_words</span><span class=o>.</span><span class=n>sort</span><span class=p>(</span><span class=n>key</span><span class=o>=</span><span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=o>-</span><span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>gw</span><span class=p>,</span> <span class=n>lh</span> <span class=ow>in</span> <span class=n>nn_generated_words</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Likelihood for </span><span class=si>{</span><span class=n>gw</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>lh</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><pre><code>Likelihood for jen: 0.0005491252522915602
Likelihood for jor: 0.0001786774955689907
Likelihood for she: 0.00017446796118747443
Likelihood for tais: 3.90183367926511e-06
Likelihood for anuir: 2.335933579900029e-08
</code></pre><p>We are using the same seed for the generator.
The words generated from the neural network are exactly the same as those generated from the probability table above, which is what we want to see.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://gejun.name/tags/bigram/>bigram</a></li><li><a href=https://gejun.name/tags/pytorch/>pytorch</a></li></ul><nav class=paginav><a class=prev href=https://gejun.name/natural-language-processing/building-makemore-mlp/><span class=title>« Prev</span><br><span>Multilayer Perceptron (MLP)</span></a>
<a class=next href=https://gejun.name/datasci-simplified/back-to-basics/><span class=title>Next »</span><br><span>Back to Basics - Data Science</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Bigram Character-level Language Model on twitter" href="https://twitter.com/intent/tweet/?text=Bigram%20Character-level%20Language%20Model&amp;url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore%2f&amp;hashtags=bigram%2cpytorch"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Bigram Character-level Language Model on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore%2f&amp;title=Bigram%20Character-level%20Language%20Model&amp;summary=Bigram%20Character-level%20Language%20Model&amp;source=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Bigram Character-level Language Model on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore%2f&title=Bigram%20Character-level%20Language%20Model"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Bigram Character-level Language Model on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Bigram Character-level Language Model on whatsapp" href="https://api.whatsapp.com/send?text=Bigram%20Character-level%20Language%20Model%20-%20https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Bigram Character-level Language Model on telegram" href="https://telegram.me/share/url?text=Bigram%20Character-level%20Language%20Model&amp;url=https%3a%2f%2fgejun.name%2fnatural-language-processing%2fbuilding-makemore%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://giscus.app/client.js data-repo=zhugejun/zhugejun.github.io data-repo-id data-category=Announcements data-category-id data-mapping=pathname data-reactions-enabled=1 data-theme=preferred_color_scheme data-language=en crossorigin=anonymous async></script><noscript>Please enable JavaScript to view the comments powered by giscus.</noscript></article></main><footer class=footer><span>&copy; 2023 <a href=https://gejun.name>Gejun's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>