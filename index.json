[{"content":" In Part2, we constructed a straightforward MLP model to generate characters based on 32k popular names. In this lecture, Andrej guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model. We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.\nData Preparation Let\u0026rsquo;s first import the necessary libraries and get the data ready. We will use the tiny shakespeare dataset, featured in Andrej Karpathy\u0026rsquo;s blog post The Unreasonable Effectiveness of Recurrent Neural Networks.\nimport math import requests import torch from torch import nn import torch.nn.functional as F import numpy as np import matplotlib.pyplot as plt device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; data_url = \u0026#34;https://t.ly/u1Ax\u0026#34; text = requests.get(data_url).text # building vocabulary chars = sorted(list(set(text))) vocab_size = len(chars) print(f\u0026#34;Vocabulary size: {vocab_size}\u0026#34;) print(f\u0026#34;Vocabulary: {repr(\u0026#39;\u0026#39;.join(chars))}\u0026#34;) # mappings stoi = {c: i for i, c in enumerate(chars)} itos = {v: k for k, v in stoi.items()} def encode(s): return [stoi[c] for c in s] def decode(l): return \u0026#39;\u0026#39;.join([itos[i] for i in l]) Vocabulary size: 65 Vocabulary: \u0026quot;\\n !$\u0026amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\u0026quot; We have 65 characters, including all lower- and upper-case letters and a few special characters, \\n !$\u0026amp;',-.3:;?. Next, we split the data into two parts: 90% of the dataset for training and 10% for validation.\n# create tensor data = torch.tensor(encode(text), dtype=torch.long) n = int(0.9*len(data)) train_data = data[:n] val_data = data[n:] print(train_data.shape) print(val_data.shape) torch.Size([1003854]) torch.Size([111540]) Training Data Feeding the entire text to the transformer all at once can be computationally expensive and prohibitive. To address this issue, neural network models use batch processing techniques to update the model\u0026rsquo;s weights and biases. This technique involves dividing the training dataset into smaller subsets, or batches, of size batch_size. The batches are then processed separately by the neural network to update the model\u0026rsquo;s parameters. For a character generation model, we need a sequence of characters as our training sample, which can be considered a time dimension. For the sample below, the input is [18] and the target is 47 at time 0, and the input is [18, 47] and the target is 56, and so on.\nblock_size = 8 x = train_data[:block_size] y = train_data[1:block_size+1] for t in range(block_size): context = x[:t+1] target = y[t] print(f\u0026#34;Time: {t}, input: {context}, target: {target}\u0026#34;) Time: 0, input: tensor([18]), target: 47 Time: 1, input: tensor([18, 47]), target: 56 Time: 2, input: tensor([18, 47, 56]), target: 57 Time: 3, input: tensor([18, 47, 56, 57]), target: 58 Time: 4, input: tensor([18, 47, 56, 57, 58]), target: 1 Time: 5, input: tensor([18, 47, 56, 57, 58, 1]), target: 15 Time: 6, input: tensor([18, 47, 56, 57, 58, 1, 15]), target: 47 Time: 7, input: tensor([18, 47, 56, 57, 58, 1, 15, 47]), target: 58 To create our training data, we select a sequence starting from the character of a fixed size block_size in each batch. We then create our input and target along the time dimension inside each sequence, resulting in batch_size time block_size training examples. The example below shows that there are $4\\times 8=32$ training examples in each batch as we have 4 sequences of 8 characters each.\nbatch_size = 4 block_size = 8 def get_batch(split): data = train_data if split == \u0026#34;train\u0026#34; else val_data idx = torch.randint(len(data) - block_size, (batch_size,)) x = torch.stack([data[i:i+block_size] for i in idx]) y = torch.stack([data[i+1:i+block_size+1] for i in idx]) x, y = x.to(device), y.to(device) return x, y x_batch, y_batch = get_batch(\u0026#34;train\u0026#34;) print(x_batch.shape, y_batch.shape) for b in range(batch_size): print(f\u0026#34;---------- Batch {b} ----------\u0026#34;) for t in range(block_size): context = x_batch[b, :t+1] target = y_batch[b, t] print(f\u0026#34;Time: {t}, input: {context}, target: {target}\u0026#34;) torch.Size([4, 8]) torch.Size([4, 8]) ---------- Batch 0 ---------- Time: 0, input: tensor([53], device='cuda:0'), target: 56 Time: 1, input: tensor([53, 56], device='cuda:0'), target: 58 Time: 2, input: tensor([53, 56, 58], device='cuda:0'), target: 46 Time: 3, input: tensor([53, 56, 58, 46], device='cuda:0'), target: 11 Time: 4, input: tensor([53, 56, 58, 46, 11], device='cuda:0'), target: 1 Time: 5, input: tensor([53, 56, 58, 46, 11, 1], device='cuda:0'), target: 41 Time: 6, input: tensor([53, 56, 58, 46, 11, 1, 41], device='cuda:0'), target: 53 Time: 7, input: tensor([53, 56, 58, 46, 11, 1, 41, 53], device='cuda:0'), target: 51 ---------- Batch 1 ---------- Time: 0, input: tensor([52], device='cuda:0'), target: 52 Time: 1, input: tensor([52, 52], device='cuda:0'), target: 53 Time: 2, input: tensor([52, 52, 53], device='cuda:0'), target: 58 Time: 3, input: tensor([52, 52, 53, 58], device='cuda:0'), target: 1 Time: 4, input: tensor([52, 52, 53, 58, 1], device='cuda:0'), target: 46 Time: 5, input: tensor([52, 52, 53, 58, 1, 46], device='cuda:0'), target: 47 Time: 6, input: tensor([52, 52, 53, 58, 1, 46, 47], device='cuda:0'), target: 58 Time: 7, input: tensor([52, 52, 53, 58, 1, 46, 47, 58], device='cuda:0'), target: 1 ---------- Batch 2 ---------- Time: 0, input: tensor([35], device='cuda:0'), target: 43 Time: 1, input: tensor([35, 43], device='cuda:0'), target: 56 Time: 2, input: tensor([35, 43, 56], device='cuda:0'), target: 58 Time: 3, input: tensor([35, 43, 56, 58], device='cuda:0'), target: 1 Time: 4, input: tensor([35, 43, 56, 58, 1], device='cuda:0'), target: 58 Time: 5, input: tensor([35, 43, 56, 58, 1, 58], device='cuda:0'), target: 46 Time: 6, input: tensor([35, 43, 56, 58, 1, 58, 46], device='cuda:0'), target: 53 Time: 7, input: tensor([35, 43, 56, 58, 1, 58, 46, 53], device='cuda:0'), target: 59 ---------- Batch 3 ---------- Time: 0, input: tensor([53], device='cuda:0'), target: 59 Time: 1, input: tensor([53, 59], device='cuda:0'), target: 50 Time: 2, input: tensor([53, 59, 50], device='cuda:0'), target: 42 Time: 3, input: tensor([53, 59, 50, 42], device='cuda:0'), target: 1 Time: 4, input: tensor([53, 59, 50, 42, 1], device='cuda:0'), target: 41 Time: 5, input: tensor([53, 59, 50, 42, 1, 41], device='cuda:0'), target: 46 Time: 6, input: tensor([53, 59, 50, 42, 1, 41, 46], device='cuda:0'), target: 53 Time: 7, input: tensor([53, 59, 50, 42, 1, 41, 46, 53], device='cuda:0'), target: 54 BigramLanguageModel Let\u0026rsquo;s rewrite our previous bigram model. Here is the main part of the model we built in Part 1.\nW = torch.randn((27, 27), requires_grad=True) logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) Base model From Part 2, we learned how to represent a token with a fixed-length, real-valued, and learnable vector, which is known as token embedding. The embedding matrix can be initialized by nn.Embedding where num_embeddings refers to the vocabulary size, and embedding_dim refers to the length of the feature vector. For consistency with the original paper, we will use d_model to represent the feature vector\u0026rsquo;s length, which will be set to 64 instead of the vocabulary size. As a result, we need to create another linear layer to ensure that the output dimension is the same as the vocabulary size.\nIt\u0026rsquo;s worth noting that we cannot compute the cross-entropy for a 3-dimensional matrix, as seen from the documentation of cross_entropy function. Therefore, we need to reshape the logits and targets before computing it.\ntorch.manual_seed(42) batch_size = 32 d_model = 64 # B: batch_size # T: time, up to block_size # C: d_model # 65: vocabulary size class BigramLanguageModel(nn.Module): def __init__(self, vocab_size): super().__init__() self.token_embedding_table = nn.Embedding(vocab_size, d_model) # 65, C self.output_linear = nn.Linear(d_model, vocab_size) # C, 65 def forward(self, idx, targets=None): # idx: B, T embedded = self.token_embedding_table(idx) # B, T, C logits = self.output_linear(embedded) # B, T, 65 # there is no target when predicting if targets is None: loss = None else: B, T, C = logits.shape logits = logits.view(B*T, C) # N, C targets = targets.view(B*T) # N loss = F.cross_entropy(logits, targets) return logits, loss def generate(self, idx, max_length): for _ in range(max_length): logits, _ = self(idx) # focus on the char on last time stamp because it\u0026#39;s a bigram model logits = logits[:, -1, :] # B, C probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) # concatenate the new generated to the old ones idx = torch.cat((idx, idx_next), dim=1) return idx base_model = BigramLanguageModel(vocab_size).to(device) idx = torch.zeros((1, 1), dtype=torch.long, device=device) print(decode(base_model.generate(idx, max_length=100).squeeze().tolist())) dF3unFC;RnXbzDP'CnT-P.lBuYkUWdXRaRnqDCk,b!:UE$J,uuheZqKPXEPYMYSAxKlRpvwisS.MIwITP$YqrgGRpP.AwYluRWGI Certainly, the 100 characters generated at this point are not meaningful as the model has not been trained yet.\nTraining optimizer = torch.optim.AdamW(base_model.parameters(), lr=1e-3) # training epochs = 10000 for epoch in range(epochs): x_batch, y_batch = get_batch(\u0026#34;train\u0026#34;) logits, loss = base_model(x_batch, y_batch) optimizer.zero_grad(set_to_none=True) loss.backward() optimizer.step() if epoch % 1000 == 0 or epoch == epochs - 1: print(f\u0026#34;Epoch {epoch}: {loss.item()}\u0026#34;) # starting with [[0]] idx = torch.zeros((1, 1), dtype=torch.long, device=device) print(decode(base_model.generate(idx, max_length=100).squeeze().tolist())) Epoch 0: 4.440201282501221 Epoch 1000: 2.5844924449920654 Epoch 2000: 2.469000816345215 Epoch 3000: 2.473245859146118 Epoch 4000: 2.4555399417877197 Epoch 5000: 2.5115771293640137 Epoch 6000: 2.3323276042938232 Epoch 7000: 2.331480026245117 Epoch 8000: 2.436919927597046 Epoch 9000: 2.473867893218994 Epoch 9999: 2.636822462081909 Tody inde eve d tlakemang yofowhas Thind. UCESer ur thathapr me machan fl haisu d iere--sthurore ce The generated characters appear more word-like than before, but most are misspelled because the bigram model only generates a new character based on the last generated character. To improve our model\u0026rsquo;s performance, we need a way to incorporate information from previously generated characters up to block_size. One solution is to use a bag-of-words model to extract features from previously generated characters. In a bag-of-words model, a text is treated as a bag of tokens, disregarding grammar and order. In the next section, we will introduce the transformer architecture from the classic paper, Attention is all you need. We will explain what attention is, how to calculate, and most importantly, how to understand it intuitively. Furthermore, we will implement it step by step and see how it improves our model\u0026rsquo;s performance.\nTransformer Architecture The transformer model architecture from the paper is shown below.\nLet\u0026rsquo;s first clarify what an encoder is. According to the paper:\n\u0026ldquo;The encoder maps an input sequence of symbol representations $(x_1, \u0026hellip;, x_n)$ to a sequence of continuous representations $z=(z_1, \u0026hellip;, z_n)$. It converts an input sequence of tokens into a sequence of embedding vectors, often called a hidden state. The encoder is composed of a stack of encoder layers, which are used to update the input embeddings to produce representations that encode some contextual information in the sequence.\u0026rdquo;\nIn the transformer architecture shown above, the encoder is on the left side inside the blue box, and it contains multiple encoder layers. The encoder compresses and extracts important information from the input sequence while discarding the irrelevant information.\nNext, let\u0026rsquo;s see what a decoder is. The decoder is inside the red box on the right side of the transformer architecture. It is also composed of a stack of decoder layers, which are similar to encoder layers except that they add an extra masked layer in the multi-head attention.\nLast but not least, the state generated from the encoder is passed to the decoder and generates the output sequence, which is referred to as cross-attention. A decoder uses the encoder\u0026rsquo;s hidden state to iteratively generate an output sequence of tokens, one at a time.\nGPT, which stands for Generative Pretrained Transformer, focuses on the decoder part. Therefore, our model architecture becomes the following.\nIn the next few sections, we will build the model from bottom to top. Since the input embedding stays the same, we will skip the input embedding section and talk about positional embedding.\nPositional Embedding The embedding of input tokens alone does not capture any information about their relative positions within the sequence. Hence a positional embedding is introduced to inject this information. According to the paper, there are multiple ways for positional embeddings, with some being fixed while others are learnable. For our implementation, we will use a learnable positional embedding with the same dimension as the token embedding, which is d_model. The num_embeddings parameter in the nn.Embedding function will be set to block_size since our training sequence has a maximum length of block_size.\nLet\u0026rsquo;s dive into the dimensions of the input tokens. The input tokens have two dimensions: the batch dimension, which indicates how many independent sequences the model processes in parallel, and the time dimension, which records the current position within the sequence up to a maximum length of block_size. After the input tokens pass through the token and positional embedding layers, they will have an additional channel dimension, which is a convention borrowed from computer vision. For simplicity, we will use B, T, and C to denote the batch, time, and channel dimensions, respectively.\nclass BigramLanguageModel(nn.Module): def __init__(self, vocab_size): super().__init__() self.token_embedding_table = nn.Embedding(vocab_size, d_model) # position embedding table self.position_embedding_table = nn.Embedding(block_size, d_model) self.output_linear = nn.Linear(d_model, vocab_size) def forward(self, idx, targets=None): # idx: B, T B, T = idx.shape token_embed = self.token_embedding_table(idx) # B, T, C posit_embed = self.position_embedding_table(torch.arange(T, device=device)) # T, C # sum of token and positional embeddings x = token_embed + posit_embed # B, T, C logits = self.output_linear(x) # B, T, vocab_size if targets is None: loss = None else: B, T, C = logits.shape logits = logits.view(B*T, C) # (N, C) targets = targets.view(B*T) # (N) loss = F.cross_entropy(logits, targets) return logits, loss base_model = BigramLanguageModel(vocab_size).to(device) optimizer = torch.optim.AdamW(base_model.parameters(), lr=1e-3) epochs = 10000 for epoch in range(epochs): x_batch, y_batch = get_batch(\u0026#34;train\u0026#34;) logits, loss = base_model(x_batch, y_batch) optimizer.zero_grad(set_to_none=True) loss.backward() optimizer.step() if epoch % 1000 == 0 or epoch == epochs - 1: print(f\u0026#34;Epoch {epoch}: {loss.item()}\u0026#34;) Epoch 0: 4.435860633850098 Epoch 1000: 2.538156270980835 Epoch 2000: 2.5488555431365967 Epoch 3000: 2.479320764541626 Epoch 4000: 2.3083598613739014 Epoch 5000: 2.472010850906372 Epoch 6000: 2.5080037117004395 Epoch 7000: 2.4842913150787354 Epoch 8000: 2.3710641860961914 Epoch 9000: 2.4978179931640625 Epoch 9999: 2.416473627090454 Attention What is attention?\n\u0026ldquo;An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\u0026rdquo;\nWe can compute the attention score using the following steps as described in the paper.\n$$Attention(Q,K,V)=softmax\\bigl( \\frac{QK^T}{\\sqrt{d_k}}\\bigr) V$$\nTo better understand the attention formula above, it\u0026rsquo;s helpful to review some linear algebra concepts.\nDot Product The dot product of two Euclidean vectors $\\vec{a}$ and $\\vec{b}$ is defined by\n$$\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^n a_ib_i$$\nwhere $n$ is the length of the vectors.\nGeometrically, the dot product of two vectors is equal to the product of their magnitudes and the cosine of the angle between them. Specifically, if $\\theta$ is the angle between $\\vec{a}$ and $\\vec{b}$, then\n$$\\vec{a} \\cdot \\vec{b} = |a| \\cdot |b| cos(\\theta)$$\nsource\nThe quantity $|a|cos(\\theta)$ is the scalar projection of $\\vec{a}$ onto $\\vec{b}$. The higher the product, the more similar two vectors. Let\u0026rsquo;s take the learned embedding from our last model and compute the dot products of some tokens from our vocabulary.\nchar1 = \u0026#39;a\u0026#39; char2 = \u0026#39;z\u0026#39; char3 = \u0026#39;e\u0026#39; token_embeddings = base_model.token_embedding_table.weight def calc_dp(char1, char2): with torch.no_grad(): embed1 = token_embeddings[stoi[char1]] embed2 = token_embeddings[stoi[char2]] return sum(embed1 * embed2) print(f\u0026#34;Dot product of {char1} and {char1}: {calc_dp(char1, char1):.6f}\u0026#34;) print(f\u0026#34;Dot product of {char1} and {char2}: {calc_dp(char1, char2):.6f}\u0026#34;) print(f\u0026#34;Dot product of {char1} and {char3}: {calc_dp(char1, char3):.6f}\u0026#34;) Dot product of a and a: 78.494980 Dot product of a and z: -14.060809 Dot product of a and e: 12.071777 The dot product of the feature vectors of a and itself is much higher than with e or z. Also, the results show that a is more similar to e then to z.\nAttention Score Every token in the input sequence generates a query vector and a key vector of the same dimension. This operation is called self-attention because $Q$, $V$, and $T$ are all derived from the same source in GPT. The dot product of the query and key vectors measures their similarity.\nLet $X_{m\\times n}$ and $W$ denote the embedding matrix of the input sequence and the weight of the linear transformation, where $m$ is the number of tokens, $n$ is the token dimension, and $k$ is the output dimension of the linear transformation or the head size of our attention. Each row represents the token embedding for each token in the input. Then, we apply three linear transformations on $X$ to project it onto 3 new vector spaces:\n$X_{m\\times n} \\cdot W^Q_{n\\times k} = Q_{m\\times k}$ to obtain the query space. $X_{m\\times n} \\cdot W^K_{n\\times k} = K_{m\\times k}$ to obtain the key space. $X_{m\\times n} \\cdot W^V_{n\\times k} = V_{m\\times k}$ to obtain the value space. $Q\\cdot K^T$ is the attention score matrix, having a shape of $m \\times m$. The larger the value, the closer the vectors and hence the more attention.\nLet\u0026rsquo;s take the learned token and positional embeddings from our previous model, apply the query and key transformations, and calculate the attention scores of the sequence sea.\nsequence = \u0026#34;sea\u0026#34; # get positional embeddings from model position_embeddings = base_model.position_embedding_table.weight tokens = torch.tensor([stoi [c] for c in sequence]) positions = torch.tensor([i for i in range(len(sequence))]) # final embedding matrix for a given sequence embed = token_embeddings[tokens] + position_embeddings[positions] # query and vector weights d_k = 16 torch.manual_seed(42) q = nn.Linear(embed.shape[1], d_k, bias=False).to(device) k = nn.Linear(embed.shape[1], d_k, bias=False).to(device) # query and key space with torch.no_grad(): Q = q(embed) K = k(embed) # similarity between query and keys score = Q @ K.T print(score) tensor([[ 1.5712, -2.8564, 3.0652], [ 1.6477, 0.1216, -0.4353], [-6.8497, -1.1358, -0.8100]], device='cuda:0') The attention score vector for e is [ 1.6477, 0.1216, -0.4353] However, the dot products may become too large in magnitude when the head size $d_k$ is large, which can result in extremely small gradients after applying the softmax function. To mitigate this issue, the scores are scaled by multiplying with the factor $\\frac{1}{\\sqrt{d_k}}$, as suggested in the paper.\nwith torch.no_grad(): score /= math.sqrt(d_k) score = F.softmax(score, dim=-1) print(score) tensor([[0.3593, 0.1188, 0.5220], [0.4392, 0.2999, 0.2609], [0.1031, 0.4302, 0.4667]], device='cuda:0') After scaling, the attention score vector for token e in sea becomes [0.4392, 0.2999, 0.2609]. This implies that the token s requires more attention than the tokens e and a.\nWait a minute! Why does the token e pay attention to the future token a in a GPT model? It is cheating in this way. How can we preserve the information from the previous tokens while not peeking the future tokens? The masking layer.\nMasking Where exactly do we apply a masking layer? Since we want to use a softmax function to normalize the attention scores until the current position so that the divided attention sums to one, it should be applied after calculating the unscaled attention score and before the softmax layer. In this way, we can exclude the future tokens. To implement this masking, we will use a PyTorch built-in function, torch.tril, which preserves the original values for the lower triangular part of the matrix while setting the upper part to zero. In our case, we replace the scores in the upper triangular part of the matrix with a very small number, such as float(\u0026quot;-inf\u0026quot;), so that they will become zeros after applying the softmax function.\nwith torch.no_grad(): mask = torch.tril(torch.ones(embed.shape[0], embed.shape[0])).to(device) score = score.masked_fill(mask == 0, float(\u0026#34;-inf\u0026#34;)) score = F.softmax(score, dim=-1) print(score) tensor([[1.0000, 0.0000, 0.0000], [0.5348, 0.4652, 0.0000], [0.2614, 0.3626, 0.3760]], device='cuda:0') Now, the scaled attention vector for e becomes [0.5348, 0.4652, 0.0000], indicating that the model pays roughly half of its attention to tokens s and e when it reaches token e while completely ignoring the future token a.\nWeighted Sum Finally, we obtain a new adjusted embedding for each token in the context by multiplying the attention matrix with the value matrix $V$.\nv = nn.Linear(embed.shape[1], d_k, bias=False).to(device) with torch.no_grad(): V = v(embed) new_embed = score @ V print(new_embed) tensor([[ 0.0959, 0.4068, -0.2983, 0.8456, -1.6365, 0.9545, -0.5414, 2.2582, -0.3868, 1.1196, 1.6244, 0.3545, -1.1479, 0.4165, -0.7899, -0.7008], [ 0.1352, -0.2616, -0.4122, 0.1182, -1.2960, 0.5224, -0.3819, 1.3335, -0.1463, 0.2113, 0.8228, -0.0095, -0.8548, 0.0567, -0.5980, -0.3525], [-0.4126, -0.4585, -0.2760, 0.0813, -0.9609, 0.2358, -0.3887, 0.7906, 0.0084, -0.1094, 0.3198, -0.5582, -0.7782, 0.4525, -0.1208, 0.1493]], device='cuda:0') To put it in another way, we force the tokens to look at each other by multiplying the attention scores with the value matrix $V$. This helps to adjust the value matrix to represent the entire sequence better as training progresses.\nDemystifying QKV How do we understand attention from intuition? Here is a great answer from Cross Validated.\nThe key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).\nsource\nHere are the intuitive meaning of these matrices:\nThe query matrix represents a piece of information we are looking for in a query we have. The key matrix is intuitively meant to represent the relevance of each word to our query. And the key matrix represents how important each word is to my overall query. The value matrix intuitively represents the contextless meaning of our input tokens. Imagine that you\u0026rsquo;re at the supermarket buying all the ingredients you need for your dinner. You have the dish\u0026rsquo;s recipe, and the ingredients (query) are what you look for in a supermarket. Scanning the shelves, you look at the labels (keys) and check whether they match an ingredient on your list. You are determining the similarity between query and keys. If you have a match, you take the item (value) from the shelf.\nLet\u0026rsquo;s put the attention layer into a single Head class.\nclass Head(nn.Module): def __init__(self, d_k): super().__init__() self.query = nn.Linear(d_model, d_k, bias=False) # C, d_k self.key = nn.Linear(d_model, d_k, bias=False) # C, d_k self.value = nn.Linear(d_model, d_k, bias=False) # C, d_k # not a model parameter self.register_buffer(\u0026#39;tril\u0026#39;, torch.tril(torch.ones(block_size, block_size))) # block_size, block_size def forward(self, x): B, T, C = x.shape q = self.query(x) # B, T, d_k k = self.key(x) # B, T, d_k score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(C) # B, T, T score = score.masked_fill(self.tril[:T, :T] == 0, float(\u0026#34;-inf\u0026#34;)) # B, T, T score = F.softmax(score, dim=-1) # B, T, T v = self.value(x) # B, T, d_k out = score @ v # (B, T, T)@(B, T, d_k) = (B, T, d_k) return out To ensure compatibility with matrix multiplication, we need to set the head size as the embedding dimension, d_model, because we currently only have one head layer. However, we will not train this model at this moment.\nclass BigramLanguageModel(nn.Module): def __init__(self, vocab_size): super().__init__() self.token_embedding_table = nn.Embedding(vocab_size, d_model) self.position_embedding_table = nn.Embedding(block_size, d_model) self.self_attn = Head(d_model) self.output_linear = nn.Linear(d_model, vocab_size) def forward(self, idx, targets=None): B, T = idx.shape token_embed = self.token_embedding_table(idx) posit_embed = self.position_embedding_table(torch.arange(T, device=device)) x = token_embed + posit_embed # apply self attention x = self.self_attn(x) logits = self.output_linear(x) if targets is None: loss = None else: B, T, C = logits.shape logits = logits.view(B*T, C) targets = targets.view(B*T) loss = F.cross_entropy(logits, targets) return logits, loss def generate(self, idx, max_length): for _ in range(max_length): logits, loss = self(idx[:, -block_size:]) logits = logits[:, -1, :] probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx, idx_next), dim=1) return idx Multi-head Attention As an old saying goes, two heads are better than one. By having multiple heads, we can apply multiple transformations to the embeddings. Each projection has its own set of learnable parameters, which enables the self-attention layer to focus on different semantic aspects of the sequence. We will denote the number of heads as h.\nclass MultiHeadAttention(nn.Module): def __init__(self, h, d_k): super.__init__() self.heads = nn.ModuleList([Head(d_k) for _ in range(h)]) def forward(self, x): return torch.cat([head(x) for head in self.heads], dim=-1) # B, T, C Dropout Dropout was proposed in Dropout: A Simple Way to Prevent Neural Networks from Overfitting by Nitish Srivastava et al. in 2014. In this technique, a certain proportion of neurons are randomly dropped out during training to prevent overfitting.\nWe apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.\nsource\nWe will apply PyTorch\u0026rsquo;s built-in function nn.Dropout to our Head and MultiHeadAttention layers.\ndropout = 0.1 class Head(nn.Module): def __init__(self, d_k): super().__init__() self.query = nn.Linear(d_model, d_k, bias=False) # C, d_k self.key = nn.Linear(d_model, d_k, bias=False) # C, d_k self.value = nn.Linear(d_model, d_k, bias=False) # C, d_k # not a model parameter self.register_buffer(\u0026#39;tril\u0026#39;, torch.tril(torch.ones(block_size, block_size))) # block_size, block_size self.dropout = nn.Dropout(dropout) def forward(self, x): B, T, C = x.shape q = self.query(x) # B, T, d_k k = self.key(x) # B, T, d_k score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(C) # B, T, T score = score.masked_fill(self.tril[:T, :T] == 0, float(\u0026#34;-inf\u0026#34;)) # B, T, T score = F.softmax(score, dim=-1) # B, T, T score = self.dropout(score) v = self.value(x) # B, T, d_k out = score @ v # (B, T, T)@(B, T, d_k) = (B, T, d_k) return out class MultiHeadAttention(nn.Module): def __init__(self, h, d_k): super.__init__() self.heads = nn.ModuleList([Head(d_k) for _ in range(h)]) self.dropout = nn.Dropout(dropout) def forward(self, x): x = torch.cat([head(x) for head in self.heads], dim=-1) # B, T, C x = self.dropout(x) return x Residual Connection The concept of residual connections was first introduced in 2015 by Kaiming He et al. in their paper Deep Residual Learning for Image Recognition. It allows the network to bypass one or more layers, which helps alleviate the vanishing gradient problem that could occur in very deep neural networks.\nsource\nTo implement residual connections and a projection layer in our multi-head attention module, we modify the MultiHeadAttention class as follows.\nclass MultiHeadAttention(nn.Module): def __init__(self, h, d_k): super().__init__() self.heads = nn.ModuleList([Head(d_k) for _ in range(h)]) self.proj = nn.Linear(d_model, d_model) self.dropout = nn.Dropout(dropout) def forward(self, x): x = torch.cat([head(x) for head in self.heads], dim=-1) x = self.proj(x) x = self.dropout(x) return x Feed-Forward As stated in the paper:\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.\nThis means that instead of processing the entire sequence of embeddings as a single vector, the feed-forward network applies the same linear transformations to each embedding individually.\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $d_{model} = 512$, and the inner-layer has dimensionality $d_{ff} = 2048$.\nThis implies that our first linear layer in the feed-forward layer has an output dimension of d_model * 4, which serves as the input dimension of the second linear layer. We also apply a dropout layer to the feed-forward layer to avoid overfitting.\nclass FeedForward(nn.Module): def __init__(self): super().__init__() self.net = nn.Sequential( nn.Linear(d_model, d_model * 4), nn.ReLU(), nn.Linear(d_model * 4, d_model), nn.Dropout(dropout) ) def forward(self, x): x = self.net(x) return x Layer Normalization The concept of layer normalization was introduced by Jimmy Lei Ba et al. in their paper Layer Normalization published in 2016. Unlike batch normalization, which normalizes the inputs to a batch of data, layer normalization normalizes the inputs to a single layer of the network. In our implementation, we apply layer normalization before self-attention and feed-forward layers.\nsource\nRefactoring Let\u0026rsquo;s refactor the code to put multi-head attention and feed-forward layers to a single Block class. Moreover, the head size would be automatically set to d_model/h.\nclass Block(nn.Module): def __init__(self, h): super().__init__() d_k = d_model // h self.attn = MultiHeadAttention(h, d_k) self.ff = FeedForward() self.ln1 = nn.LayerNorm(d_model) self.ln2 = nn.LayerNorm(d_model) def forward(self, x): # attention + residual connection x = x + self.attn(x) # layer normalization x = self.ln1(x) # feed forward x = x + self.ff(x) # layer normalization x = self.ln2(x) return x Put Everything Together Here are the steps to build a GPT with transformer architecture:\nInitialize the token embedding table with the vocabulary size and embedding dimension (vocab_size, d_model). Initialize the positional embedding table with the maximum sequence length and embedding dimension (block_size, d_model). Create N identical decoder layers using the Block class with multi-head attention, feed-forward, and layer normalization layers. The head_size parameter will be automatically set to d_model/h. Add a linear output layer with the output dimension equal to the vocab_size. batch_size = 16 block_size = 32 eval_interval = 1000 eval_iters = 200 learning_rate = 1e-3 epochs = 10000 d_model = 64 # dimension of embedding h = 8 # number of heads N = 6 # number of identical layers dropout = 0.1 # dropout percentage device = \u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39; @torch.no_grad() def estimate_loss(): out = {} model.eval() for split in [\u0026#39;train\u0026#39;, \u0026#39;val\u0026#39;]: losses = torch.zeros(eval_iters) for k in range(eval_iters): X, Y = get_batch(split) logits, loss = model(X, Y) losses[k] = loss.item() out[split] = losses.mean() model.train() return out class BigramLanguageModel(nn.Module): def __init__(self): super().__init__() self.token_embedding_table = nn.Embedding(vocab_size, d_model) self.position_embedding_table = nn.Embedding(block_size, d_model) self.blocks = nn.Sequential(*[Block(h) for _ in range(N)]) self.output_linear = nn.Linear(d_model, vocab_size) def forward(self, idx, targets=None): B, T = idx.shape token_embed = self.token_embedding_table(idx) posit_embed = self.position_embedding_table(torch.arange(T, device=device)) x = token_embed + posit_embed x = self.blocks(x) logits = self.output_linear(x) if targets is None: loss = None else: B, T, C = logits.shape logits = logits.view(B*T, C) targets = targets.view(B*T) loss = F.cross_entropy(logits, targets) return logits, loss def generate(self, idx, max_length): for _ in range(max_length): logits, _ = self(idx[:, -block_size:]) logits = logits[:, -1, :] probs = F.softmax(logits, dim=-1) idx_next = torch.multinomial(probs, num_samples=1) idx = torch.cat((idx, idx_next), dim=1) return idx model = BigramLanguageModel().to(device) print(sum(p.numel() for p in model.parameters())/1e6, \u0026#39;M parameters\u0026#39;) 0.309185 M parameters Retraining optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate) for i in range(epochs): if i % eval_interval == 0 or i == epochs - 1: losses = estimate_loss() print(f\u0026#34;step {i:\u0026gt;6}: train loss {losses[\u0026#39;train\u0026#39;]:.4f}, val loss {losses[\u0026#39;val\u0026#39;]:.4f}\u0026#34;) x_batch, y_batch = get_batch(\u0026#39;train\u0026#39;) logits, loss = model(x_batch, y_batch) optimizer.zero_grad(set_to_none=True) loss.backward() optimizer.step() context = torch.zeros((1, 1), dtype=torch.long, device=device) print(decode(model.generate(context, max_length=2000)[0].tolist())) step 0: train loss 4.4133, val loss 4.4188 step 1000: train loss 2.1523, val loss 2.1733 step 2000: train loss 1.9162, val loss 1.9929 step 3000: train loss 1.8095, val loss 1.9325 step 4000: train loss 1.7424, val loss 1.8743 step 5000: train loss 1.7031, val loss 1.8359 step 6000: train loss 1.6730, val loss 1.8091 step 7000: train loss 1.6381, val loss 1.8015 step 8000: train loss 1.6231, val loss 1.7956 step 9000: train loss 1.6010, val loss 1.7734 step 9999: train loss 1.5991, val loss 1.7507 POLINCE: O-momen, marran, stack the blow. VALUMNIA: TRong it is 'twill o despreng. MARCIUS: She are. COMSARIO: Thraby, the tongue, And lefe to he she this highnoural, Have any but ut your to spuake it. LEONTES: Goot saled shur he wrett. SICIDIUS: Be she done! te. First KINquel: Thy had diul as recat my deasury? Faulter'er mean, on altal, If none banch with to times? York, Vom have yzage; this hight think noble of eye bewill fre, In gring might to jue of knot it the clunter, Were henrey quoring to jurition tone stime to known? Pryity and bear. KING EL.TAh, is leaven. For I would in Ancompers, for comen telms things: I worn apene so Herdow procked love; dime so worder. LORIS: It is here bear of go him. ROMEY: How I Leffater the death? And mearrinad king cans no myselfy that bartt, If you I decdom to be in tellothen, Low ke'es hath s duck, and within kindes, that found als In he house his Of the spine confeive inther his dear to gater: And go agonst Marcito, I'll wid my countery, I way, lientifirn tenving rulby us my follow honour yield stent poon, Jufe the be dared on the kial je: The day my Lounges, be agains in have once as to plating exvage of his tonake That themn were by to the hance, The sold long, po somebn o'er becelds Is this ofseding on this soak? alrick. UMISmed! HO, answer Off Humbledy, that's will forted yial with pring's lord. Forth, Jolsn'd ladib tod But thy shorly be this mine stons. Good you withnlieds think, this mance and thingn blunge his of be be reep steep your intent for thou way, the nober, and visy From the pot of lord? Mast all to be endought: what my loness, Tis is monius and from out of Sunscoa may, And not my the see to all, everstrer. KING RICHARD III: My hidoner, and strangems, honours, Before requick? ELIFFOLLY: O, ce, but and 't: her I near afta humbhal gittled here, O tAlker of off it dispuised here the heam we froens, Wasce, not he rese that dear'd, to, And in stay be I have will am gove his derefy: lade them brooks it in The newly generated text contains more word-like characters and resembles the style of Shakespeare, with a more significant proportion of correctly spelled words.\nRevisiting Attention sequence = \u0026#34;\u0026#34;\u0026#34;MENENIUS:\\nWhat is gra\u0026#34;\u0026#34;\u0026#34; token_embeddings = model.token_embedding_table.weight position_embeddings = model.position_embedding_table.weight tokens = torch.tensor([stoi [c] for c in sequence]) positions = torch.tensor([i for i in range(len(sequence))]) embed = token_embeddings[tokens] + position_embeddings[positions] # query and vector weights for last head of the last block q = model.blocks[5].attn.heads[7].query k = model.blocks[5].attn.heads[7].key v = model.blocks[5].attn.heads[7].value # query and key space with torch.no_grad(): Q = q(embed) K = k(embed) score = Q @ K.T score /= math.sqrt(d_model // h) mask = torch.tril(torch.ones(embed.shape[0], embed.shape[0])).to(device) score = score.masked_fill(mask == 0, float(\u0026#34;-inf\u0026#34;)) score = F.softmax(score, dim=-1) V = v(embed) new_embed = score @ V print(f\u0026#34;Attention scores for the sequence:\\n {score[-1, :]}\u0026#34;) print(f\u0026#34;Adjusted and compressed embeddings for the sequence:\\n {new_embed}\u0026#34;) Attention scores for the sequence: tensor([1.0275e-01, 6.3248e-03, 1.2576e-02, 7.7688e-04, 1.2232e-03, 1.0114e-01, 5.6094e-03, 1.2616e-01, 1.0319e-01, 1.5049e-01, 4.3153e-02, 4.5383e-03, 9.5087e-03, 4.0352e-03, 1.8735e-01, 1.6233e-03, 1.2997e-01, 5.6082e-03, 2.0060e-05, 3.1588e-04, 3.6474e-03], device='cuda:0') Adjusted and compressed embeddings for the sequence: tensor([[ 3.8285e-01, -5.6125e-01, -1.2138e+00, -5.2913e-01, 9.2973e-01, -4.2545e-01, -2.4848e+00, 6.8524e-03], [ 3.6920e-01, -5.4390e-01, -9.2868e-01, -4.8776e-01, 8.3288e-01, -3.3776e-01, -2.2440e+00, 1.1147e-01], [ 4.0217e-01, -4.6048e-01, 8.1029e-01, -1.8336e-01, 2.0483e-01, 2.0015e-01, -5.8833e-01, 7.3810e-01], [ 1.4945e+00, -6.4184e-01, -2.4202e-01, 1.0156e-01, 2.0985e-01, -1.5870e-01, 5.5549e-02, 3.1318e-01], [ 3.5607e-01, -5.5418e-02, 1.5003e+00, -2.7288e-01, -8.2235e-02, 1.1763e-01, -7.0800e-01, 1.2626e+00], [-6.7275e-02, -1.2190e+00, -1.7885e-01, 2.6792e-01, -2.2870e-01, -8.5028e-01, 3.4890e-01, 1.3680e-01], [ 1.7077e+00, -8.5935e-01, -6.5319e-01, 1.4917e-01, 2.7577e-01, -2.7634e-01, 3.0645e-01, 3.7927e-02], [-5.1688e-02, -8.2619e-01, 8.0506e-02, 2.1702e-01, -1.6939e-02, -6.4278e-01, 1.9751e-01, 8.1660e-02], [-7.1723e-02, -5.1926e-01, -2.9651e-01, -5.3577e-02, 1.8432e-01, -4.6867e-01, -7.6557e-01, -1.7440e-01], [ 8.1420e-01, -4.1661e-01, 1.0995e+00, -3.2608e-01, 2.8869e-02, 2.2275e-02, -5.5174e-02, 8.0169e-01], [ 2.9553e-01, -5.1129e-01, 2.6954e-01, -1.0131e-01, 1.6535e-03, -2.3739e-01, 2.4023e-01, 2.7450e-03], [ 9.2807e-01, -5.3834e-01, -4.8175e-01, -1.7232e-02, 1.6207e-01, -1.7096e-01, 3.0736e-01, -9.1554e-02], [ 4.2730e-01, 6.4469e-01, 8.8334e-01, 4.4953e-01, -3.0363e-01, 1.3055e-01, 1.1382e+00, -6.6804e-01], [ 8.4047e-01, -4.7317e-01, -6.5326e-02, -5.7882e-02, 1.3698e-01, -1.0259e-01, 2.5059e-01, 6.1572e-02], [ 2.9731e-01, -8.2256e-01, -2.8259e-02, 3.3942e-01, -2.8240e-01, 1.9379e-01, -9.6743e-02, 2.3589e-01], [ 6.0484e-01, -1.0521e-01, 2.7202e-01, 2.2309e-01, -6.7768e-01, 2.5342e-01, -4.1722e-01, 8.2589e-02], [ 4.1097e-01, 6.0131e-01, 8.3584e-01, 4.4749e-01, -2.8864e-01, 1.3370e-01, 1.0975e+00, -6.3150e-01], [-5.1310e-01, -3.5065e-01, -1.4606e-01, 4.4343e-01, 2.1451e-01, 7.1118e-02, -1.8510e-02, 6.4416e-01], [ 1.3922e-01, -5.7186e-02, -2.0533e-01, -2.0123e-01, -2.3971e-01, 2.8392e-01, -2.8814e-01, 3.0751e-01], [ 3.3605e-01, 5.6808e-01, 8.5728e-01, 3.2310e-01, -3.3082e-01, 1.1003e-01, 1.1402e+00, -6.2344e-01], [ 1.1078e-01, -2.0579e-02, -1.6989e-01, -8.3665e-02, -1.2148e-02, 5.8077e-02, -3.4206e-01, 3.3760e-01]], device='cuda:0') Notes Here are some tiny differences between my code and the code in the video.\nI applied layer normalization after the self-attention layer, while he applied immediately on x before x entered the self-attention and feed-forward layers. class Block(nn.Module): \u0026#34;\u0026#34;\u0026#34; Transformer block: communication followed by computation \u0026#34;\u0026#34;\u0026#34; def __init__(self, n_embd, n_head): # n_embd: embedding dimension, n_head: the number of heads we\u0026#39;d like super().__init__() head_size = n_embd // n_head self.sa = MultiHeadAttention(n_head, head_size) self.ffwd = FeedFoward(n_embd) self.ln1 = nn.LayerNorm(n_embd) self.ln2 = nn.LayerNorm(n_embd) def forward(self, x): x = x + self.sa(self.ln1(x)) x = x + self.ffwd(self.ln2(x)) return x The scaling factor I used was $d_k$ instead of $d_model$ (maybe it\u0026rsquo;s a typo in his code?). class Head(nn.Module): \u0026#34;\u0026#34;\u0026#34; one head of self-attention \u0026#34;\u0026#34;\u0026#34; def __init__(self, d_k): super().__init__() self.key = nn.Linear(n_embd, head_size, bias=False) self.query = nn.Linear(n_embd, head_size, bias=False) self.value = nn.Linear(n_embd, head_size, bias=False) self.register_buffer(\u0026#39;tril\u0026#39;, torch.tril(torch.ones(block_size, block_size))) self.dropout = nn.Dropout(dropout) def forward(self, x): B, T, C = x.shape # batch_size, block_size, n_embd k = self.key(x) # (B,T,C) q = self.query(x) # (B,T,C) # compute attention scores (\u0026#34;affinities\u0026#34;) wei = q @ k.transpose(-2,-1) * C **-0.5 # (B, T, C) @ (B, C, T) -\u0026gt; (B, T, T) wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\u0026#39;-inf\u0026#39;)) # (B, T, T) wei = F.softmax(wei, dim=-1) # (B, T, T) wei = self.dropout(wei) # perform the weighted aggregation of the values v = self.value(x) # (B,T,C) out = wei @ v # (B, T, T) @ (B, T, C) -\u0026gt; (B, T, C) return out Other Resources https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html https://jalammar.github.io/illustrated-transformer/ https://www.youtube.com/watch?v=ptuGllU5SQQ\u0026amp;list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ\u0026amp;index=9 https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf https://learning.oreilly.com/library/view/natural-language-processing/9781098136789/ https://nlp.seas.harvard.edu/annotated-transformer/ ","permalink":"https://gejun.name/natural-language-processing/understanding-attention-nlp/","summary":"In Part2, we constructed a straightforward MLP model to generate characters based on 32k popular names. In this lecture, Andrej guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model. We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.\nData Preparation Let\u0026rsquo;s first import the necessary libraries and get the data ready.","title":"Understanding Transformer Architecture by Building GPT"},{"content":"In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters. This lack of context can lead to poor performance of bigram models. In this lecture, Andrej shows us how to build a multilayer neural network to improve the model performance.\nUnlike the bigram model we built in the last lecture, our new mode is a multilayer perceptron (MLP) that takes the previous 2 characters to predict the probabilities of the next character. This MLP language model was proposed in the paper A Neural Probabilistic Language Model by Bengio et al. in 2003. As always, the official Jupyter Notebook for this part is here.\nData Preparation First, we build our vocabulary as we did before.\nfrom collections import Counter import torch import string import matplotlib.pyplot as plt import torch.nn.functional as F words = open(\u0026#34;names.txt\u0026#34;, \u0026#34;r\u0026#34;).read().splitlines() chars = string.ascii_lowercase stoi = {s: i+1 for i, s in enumerate(chars)} stoi[\u0026#34;.\u0026#34;] = 0 itos = {i: s for s, i in stoi.items()} print(stoi, itos) {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'} Next, we create the training data. This time, we use the last 2 characters, instead of 1, to predict the next character, which is a 3-gram or trigram model.\nblock_size = 3 X, y = [], [] for word in words: # initialize context context = [0] * block_size for char in word + \u0026#34;.\u0026#34;: idx = stoi[char] X.append(context) y.append(idx) # truncate the first char and add the new char context = context[1:] + [idx] X = torch.tensor(X) y = torch.tensor(y) print(X.shape, y.shape) torch.Size([228146, 3]) torch.Size([228146]) Multilayer Perceptron (MLP) As stated in the name, our neural network model will have multiple hidden layers. Besides this, we will also learn a new way to represent characters.\nFeature Vector The paper proposed that each word would be associated with a feature vector which can be learned as training progresses. In other words, we use feature vectors to represent words in a language model. The number of features or the length of the vector is much smaller than the size of the vocabulary. Since the size of our vocabulary is 27, we will use a vector of length of 2 for now. This feature vector can be considered as word embedding nowadays.\ng = torch.Generator().manual_seed(42) # initialize lookup table C = torch.randn((27, 2), generator=g) print(f\u0026#34;Vector representation for character a is: {C[stoi[\u0026#39;a\u0026#39;]]}\u0026#34;) Vector representation for character a is: tensor([ 0.9007, -2.1055]) The code above initializes our lookup table with $27\\times 2$ random numbers using torch.randn function. As we can see that the vector representation for the character a is [ 0.9007, -2.1055].\nNext, we are going to replace the indices in matrix X with vector representations. Since multiplying a one-hot encoding vector having a 1 at index i with the weight matrix W is the same as getting the ith row of W, we will extract the ith row from the embedding matrix directly instead of multiplying one-hot encoding with it.\nAccording to the tensor indexing documentation of PyTorch, we can extract corresponding feature vectors by treating X as an indexing matrix.\nembed = C[X] print(f\u0026#34;First row of X: {X[0, :]}\u0026#34;) print(f\u0026#34;First row of embedding: {embed[0,:]}\u0026#34;) print(embed.shape) First row of X: tensor([0, 0, 0]) First row of embedding: tensor([[1.9269, 1.4873], [1.9269, 1.4873], [1.9269, 1.4873]]) torch.Size([228146, 3, 2]) To put it in another way, we transform the matrix X of $228146\\times 3$ to the embedding matrix embed of $228146\\times 3 \\times 2$ because all the indices have been replaced with a vector of $1\\times 2$.\nModel Architecture As highlighted in the picture above, we should have a vector for each trigram after extracting its feature from the lookup table. This way, we can do matrix multiplication like before. However, we have a $3\\times 2$ matrix for each trigram instead. So we need to concatenate all the rows of the matrix into one vector. We can use torch.cat to concatenate the second dimension together, but PyTorch has a more efficient way, the view function(doc), to do so. See this blog post for more details about tensor and PyTorch internals.\nprint(embed[0, :]) print(embed.view(-1, 6)[0, :]) tensor([[1.9269, 1.4873], [1.9269, 1.4873], [1.9269, 1.4873]]) tensor([1.9269, 1.4873, 1.9269, 1.4873, 1.9269, 1.4873]) Building Model Next, we are going to initialize the weights and biases of our first and second hidden layers. Since the input dimension of our first layer is 6 and the number of neurons is 100, we initialize the weight matrix of shape $6\\times 100$ and the bias vector of length 100. The same rule applies to the second layer. The second layer\u0026rsquo;s input dimension is the first layer\u0026rsquo;s output dimension, 100. Because the output of the second layer is the probability of all 27 characters, we initialize the weight matrix of shape $100\\times 27$ with the bias vector of length 27.\n# 1st hidden layer W1 = torch.randn((6, 100)) b1 = torch.randn(100) # output for 1st layer h = embed.view(-1, 6) @ W1 + b1 # 2nd hidden layer W2 = torch.randn((100, 27)) b2 = torch.randn(27) # output for 2nd layer logits = h @ W2 + b2 Making Predictions The next step is our first forward pass to obtain the probabilities of the next characters.\n# softmax counts = logits.exp() probs = counts / counts.sum(1, keepdims=True) loss = -probs[torch.arange(X.shape[0]), y].log().mean() print(f\u0026#34;Overall loss: {loss:.6f}\u0026#34;) Overall loss: nan However, as Andrej mentioned in the video, there is a potential issue with calculating the softmax function traditionally, as we saw above. If the output logits contain a large value, such as 100, applying the exponential function can result in nan values. Therefore, a better way to calculate the loss is to use the built-in cross_entropy function instead.\nloss = F.cross_entropy(logits, y) print(f\u0026#34;Overall loss: {loss:.6f}\u0026#34;) Overall loss: 78.392731 Put Everything Together Here is the code after we put everything together and enabled backward pass.\ng = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True print(f\u0026#34;Total parameters: {sum(p.nelement() for p in parameters)}\u0026#34;) Total parameters: 3481 The total number of learnable parameters of our model is 3482. Next, we are going to run the model for 10 epochs and see how loss changes. Notice that we apply the activation function tanh as described in the paper in the code below.\nfor _ in range(10): # forward pass embed = C[X] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y) print(f\u0026#34;Loss: {loss.item()}\u0026#34;) # backward pass for p in parameters: p.grad = None loss.backward() # update weights lr = 0.1 for p in parameters: p.data += -lr * p.grad Loss: 16.72646713256836 Loss: 14.942943572998047 Loss: 13.863017082214355 Loss: 13.003837585449219 Loss: 12.292213439941406 Loss: 11.732643127441406 Loss: 11.270574569702148 Loss: 10.859720230102539 Loss: 10.479723930358887 Loss: 10.136445045471191 The model\u0026rsquo;s loss decreases as expected. However, you will notice that the loss comes out slower if you have a larger model with much more parameters. Why? Because we are using the whole dataset as a batch to calculate the loss and update the weights accordingly. In Stochastic Gradient Descent (SGD), the model parameters are updated based on the gradient of the loss function with respect to a randomly selected subset of the training data. Moreover, we can apply this idea to accelerate the training process.\nApplying Mini-batch We pick 32 as the mini-batch size, and the model runs very fast for 1000 epochs.\nfor i in range(1000): # batch_size = 32 idx = torch.randint(0, X.shape[0], (32, )) # forward pass embed = C[X[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 # using the whole dataset as a batch loss = F.cross_entropy(logits, y[idx]) if i % 50 == 0: print(f\u0026#34;Loss: {loss.item()}\u0026#34;) # backward pass for p in parameters: p.grad = None loss.backward() # update weights lr = 0.1 for p in parameters: p.data += -lr * p.grad embed = C[X] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y) print(f\u0026#34;Overall loss: {loss.item()}\u0026#34;) Loss: 10.522725105285645 Loss: 4.547809600830078 Loss: 3.9053943157196045 Loss: 3.5418882369995117 Loss: 3.312927722930908 Loss: 3.10072660446167 Loss: 3.188538074493408 Loss: 2.6955881118774414 Loss: 2.9730937480926514 Loss: 2.5453033447265625 Loss: 3.034700870513916 Loss: 2.2029476165771484 Loss: 2.5462143421173096 Loss: 2.6591145992279053 Loss: 2.9640085697174072 Loss: 3.142090082168579 Loss: 2.5031352043151855 Loss: 2.721736431121826 Loss: 2.7801644802093506 Loss: 2.32700252532959 Overall loss: 2.6130335330963135 Learning Rate Selection So how can we determine a suitable learning rate? In our previous training processes, we used a fixed learning rate of 0.1, but how can we know that 0.1 is optimal? Next, we are going to do some experiments to explore how to choose a good learning rate.\ng = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True # logarithm learning rate, base 10 lre = torch.linspace(-3, 0, 1000) # learning rates lrs = 10 ** lre losses = [] for i in range(1000): idx = torch.randint(0, X.shape[0], (32, )) embed = C[X[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y[idx]) if i % 50 == 0: print(f\u0026#34;Loss: {loss.item()}\u0026#34;) for p in parameters: p.grad = None loss.backward() lr = lrs[i] for p in parameters: p.data += -lr * p.grad losses.append(loss.item()) plt.plot(lre, losses) Loss: 17.930665969848633 Loss: 15.90300464630127 Loss: 14.608807563781738 Loss: 11.146048545837402 Loss: 14.368053436279297 Loss: 10.241884231567383 Loss: 10.7547607421875 Loss: 9.06742000579834 Loss: 6.721671104431152 Loss: 4.959266185760498 Loss: 7.631305694580078 Loss: 6.03385591506958 Loss: 3.6078100204467773 Loss: 3.7624008655548096 Loss: 2.994145393371582 Loss: 2.6852164268493652 Loss: 3.392582893371582 Loss: 3.5405192375183105 Loss: 4.318221569061279 Loss: 5.7273149490356445 Figure 1: A plot for loss on different logarithm of learing rates According to the plot in Figure 1, the optimal logarithmic learning rate is around -1.0, which makes the learning rate 0.1.\nLearning Rate Decay As the training progresses, the loss could encounter a plateau, meaning that it stops decreasing even though the training process is still ongoing. To overcome this, learning rate decay can be applied, which decreases the learning rate over time as the training progresses. The model can escape from plateaus and continue improving its performance.\ng = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True losses = [] epochs = 20000 for i in range(epochs): idx = torch.randint(0, X.shape[0], (32, )) embed = C[X[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y[idx]) for p in parameters: p.grad = None loss.backward() # learning rate decay lr = 0.1 if i \u0026lt; epochs // 2 else 0.001 for p in parameters: p.data += -lr * p.grad losses.append(loss.item()) plt.plot(range(epochs), losses) Train, Validation, and Test Evaluating the model performance on unseen data is important to make sure it generalizes well. It is common practice to split the training data into three parts: 80% for training, 10% for validation, and 10% for testing. The validation set could also be used for early stopping, which means stopping the training process when the performance on the validation set starts to degrade, preventing the model from overfitting to the training set.\ndef build_dataset(words, block_size=3): X, Y = [], [] for w in words: context = [0] * block_size for char in w + \u0026#34;.\u0026#34;: idx = stoi[char] X.append(context) Y.append(idx) context = context[1:] + [idx] X = torch.tensor(X) Y = torch.tensor(Y) print(X.shape, Y.shape) return X, Y import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words)) X_tr, y_tr = build_dataset(words[:n1]) X_va, y_va = build_dataset(words[n1:n2]) X_te, y_te = build_dataset(words[n2:]) g = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True tr_losses = [] va_losses = [] epochs = 20000 for i in range(epochs): idx = torch.randint(0, X_tr.shape[0], (32, )) embed = C[X_tr[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y_tr[idx]) for p in parameters: p.grad = None loss.backward() # learning rate decay lr = 0.1 if i \u0026lt; epochs // 2 else 0.01 for p in parameters: p.data += -lr * p.grad tr_losses.append(loss.item()) val_embed = C[X_va] val_h = torch.tanh(val_embed.view(-1, 6) @ W1 + b1) val_logits = val_h @ W2 + b2 val_loss = F.cross_entropy(val_logits, y_va) va_losses.append(val_loss.item()) plt.plot(range(epochs), tr_losses, label=\u0026#39;Training Loss\u0026#39;) plt.plot(range(epochs), va_losses, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training and Validation Loss\u0026#39;) plt.xlabel(\u0026#39;Epochs\u0026#39;) plt.ylabel(\u0026#39;Loss\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.show() torch.Size([182625, 3]) torch.Size([182625]) torch.Size([22655, 3]) torch.Size([22655]) torch.Size([22866, 3]) torch.Size([22866]) Figure 2: Plot for training and validation loss As shown in Figure 2, there is a tiny drop in the validation loss at 10000 epochs, which indicates that our training did encounter a plateau and learning rate decay works very well. Let\u0026rsquo;s check the loss of the testing data.\ntest_embed = C[X_te] test_h = torch.tanh(test_embed.view(-1, 6) @ W1 + b1) test_logits = test_h @ W2 + b2 test_loss = F.cross_entropy(test_logits, y_te) print(f\u0026#34;Loss on validation data: {val_loss:.6f}\u0026#34;) print(f\u0026#34;Loss on testing data: {test_loss:.6f}\u0026#34;) Loss on validation data: 2.375811 Loss on testing data: 2.374066 The losses on validation and testing data are close, indicating we are not overfitting.\nVisualization of Embedding Let\u0026rsquo;s visualize our embedding matrix.\nplt.figure(figsize=(8,8)) plt.scatter(C[:, 0].data, C[:, 1].data, s = 200) for i in range(C.shape[0]): plt.text(C[i, 0].item(), C[i, 1].item(), itos[i], ha=\u0026#34;center\u0026#34;, va=\u0026#34;center\u0026#34;, color=\u0026#34;white\u0026#34;) plt.grid(\u0026#39;minor\u0026#39;) plt.show() Figure 3: Visualization of 2D embedding matrix As depicted in Figure 3, the vowels are closely grouped in the left bottom corner of the plot, while the . is situated far away in the top right corner.\nWord Generation The last thing we want to do is word generation.\ng = torch.Generator().manual_seed(420) for _ in range(20): out = [] context = [0] * block_size while True: embed = C[torch.tensor([context])] h = torch.tanh(embed.view(1, -1) @ W1 + b1) logits = h @ W2 + b2 probs = F.softmax(logits, dim=1) idx = torch.multinomial(probs, num_samples=1, generator=g).item() context = context[1:] + [idx] if idx == 0: break out.append(idx) print(\u0026#39;\u0026#39;.join(itos[i] for i in out)) rai mal lemistani iua kacyt tan zatlixahnen rarbi zethanli blie mozien nar ameson xaxun koma aedh sarixstah elin dyannili saom The words generated by the multilayer perceptron model make more sense than those from our last model. Still, there are many other ways to improve model performance. For example, train more epochs with learning rate decay, increase the batch size to make the training more stable, and add more data.\n","permalink":"https://gejun.name/natural-language-processing/building-makemore-mlp/","summary":"In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters.","title":"Multilayer Perceptron (MLP)"},{"content":"This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.\nIn this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let\u0026rsquo;s prepare the data first.\nData Preparation Load data We are using the most common 32k names of 2018 from ssa.gov website as our data source. First, we apply the code below to obtain each bigram\u0026rsquo;s frequency. If you don\u0026rsquo;t know what a bigram is, a bigram is a sequence of two adjacent words or characters in a text. We also add a special character, \u0026ldquo;.\u0026rdquo;, to the name\u0026rsquo;s beginning and end to indicate its start and end, respectively. As can be seen that the top 5 common bigrams in the data are n., a., an, .a, and e..\nfrom collections import Counter words = open(\u0026#34;names.txt\u0026#34;, \u0026#34;r\u0026#34;).read().splitlines() counter = Counter() for word in words: chs = list(\u0026#34;.\u0026#34; + word + \u0026#34;.\u0026#34;) for c1, c2 in zip(chs, chs[1:]): bigram = (c1, c2) counter[bigram] += 1 for bigram, frequency in counter.most_common(5): print(f\u0026#34;Frequency of {\u0026#39;\u0026#39;.join(bigram)}: {frequency}\u0026#34;) Frequency of n.: 6763 Frequency of a.: 6640 Frequency of an: 5438 Frequency of .a: 4410 Frequency of e.: 3983 Numericallization As is known that computers are good at processing numerical data; however, they may not be efficient in dealing with text. So our second step is to create two mappings: string to index and index to string. These mappings are used to represent words or characters numerically. This process is sometimes called numericalization.\nimport torch import string import matplotlib.pyplot as plt chars = string.ascii_lowercase stoi = {s: i+1 for i, s in enumerate(chars)} stoi[\u0026#34;.\u0026#34;] = 0 itos = {i: s for s, i in stoi.items()} print(stoi, itos) {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'} Counting Approach Frequency Our first step is to obtain the frequencies of the bigrams. Since we have a vocabulary of 27 characters-26 letters in lowercase plus 1 special character, we need a $27\\times 27$ matrix to store the frequencies of all possible bigrams. Figure 1 is a heatmap of the calculated frequencies. The darker the color, the higher the frequency of the bigram.\nN = torch.zeros((27, 27), dtype=torch.int32) for (c1, c2), freq in counter.items(): idx1 = stoi[c1] idx2 = stoi[c2] N[idx1, idx2] = freq plt.figure(figsize=(16, 16)) plt.imshow(N, cmap=\u0026#34;Blues\u0026#34;) for i in range(27): for j in range(27): chstr = itos[i] + itos[j] plt.text(j, i, chstr, ha=\u0026#34;center\u0026#34;, va=\u0026#34;bottom\u0026#34;, color=\u0026#34;gray\u0026#34;) plt.text(j, i, N[i, j].item(), ha=\u0026#34;center\u0026#34;, va=\u0026#34;top\u0026#34;, color=\u0026#34;gray\u0026#34;) plt.axis(\u0026#34;off\u0026#34;) plt.show() Figure 1: A heatmap plot for frequencies of bigrams Probability To get the probability of each bigram, we want to normalize the matrix N by row. Why? Because we want to know the probability of the character given the current character we have in the process of character generation, i.e., $P(next\\ char | current\\ char)$. To avoid calculating $log0$ later on, we add 1 to the frequency of each bigram.\nP = (N + 1).float() P /= P.sum(1, keepdims=True) Maximum Likelihood Maximum likelihood is a statistical method to estimate the parameters of a probability distribution based on observed data. The goal of maximum likelihood is to find the values of the distribution\u0026rsquo;s parameters that make the observed data most likely to have been generated by that distribution. In our case, we want the next generated character comes from the probability distribution as much as possible. How do we calculate the likelihood? It is the product of the probability of each bigram in a word. $$ L(\\theta) = P(X_1=x_1, X_2=x_2, \u0026hellip;, X_n=x_n) = \\Pi_i^n P(X_i=x_i)$$ For example, the likelihood of the word good is calculated as $$Likelihood= P(\u0026quot;.g\u0026quot;) * P(\u0026ldquo;go\u0026rdquo;) * P(\u0026ldquo;oo\u0026rdquo;) * P(\u0026ldquo;od\u0026rdquo;) * P(\u0026ldquo;d.\u0026rdquo;) $$ $$ = 0.0209*0.0430*0.0146*0.0240*0.0936=2.9399e-8$$\ndef calc_likelihood(word, verbose=False): word = list(\u0026#34;.\u0026#34; + word + \u0026#34;.\u0026#34;) likelihood = 1.0 for c1, c2 in zip(word, word[1:]): idx1 = stoi[c1] idx2 = stoi[c2] prob = P[idx1, idx2] if verbose: print(f\u0026#34;probability for {\u0026#39;\u0026#39;.join((c1, c2))}: {prob:.4f}\u0026#34;) likelihood *= prob return likelihood prob = calc_likelihood(\u0026#34;good\u0026#34;, verbose=True) print(f\u0026#34;Likelihood for good is: {prob:.4e}\u0026#34;) probability for .g: 0.0209 probability for go: 0.0430 probability for oo: 0.0146 probability for od: 0.0240 probability for d.: 0.0936 Likelihood for good is: 2.9399e-08 Let\u0026rsquo;s generate some words by randomly picking the bigram according to its probability using torch.multinomial function and calculate their likelihoods.\ng = torch.Generator().manual_seed(420) generated_words = [] for i in range(5): out = [] ix = 0 while True: p = P[ix] ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() if ix == 0: break out.append(itos[ix]) generated_words.append((\u0026#34;\u0026#34;.join(out), calc_likelihood(\u0026#34;\u0026#34;.join(out)).item())) generated_words.sort(key=lambda x: -x[1]) for gw, lh in generated_words: print(f\u0026#34;Likelihood for {gw}: {lh}\u0026#34;) Likelihood for jen: 0.0005491252522915602 Likelihood for jor: 0.0001786774955689907 Likelihood for she: 0.00017446796118747443 Likelihood for tais: 3.90183367926511e-06 Likelihood for anuir: 2.335933579900029e-08 It turns out jen which has the maximum likelihood 0.000549 is the winner in these 5 randomly generated words. Remember that our goal is to maximize the likelihood of the word the model generates because the higher the likelihood, the better the model. However, notice that the likelihoods for the generated words are too small, so applying a log function to each probability would make it easier to work with. $$ logL(\\theta) = log\\Pi_i^n P(X_i=x_i)=\\Sigma_i^nlogP(X_i=x_i)$$\nAdditionally, maximizing the likelihood is the same as maximizing the log-likelihood because the logarithm function is a monotonic increasing function, which is the same as minimizing the negative log-likelihood. We prefer minimization to maximization in any optimization problem. Let\u0026rsquo;s calculate the average negative log-likelihood of our name dataset, which is 2.454679.\ndef calc_nll(word): word = list(\u0026#34;.\u0026#34; + word + \u0026#34;.\u0026#34;) log_likelihood = 0.0 for c1, c2 in zip(word, word[1:]): idx1 = stoi[c1] idx2 = stoi[c2] prob = P[idx1, idx2] log_prob = torch.log(prob) log_likelihood += log_prob return -log_likelihood nlls = [calc_nll(w) for w in words] ns = [len(w) + 1 for w in words] print(f\u0026#34;Average negative log-likelihood: {sum(nlls)/sum(ns):.6f}\u0026#34;) Average negative log-likelihood: 2.454579 Neural Network How does a neural network model fit in the character generation? Think of it in this way: given the last generated character, we want the model to output a probability distribution for the next character, in which we can find the most likely character to follow it. In other words, our task is to use the model to estimate the probability distribution based on the dataset rather than relying on counting the occurrences of each bigram. As always, let\u0026rsquo;s prepare the data in the first step.\nTraining Data Preparation The training data is created using bigrams, where the first character is the input feature, and the second character is used as the target of the model. Since feeding integers into a neural network and multiplying them with weights does not make sense, we need to transform them into a different format. The most common method is one-hot encoding, which transforms each integer into a vector with all 0s except for a 1 at the index corresponding to the integer. PyTorch provides a built-in torch.nn.functional.one_hot function for one-hot encoding.\nimport torch.nn.functional as F xs, ys = [], [] for word in words: chs = list(\u0026#34;.\u0026#34; + word + \u0026#34;.\u0026#34;) for c1, c2 in zip(chs, chs[1:]): idx1 = stoi[c1] idx2 = stoi[c2] xs.append(idx1) ys.append(idx2) # tensor function returns the same type as its original xs = torch.tensor(xs) ys = torch.tensor(ys) xenc = F.one_hot(xs, num_classes=27).float() print(xenc.shape) torch.Size([228146, 27]) After applying one-hot encoding, we have a tensor xenc of shape $228146\\times 27$.\nUnderstanding Weights The weight matrix of our model has the same shape as the matrix N above but is initialized with random values. PyTorch\u0026rsquo;s built-in function torch.randn gives us random numbers from a normal distribution with mean 0 and standard deviation 1, resulting in positive and negative values. After multiplying the one-hot encoding matrix with weights, we obtain the output of the first layer, which may contain negative values. However, we want the output to represent the probability of the next character, as we calculated above. To achieve this, we can treat the output as the logarithm of the frequencies and apply the exponential function to obtain the positive values, which can be interpreted as the frequencies of the bigrams starting with the input feature. Why? Because multiplying a one-hot encoding vector having a 1 at index i, with the weight matrix W is the same as getting the ith row of W. And we want this frequency matrix to be close to the matrix N as close as possible. If we further normalize the output over the rows, we can obtain the probability distribution of bigrams. In fact, the last two steps, applying exponential function and normalization, of calculation are known as the softmax function.\ng = torch.Generator().manual_seed(420) W = torch.randn((27, 27), generator=g, requires_grad=True) # log-counts logits = xenc @ W # (228146, 27) x (27, 27) # counts counts = logits.exp() # probability probs = counts / counts.sum(1, keepdim=True) print(probs.shape) print(sum(probs[1,:])) torch.Size([228146, 27]) tensor(1.0000, grad_fn=\u0026lt;AddBackward0\u0026gt;) Optimization Remember that our goal is to approach the actual probabilities from the training data using maximum likelihood estimation. As the training progresses, the model adjusts the weights in such a way that the predicted probabilities for the next character in a word are as close to the actual probabilities of the training data. By minimizing the negative log-likelihood, we effectively minimize the distance between predicted and actual probabilities. Let\u0026rsquo;s take the first word, emma, as an example and see how the neural network calculates its loss. This step is also called forward pass. The first bigram is .e with the input . (index 0) and actual label e (index 5). The one-hot encoding for . is [1, 0, ..., 0], and the output probability for e is 0.0246. Applying log and negation, we have the loss as 3.7050. The same calculation applies to em, mm, ma, and a.. Finally, we get the loss for emma is 3.6985.\nnlls = torch.zeros(5) for i in range(5): x = xs[i].item() y = ys[i].item() print(\u0026#39;-\u0026#39; * 50) print(f\u0026#39;bigram example {i+1}: {itos[x]} {itos[y]} (indexes {x}, {y})\u0026#39;) print(f\u0026#39;input to the neural network: {x}\u0026#39;) print(f\u0026#39;output probbabilities from the nn: {probs[i]}\u0026#39;) print(f\u0026#39;label (actual next character): {y}\u0026#39;) p = probs[i, y] print(f\u0026#39;probability assigned by the nn to the correct character: {p.item()}\u0026#39;) logp = torch.log(p) print(f\u0026#39;log-likelihood: {logp.item()}\u0026#39;) nll = -logp print(f\u0026#39;negative log likelihood: {nll}\u0026#39;) nlls[i] = nll print(f\u0026#34;Average negative log-likelihood, i.e., loss={nlls.mean().item()}\u0026#34;) -------------------------------------------------- bigram example 1: . e (indexes 0, 5) input to the neural network: 0 output probbabilities from the nn: tensor([0.0167, 0.0278, 0.0328, 0.0114, 0.0173, 0.0246, 0.0100, 0.0341, 0.1024, 0.0259, 0.2364, 0.0219, 0.0422, 0.0108, 0.1262, 0.0647, 0.0130, 0.0162, 0.0157, 0.0093, 0.0184, 0.0022, 0.0482, 0.0090, 0.0069, 0.0195, 0.0362], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 5 probability assigned by the nn to the correct character: 0.024598384276032448 log-likelihood: -3.7050745487213135 negative log likelihood: 3.7050745487213135 -------------------------------------------------- bigram example 2: e m (indexes 5, 13) input to the neural network: 5 output probbabilities from the nn: tensor([0.1219, 0.0087, 0.0157, 0.0546, 0.0067, 0.0149, 0.0185, 0.0338, 0.0110, 0.0030, 0.0060, 0.0697, 0.0211, 0.0579, 0.0061, 0.0043, 0.0746, 0.0416, 0.0264, 0.0611, 0.0823, 0.0124, 0.0179, 0.0129, 0.0374, 0.1633, 0.0162], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 13 probability assigned by the nn to the correct character: 0.057898372411727905 log-likelihood: -2.8490660190582275 negative log likelihood: 2.8490660190582275 -------------------------------------------------- bigram example 3: m m (indexes 13, 13) input to the neural network: 13 output probbabilities from the nn: tensor([0.3351, 0.0126, 0.0370, 0.0075, 0.0302, 0.0635, 0.0042, 0.0339, 0.0155, 0.0512, 0.0080, 0.0283, 0.0557, 0.0171, 0.0388, 0.0103, 0.0507, 0.0398, 0.0191, 0.0074, 0.0174, 0.0132, 0.0121, 0.0245, 0.0307, 0.0219, 0.0142], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 13 probability assigned by the nn to the correct character: 0.017136109992861748 log-likelihood: -4.066567420959473 negative log likelihood: 4.066567420959473 -------------------------------------------------- bigram example 4: m a (indexes 13, 1) input to the neural network: 13 output probbabilities from the nn: tensor([0.3351, 0.0126, 0.0370, 0.0075, 0.0302, 0.0635, 0.0042, 0.0339, 0.0155, 0.0512, 0.0080, 0.0283, 0.0557, 0.0171, 0.0388, 0.0103, 0.0507, 0.0398, 0.0191, 0.0074, 0.0174, 0.0132, 0.0121, 0.0245, 0.0307, 0.0219, 0.0142], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 1 probability assigned by the nn to the correct character: 0.012621787376701832 log-likelihood: -4.372330665588379 negative log likelihood: 4.372330665588379 -------------------------------------------------- bigram example 5: a . (indexes 1, 0) input to the neural network: 1 output probbabilities from the nn: tensor([0.0302, 0.0788, 0.0096, 0.0099, 0.0151, 0.1021, 0.0146, 0.0253, 0.0076, 0.0107, 0.0429, 0.0286, 0.0371, 0.0437, 0.0168, 0.0133, 0.0129, 0.0075, 0.0038, 0.0199, 0.0854, 0.0875, 0.1194, 0.1119, 0.0151, 0.0325, 0.0177], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 0 probability assigned by the nn to the correct character: 0.030220769345760345 log-likelihood: -3.4992258548736572 negative log likelihood: 3.4992258548736572 Average negative log-likelihood, i.e., loss=3.6984527111053467 So how do we calculate the loss efficiently? It turns out that we can pass all these row and column indices to the matrix, and then take log and mean afterwards. The loss for the forward pass is 3.6374.\nloss = -probs[torch.arange(len(xs)), ys].log().mean() print(f\u0026#34;Overall loss: {loss.item()}\u0026#34;) Overall loss: 3.637367010116577 After obtaining the average loss from the forward pass, we need a backward pass to update the weights. To do this, we need to make sure that the parameter requires_grad is set to True for the weight matrix W. Next, we zero out all gradients to avoid the accumulation of gradients across batches. We then call loss.backward() to compute the gradient of the oss with regard to each weight. The gradient of a weight indicates how much that increasing that weight will affect the loss. If it is positive, increasing the weight will increase the loss too. Conversely, increasing the weight will decrease the loss if the gradient is negative. For example, W.grad[0, 0]=0.002339 means that W[0, 0] has a positive effect on the loss.\n# set the gradient to zero W.grad = None # backward pass loss.backward() The next step is to update the weights and recalculate the averge loss.\nlr = 0.1 W.data += -lr * W.grad # forward pass logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) loss = -probs[torch.arange(len(xs)), ys].log().mean() print(f\u0026#34;Overall loss: {loss.item()}\u0026#34;) Overall loss: 3.6365137100219727 The overall loss is now 3.6365, which is slightly lower than before. We can keep doing this gradient descent step until the model performance is good enough. As we train more epochs, the overall loss is getting closer to the actual overall loss, which is 2.454579.\nlr = 50 for i in range(301): logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) loss = -probs[torch.arange(len(xs)), ys].log().mean() if i % 50 == 0: print(f\u0026#34;Epochs: {i}, loss: {loss.item()}\u0026#34;) W.grad = None loss.backward() W.data += -lr * W.grad Epochs: 0, loss: 3.6365137100219727 Epochs: 50, loss: 2.4955990314483643 Epochs: 100, loss: 2.4727954864501953 Epochs: 150, loss: 2.4657769203186035 Epochs: 200, loss: 2.4625258445739746 Epochs: 250, loss: 2.4606406688690186 Epochs: 300, loss: 2.459399700164795 Note that our current neural network only has one hidden layer. We can add more hidden layers to improve the model performance. Additionally, we can add a regularization item (e.g., the mean of the square of all weights) in the loss function to prevent overfitting.\nloss = -probs[torch.arange(len(xs)), ys].log().mean() + 0.01 * (W ** 2).mean() print(loss) tensor(2.4834, grad_fn=\u0026lt;AddBackward0\u0026gt;) In this case, the optimization has two components\u0026ndash;average negative log-likelihood and mean of the square of weights. The regularization item works like a force to squeeze the weights and make them close to zeros as much as possible. The last step is to sample characters from the neural network model.\ng = torch.Generator().manual_seed(420) nn_generated_words = [] for _ in range(5): out = [] idx = 0 while True: xenc = F.one_hot(torch.tensor([idx]), num_classes=27).float() logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item() if idx == 0: break out.append(itos[idx]) nn_generated_words.append((\u0026#34;\u0026#34;.join(out), calc_likelihood(\u0026#34;\u0026#34;.join(out)).item())) nn_generated_words.sort(key=lambda x: -x[1]) for gw, lh in nn_generated_words: print(f\u0026#34;Likelihood for {gw}: {lh}\u0026#34;) Likelihood for jen: 0.0005491252522915602 Likelihood for jor: 0.0001786774955689907 Likelihood for she: 0.00017446796118747443 Likelihood for tais: 3.90183367926511e-06 Likelihood for anuir: 2.335933579900029e-08 We are using the same seed for the generator. The words generated from the neural network are exactly the same as those generated from the probability table above, which is what we want to see.\n","permalink":"https://gejun.name/natural-language-processing/building-makemore/","summary":"This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.\nIn this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let\u0026rsquo;s prepare the data first.","title":"Bigram Character-level Language Model"}]