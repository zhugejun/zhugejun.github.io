[{"content":"In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters. This lack of context can lead to poor performance of bigram models. In this lecture, Andrej shows us how to build a multilayer neural network to improve the model performance.\nUnlike the bigram model we built in the last lecture, our new mode is a multilayer perceptron (MLP) that takes the previous 2 characters to predict the probabilities of the next character. This MLP language model was proposed in the paper A Neural Probabilistic Language Model by Bengio et al. in 2003. As always, the official Jupyter Notebook for this part is here.\nData Preparation First, we build our vocabulary as we did before.\nfrom collections import Counter import torch import string import matplotlib.pyplot as plt import torch.nn.functional as F words = open(\u0026#34;names.txt\u0026#34;, \u0026#34;r\u0026#34;).read().splitlines() chars = string.ascii_lowercase stoi = {s: i+1 for i, s in enumerate(chars)} stoi[\u0026#34;.\u0026#34;] = 0 itos = {i: s for s, i in stoi.items()} print(stoi, itos) {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'} Next, we create the training data. This time, we use the last 2 characters, instead of 1, to predict the next character, which is a 3-gram or trigram model.\nblock_size = 3 X, y = [], [] for word in words: # initialize context context = [0] * block_size for char in word + \u0026#34;.\u0026#34;: idx = stoi[char] X.append(context) y.append(idx) # truncate the first char and add the new char context = context[1:] + [idx] X = torch.tensor(X) y = torch.tensor(y) print(X.shape, y.shape) torch.Size([228146, 3]) torch.Size([228146]) Multilayer Perceptron (MLP) As stated in the name, our neural network model will have multiple hidden layers. Besides this, we will also learn a new way to represent characters.\nFeature Vector The paper proposed that each word would be associated with a feature vector which can be learned as training progresses. In other words, we use feature vectors to represent words in a language model. The number of features or the length of the vector is much smaller than the size of the vocabulary. Since the size of our vocabulary is 27, we will use a vector of length of 2 for now. This feature vector can be considered as word embedding nowadays.\ng = torch.Generator().manual_seed(42) # initialize lookup table C = torch.randn((27, 2), generator=g) print(f\u0026#34;Vector representation for character a is: {C[stoi[\u0026#39;a\u0026#39;]]}\u0026#34;) Vector representation for character a is: tensor([ 0.9007, -2.1055]) The code above initializes our lookup table with $27\\times 2$ random numbers using torch.randn function. As we can see that the vector representation for the character a is [ 0.9007, -2.1055].\nNext, we are going to replace the indices in matrix X with vector representations. Since multiplying a one-hot encoding vector having a 1 at index i with the weight matrix W is the same as getting the ith row of W, we will extract the ith row from the embedding matrix directly instead of multiplying one-hot encoding with it.\nAccording to the tensor indexing documentation of PyTorch, we can extract corresponding feature vectors by treating X as an indexing matrix.\nembed = C[X] print(f\u0026#34;First row of X: {X[0, :]}\u0026#34;) print(f\u0026#34;First row of embedding: {embed[0,:]}\u0026#34;) print(embed.shape) First row of X: tensor([0, 0, 0]) First row of embedding: tensor([[1.9269, 1.4873], [1.9269, 1.4873], [1.9269, 1.4873]]) torch.Size([228146, 3, 2]) To put it in another way, we transform the matrix X of $228146\\times 3$ to the embedding matrix embed of $228146\\times 3 \\times 2$ because all the indices have been replaced with a vector of $1\\times 2$.\nModel Architecture As highlighted in the picture above, we should have a vector for each trigram after extracting its feature from the lookup table. This way, we can do matrix multiplication like before. However, we have a $3\\times 2$ matrix for each trigram instead. So we need to concatenate all the rows of the matrix into one vector. We can use torch.cat to concatenate the second dimension together, but PyTorch has a more efficient way, the view function(doc), to do so. See this blog post for more details about tensor and PyTorch internals.\nprint(embed[0, :]) print(embed.view(-1, 6)[0, :]) tensor([[1.9269, 1.4873], [1.9269, 1.4873], [1.9269, 1.4873]]) tensor([1.9269, 1.4873, 1.9269, 1.4873, 1.9269, 1.4873]) Building Model Next, we are going to initialize the weights and biases of our first and second hidden layers. Since the input dimension of our first layer is 6 and the number of neurons is 100, we initialize the weight matrix of shape $6\\times 100$ and the bias vector of length 100. The same rule applies to the second layer. The second layer\u0026rsquo;s input dimension is the first layer\u0026rsquo;s output dimension, 100. Because the output of the second layer is the probability of all 27 characters, we initialize the weight matrix of shape $100\\times 27$ with the bias vector of length 27.\n# 1st hidden layer W1 = torch.randn((6, 100)) b1 = torch.randn(100) # output for 1st layer h = embed.view(-1, 6) @ W1 + b1 # 2nd hidden layer W2 = torch.randn((100, 27)) b2 = torch.randn(27) # output for 2nd layer logits = h @ W2 + b2 Making Predictions The next step is our first forward pass to obtain the probabilities of the next characters.\n# softmax counts = logits.exp() probs = counts / counts.sum(1, keepdims=True) loss = -probs[torch.arange(X.shape[0]), y].log().mean() print(f\u0026#34;Overall loss: {loss:.6f}\u0026#34;) Overall loss: nan However, as Andrej mentioned in the video, there is a potential issue with calculating the softmax function traditionally, as we saw above. If the output logits contain a large value, such as 100, applying the exponential function can result in nan values. Therefore, a better way to calculate the loss is to use the built-in cross_entropy function instead.\nloss = F.cross_entropy(logits, y) print(f\u0026#34;Overall loss: {loss:.6f}\u0026#34;) Overall loss: 78.392731 Put Everything Together Here is the code after we put everything together and enabled backward pass.\ng = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True print(f\u0026#34;Total parameters: {sum(p.nelement() for p in parameters)}\u0026#34;) Total parameters: 3481 The total number of learnable parameters of our model is 3482. Next, we are going to run the model for 10 epochs and see how loss changes. Notice that we apply the activation function tanh as described in the paper in the code below.\nfor _ in range(10): # forward pass embed = C[X] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y) print(f\u0026#34;Loss: {loss.item()}\u0026#34;) # backward pass for p in parameters: p.grad = None loss.backward() # update weights lr = 0.1 for p in parameters: p.data += -lr * p.grad Loss: 16.72646713256836 Loss: 14.942943572998047 Loss: 13.863017082214355 Loss: 13.003837585449219 Loss: 12.292213439941406 Loss: 11.732643127441406 Loss: 11.270574569702148 Loss: 10.859720230102539 Loss: 10.479723930358887 Loss: 10.136445045471191 The model\u0026rsquo;s loss decreases as expected. However, you will notice that the loss comes out slower if you have a larger model with much more parameters. Why? Because we are using the whole dataset as a batch to calculate the loss and update the weights accordingly. In Stochastic Gradient Descent (SGD), the model parameters are updated based on the gradient of the loss function with respect to a randomly selected subset of the training data. Moreover, we can apply this idea to accelerate the training process.\nApplying Mini-batch We pick 32 as the mini-batch size, and the model runs very fast for 1000 epochs.\nfor i in range(1000): # batch_size = 32 idx = torch.randint(0, X.shape[0], (32, )) # forward pass embed = C[X[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 # using the whole dataset as a batch loss = F.cross_entropy(logits, y[idx]) if i % 50 == 0: print(f\u0026#34;Loss: {loss.item()}\u0026#34;) # backward pass for p in parameters: p.grad = None loss.backward() # update weights lr = 0.1 for p in parameters: p.data += -lr * p.grad embed = C[X] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y) print(f\u0026#34;Overall loss: {loss.item()}\u0026#34;) Loss: 10.522725105285645 Loss: 4.547809600830078 Loss: 3.9053943157196045 Loss: 3.5418882369995117 Loss: 3.312927722930908 Loss: 3.10072660446167 Loss: 3.188538074493408 Loss: 2.6955881118774414 Loss: 2.9730937480926514 Loss: 2.5453033447265625 Loss: 3.034700870513916 Loss: 2.2029476165771484 Loss: 2.5462143421173096 Loss: 2.6591145992279053 Loss: 2.9640085697174072 Loss: 3.142090082168579 Loss: 2.5031352043151855 Loss: 2.721736431121826 Loss: 2.7801644802093506 Loss: 2.32700252532959 Overall loss: 2.6130335330963135 Learning Rate Selection So how can we determine a suitable learning rate? In our previous training processes, we used a fixed learning rate of 0.1, but how can we know that 0.1 is optimal? Next, we are going to do some experiments to explore how to choose a good learning rate.\ng = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True # logarithm learning rate, base 10 lre = torch.linspace(-3, 0, 1000) # learning rates lrs = 10 ** lre losses = [] for i in range(1000): idx = torch.randint(0, X.shape[0], (32, )) embed = C[X[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y[idx]) if i % 50 == 0: print(f\u0026#34;Loss: {loss.item()}\u0026#34;) for p in parameters: p.grad = None loss.backward() lr = lrs[i] for p in parameters: p.data += -lr * p.grad losses.append(loss.item()) plt.plot(lre, losses) Loss: 17.930665969848633 Loss: 15.90300464630127 Loss: 14.608807563781738 Loss: 11.146048545837402 Loss: 14.368053436279297 Loss: 10.241884231567383 Loss: 10.7547607421875 Loss: 9.06742000579834 Loss: 6.721671104431152 Loss: 4.959266185760498 Loss: 7.631305694580078 Loss: 6.03385591506958 Loss: 3.6078100204467773 Loss: 3.7624008655548096 Loss: 2.994145393371582 Loss: 2.6852164268493652 Loss: 3.392582893371582 Loss: 3.5405192375183105 Loss: 4.318221569061279 Loss: 5.7273149490356445 Figure 1: A plot for loss on different logarithm of learing rates According to the plot in Figure 1, the optimal logarithmic learning rate is around -1.0, which makes the learning rate 0.1.\nLearning Rate Decay As the training progresses, the loss could encounter a plateau, meaning that it stops decreasing even though the training process is still ongoing. To overcome this, learning rate decay can be applied, which decreases the learning rate over time as the training progresses. The model can escape from plateaus and continue improving its performance.\ng = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True losses = [] epochs = 20000 for i in range(epochs): idx = torch.randint(0, X.shape[0], (32, )) embed = C[X[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y[idx]) for p in parameters: p.grad = None loss.backward() # learning rate decay lr = 0.1 if i \u0026lt; epochs // 2 else 0.001 for p in parameters: p.data += -lr * p.grad losses.append(loss.item()) plt.plot(range(epochs), losses) Train, Validation, and Test Evaluating the model performance on unseen data is important to make sure it generalizes well. It is common practice to split the training data into three parts: 80% for training, 10% for validation, and 10% for testing. The validation set could also be used for early stopping, which means stopping the training process when the performance on the validation set starts to degrade, preventing the model from overfitting to the training set.\ndef build_dataset(words, block_size=3): X, Y = [], [] for w in words: context = [0] * block_size for char in w + \u0026#34;.\u0026#34;: idx = stoi[char] X.append(context) Y.append(idx) context = context[1:] + [idx] X = torch.tensor(X) Y = torch.tensor(Y) print(X.shape, Y.shape) return X, Y import random random.seed(42) random.shuffle(words) n1 = int(0.8*len(words)) n2 = int(0.9*len(words)) X_tr, y_tr = build_dataset(words[:n1]) X_va, y_va = build_dataset(words[n1:n2]) X_te, y_te = build_dataset(words[n2:]) g = torch.Generator().manual_seed(42) C = torch.randn((27, 2), generator=g) W1 = torch.randn((6, 100), generator=g) b1 = torch.randn(100, generator=g) W2 = torch.randn((100, 27), generator=g) b2 = torch.randn(27, generator=g) parameters = [C, W1, b1, W2, b2] for p in parameters: p.requires_grad = True tr_losses = [] va_losses = [] epochs = 20000 for i in range(epochs): idx = torch.randint(0, X_tr.shape[0], (32, )) embed = C[X_tr[idx]] h = torch.tanh(embed.view(-1, 6) @ W1 + b1) logits = h @ W2 + b2 loss = F.cross_entropy(logits, y_tr[idx]) for p in parameters: p.grad = None loss.backward() # learning rate decay lr = 0.1 if i \u0026lt; epochs // 2 else 0.01 for p in parameters: p.data += -lr * p.grad tr_losses.append(loss.item()) val_embed = C[X_va] val_h = torch.tanh(val_embed.view(-1, 6) @ W1 + b1) val_logits = val_h @ W2 + b2 val_loss = F.cross_entropy(val_logits, y_va) va_losses.append(val_loss.item()) plt.plot(range(epochs), tr_losses, label=\u0026#39;Training Loss\u0026#39;) plt.plot(range(epochs), va_losses, label=\u0026#39;Validation Loss\u0026#39;) plt.title(\u0026#39;Training and Validation Loss\u0026#39;) plt.xlabel(\u0026#39;Epochs\u0026#39;) plt.ylabel(\u0026#39;Loss\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.show() torch.Size([182625, 3]) torch.Size([182625]) torch.Size([22655, 3]) torch.Size([22655]) torch.Size([22866, 3]) torch.Size([22866]) Figure 2: Plot for training and validation loss As shown in Figure 2, there is a tiny drop in the validation loss at 10000 epochs, which indicates that our training did encounter a plateau and learning rate decay works very well. Let\u0026rsquo;s check the loss of the testing data.\ntest_embed = C[X_te] test_h = torch.tanh(test_embed.view(-1, 6) @ W1 + b1) test_logits = test_h @ W2 + b2 test_loss = F.cross_entropy(test_logits, y_te) print(f\u0026#34;Loss on validation data: {val_loss:.6f}\u0026#34;) print(f\u0026#34;Loss on testing data: {test_loss:.6f}\u0026#34;) Loss on validation data: 2.375811 Loss on testing data: 2.374066 The losses on validation and testing data are close, indicating we are not overfitting.\nVisualization of Embedding Let\u0026rsquo;s visualize our embedding matrix.\nplt.figure(figsize=(8,8)) plt.scatter(C[:, 0].data, C[:, 1].data, s = 200) for i in range(C.shape[0]): plt.text(C[i, 0].item(), C[i, 1].item(), itos[i], ha=\u0026#34;center\u0026#34;, va=\u0026#34;center\u0026#34;, color=\u0026#34;white\u0026#34;) plt.grid(\u0026#39;minor\u0026#39;) plt.show() Figure 3: Visualization of 2D embedding matrix As depicted in Figure 3, the vowels are closely grouped in the left bottom corner of the plot, while the . is situated far away in the top right corner.\nWord Generation The last thing we want to do is word generation.\ng = torch.Generator().manual_seed(420) for _ in range(20): out = [] context = [0] * block_size while True: embed = C[torch.tensor([context])] h = torch.tanh(embed.view(1, -1) @ W1 + b1) logits = h @ W2 + b2 probs = F.softmax(logits, dim=1) idx = torch.multinomial(probs, num_samples=1, generator=g).item() context = context[1:] + [idx] if idx == 0: break out.append(idx) print(\u0026#39;\u0026#39;.join(itos[i] for i in out)) rai mal lemistani iua kacyt tan zatlixahnen rarbi zethanli blie mozien nar ameson xaxun koma aedh sarixstah elin dyannili saom The words generated by the multilayer perceptron model make more sense than those from our last model. Still, there are many other ways to improve model performance. For example, train more epochs with learning rate decay, increase the batch size to make the training more stable, and add more data.\n","permalink":"https://gejun.name/natural-language-processing/building-makemore-mlp/","summary":"In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters.","title":"Multilayer Perceptron (MLP)"},{"content":"This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.\nIn this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let\u0026rsquo;s prepare the data first.\nData Preparation Load data We are using the most common 32k names of 2018 from ssa.gov website as our data source. First, we apply the code below to obtain each bigram\u0026rsquo;s frequency. If you don\u0026rsquo;t know what a bigram is, a bigram is a sequence of two adjacent words or characters in a text. We also add a special character, \u0026ldquo;.\u0026rdquo;, to the name\u0026rsquo;s beginning and end to indicate its start and end, respectively. As can be seen that the top 5 common bigrams in the data are n., a., an, .a, and e..\nfrom collections import Counter words = open(\u0026#34;names.txt\u0026#34;, \u0026#34;r\u0026#34;).read().splitlines() counter = Counter() for word in words: chs = list(\u0026#34;.\u0026#34; + word + \u0026#34;.\u0026#34;) for c1, c2 in zip(chs, chs[1:]): bigram = (c1, c2) counter[bigram] += 1 for bigram, frequency in counter.most_common(5): print(f\u0026#34;Frequency of {\u0026#39;\u0026#39;.join(bigram)}: {frequency}\u0026#34;) Frequency of n.: 6763 Frequency of a.: 6640 Frequency of an: 5438 Frequency of .a: 4410 Frequency of e.: 3983 Numericallization As is known that computers are good at processing numerical data; however, they may not be efficient in dealing with text. So our second step is to create two mappings: string to index and index to string. These mappings are used to represent words or characters numerically. This process is sometimes called numericalization.\nimport torch import string import matplotlib.pyplot as plt chars = string.ascii_lowercase stoi = {s: i+1 for i, s in enumerate(chars)} stoi[\u0026#34;.\u0026#34;] = 0 itos = {i: s for s, i in stoi.items()} print(stoi, itos) {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0} {1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'} Counting Approach Frequency Our first step is to obtain the frequencies of the bigrams. Since we have a vocabulary of 27 characters-26 letters in lowercase plus 1 special character, we need a $27\\times 27$ matrix to store the frequencies of all possible bigrams. Figure 1 is a heatmap of the calculated frequencies. The darker the color, the higher the frequency of the bigram.\nN = torch.zeros((27, 27), dtype=torch.int32) for (c1, c2), freq in counter.items(): idx1 = stoi[c1] idx2 = stoi[c2] N[idx1, idx2] = freq plt.figure(figsize=(16, 16)) plt.imshow(N, cmap=\u0026#34;Blues\u0026#34;) for i in range(27): for j in range(27): chstr = itos[i] + itos[j] plt.text(j, i, chstr, ha=\u0026#34;center\u0026#34;, va=\u0026#34;bottom\u0026#34;, color=\u0026#34;gray\u0026#34;) plt.text(j, i, N[i, j].item(), ha=\u0026#34;center\u0026#34;, va=\u0026#34;top\u0026#34;, color=\u0026#34;gray\u0026#34;) plt.axis(\u0026#34;off\u0026#34;) plt.show() Figure 1: A heatmap plot for frequencies of bigrams Probability To get the probability of each bigram, we want to normalize the matrix N by row. Why? Because we want to know the probability of the character given the current character we have in the process of character generation, i.e., $P(next\\ char | current\\ char)$. To avoid calculating $log0$ later on, we add 1 to the frequency of each bigram.\nP = (N + 1).float() P /= P.sum(1, keepdims=True) Maximum Likelihood Maximum likelihood is a statistical method to estimate the parameters of a probability distribution based on observed data. The goal of maximum likelihood is to find the values of the distribution\u0026rsquo;s parameters that make the observed data most likely to have been generated by that distribution. In our case, we want the next generated character comes from the probability distribution as much as possible. How do we calculate the likelihood? It is the product of the probability of each bigram in a word. $$ L(\\theta) = P(X_1=x_1, X_2=x_2, \u0026hellip;, X_n=x_n) = \\Pi_i^n P(X_i=x_i)$$ For example, the likelihood of the word good is calculated as $$Likelihood= P(\u0026quot;.g\u0026quot;) * P(\u0026ldquo;go\u0026rdquo;) * P(\u0026ldquo;oo\u0026rdquo;) * P(\u0026ldquo;od\u0026rdquo;) * P(\u0026ldquo;d.\u0026rdquo;) $$ $$ = 0.0209*0.0430*0.0146*0.0240*0.0936=2.9399e-8$$\ndef calc_likelihood(word, verbose=False): word = list(\u0026#34;.\u0026#34; + word + \u0026#34;.\u0026#34;) likelihood = 1.0 for c1, c2 in zip(word, word[1:]): idx1 = stoi[c1] idx2 = stoi[c2] prob = P[idx1, idx2] if verbose: print(f\u0026#34;probability for {\u0026#39;\u0026#39;.join((c1, c2))}: {prob:.4f}\u0026#34;) likelihood *= prob return likelihood prob = calc_likelihood(\u0026#34;good\u0026#34;, verbose=True) print(f\u0026#34;Likelihood for good is: {prob:.4e}\u0026#34;) probability for .g: 0.0209 probability for go: 0.0430 probability for oo: 0.0146 probability for od: 0.0240 probability for d.: 0.0936 Likelihood for good is: 2.9399e-08 Let\u0026rsquo;s generate some words by randomly picking the bigram according to its probability using torch.multinomial function and calculate their likelihoods.\ng = torch.Generator().manual_seed(420) generated_words = [] for i in range(5): out = [] ix = 0 while True: p = P[ix] ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() if ix == 0: break out.append(itos[ix]) generated_words.append((\u0026#34;\u0026#34;.join(out), calc_likelihood(\u0026#34;\u0026#34;.join(out)).item())) generated_words.sort(key=lambda x: -x[1]) for gw, lh in generated_words: print(f\u0026#34;Likelihood for {gw}: {lh}\u0026#34;) Likelihood for jen: 0.0005491252522915602 Likelihood for jor: 0.0001786774955689907 Likelihood for she: 0.00017446796118747443 Likelihood for tais: 3.90183367926511e-06 Likelihood for anuir: 2.335933579900029e-08 It turns out jen which has the maximum likelihood 0.000549 is the winner in these 5 randomly generated words. Remember that our goal is to maximize the likelihood of the word the model generates because the higher the likelihood, the better the model. However, notice that the likelihoods for the generated words are too small, so applying a log function to each probability would make it easier to work with. $$ logL(\\theta) = log\\Pi_i^n P(X_i=x_i)=\\Sigma_i^nlogP(X_i=x_i)$$\nAdditionally, maximizing the likelihood is the same as maximizing the log-likelihood because the logarithm function is a monotonic increasing function, which is the same as minimizing the negative log-likelihood. We prefer minimization to maximization in any optimization problem. Let\u0026rsquo;s calculate the average negative log-likelihood of our name dataset, which is 2.454679.\ndef calc_nll(word): word = list(\u0026#34;.\u0026#34; + word + \u0026#34;.\u0026#34;) log_likelihood = 0.0 for c1, c2 in zip(word, word[1:]): idx1 = stoi[c1] idx2 = stoi[c2] prob = P[idx1, idx2] log_prob = torch.log(prob) log_likelihood += log_prob return -log_likelihood nlls = [calc_nll(w) for w in words] ns = [len(w) + 1 for w in words] print(f\u0026#34;Average negative log-likelihood: {sum(nlls)/sum(ns):.6f}\u0026#34;) Average negative log-likelihood: 2.454579 Neural Network How does a neural network model fit in the character generation? Think of it in this way: given the last generated character, we want the model to output a probability distribution for the next character, in which we can find the most likely character to follow it. In other words, our task is to use the model to estimate the probability distribution based on the dataset rather than relying on counting the occurrences of each bigram. As always, let\u0026rsquo;s prepare the data in the first step.\nTraining Data Preparation The training data is created using bigrams, where the first character is the input feature, and the second character is used as the target of the model. Since feeding integers into a neural network and multiplying them with weights does not make sense, we need to transform them into a different format. The most common method is one-hot encoding, which transforms each integer into a vector with all 0s except for a 1 at the index corresponding to the integer. PyTorch provides a built-in torch.nn.functional.one_hot function for one-hot encoding.\nimport torch.nn.functional as F xs, ys = [], [] for word in words: chs = list(\u0026#34;.\u0026#34; + word + \u0026#34;.\u0026#34;) for c1, c2 in zip(chs, chs[1:]): idx1 = stoi[c1] idx2 = stoi[c2] xs.append(idx1) ys.append(idx2) # tensor function returns the same type as its original xs = torch.tensor(xs) ys = torch.tensor(ys) xenc = F.one_hot(xs, num_classes=27).float() print(xenc.shape) torch.Size([228146, 27]) After applying one-hot encoding, we have a tensor xenc of shape $228146\\times 27$.\nUnderstanding Weights The weight matrix of our model has the same shape as the matrix N above but is initialized with random values. PyTorch\u0026rsquo;s built-in function torch.randn gives us random numbers from a normal distribution with mean 0 and standard deviation 1, resulting in positive and negative values. After multiplying the one-hot encoding matrix with weights, we obtain the output of the first layer, which may contain negative values. However, we want the output to represent the probability of the next character, as we calculated above. To achieve this, we can treat the output as the logarithm of the frequencies and apply the exponential function to obtain the positive values, which can be interpreted as the frequencies of the bigrams starting with the input feature. Why? Because multiplying a one-hot encoding vector having a 1 at index i, with the weight matrix W is the same as getting the ith row of W. And we want this frequency matrix to be close to the matrix N as close as possible. If we further normalize the output over the rows, we can obtain the probability distribution of bigrams. In fact, the last two steps, applying exponential function and normalization, of calculation are known as the softmax function.\ng = torch.Generator().manual_seed(420) W = torch.randn((27, 27), generator=g, requires_grad=True) # log-counts logits = xenc @ W # (228146, 27) x (27, 27) # counts counts = logits.exp() # probability probs = counts / counts.sum(1, keepdim=True) print(probs.shape) print(sum(probs[1,:])) torch.Size([228146, 27]) tensor(1.0000, grad_fn=\u0026lt;AddBackward0\u0026gt;) Optimization Remember that our goal is to approach the actual probabilities from the training data using maximum likelihood estimation. As the training progresses, the model adjusts the weights in such a way that the predicted probabilities for the next character in a word are as close to the actual probabilities of the training data. By minimizing the negative log-likelihood, we effectively minimize the distance between predicted and actual probabilities. Let\u0026rsquo;s take the first word, emma, as an example and see how the neural network calculates its loss. This step is also called forward pass. The first bigram is .e with the input . (index 0) and actual label e (index 5). The one-hot encoding for . is [1, 0, ..., 0], and the output probability for e is 0.0246. Applying log and negation, we have the loss as 3.7050. The same calculation applies to em, mm, ma, and a.. Finally, we get the loss for emma is 3.6985.\nnlls = torch.zeros(5) for i in range(5): x = xs[i].item() y = ys[i].item() print(\u0026#39;-\u0026#39; * 50) print(f\u0026#39;bigram example {i+1}: {itos[x]} {itos[y]} (indexes {x}, {y})\u0026#39;) print(f\u0026#39;input to the neural network: {x}\u0026#39;) print(f\u0026#39;output probbabilities from the nn: {probs[i]}\u0026#39;) print(f\u0026#39;label (actual next character): {y}\u0026#39;) p = probs[i, y] print(f\u0026#39;probability assigned by the nn to the correct character: {p.item()}\u0026#39;) logp = torch.log(p) print(f\u0026#39;log-likelihood: {logp.item()}\u0026#39;) nll = -logp print(f\u0026#39;negative log likelihood: {nll}\u0026#39;) nlls[i] = nll print(f\u0026#34;Average negative log-likelihood, i.e., loss={nlls.mean().item()}\u0026#34;) -------------------------------------------------- bigram example 1: . e (indexes 0, 5) input to the neural network: 0 output probbabilities from the nn: tensor([0.0167, 0.0278, 0.0328, 0.0114, 0.0173, 0.0246, 0.0100, 0.0341, 0.1024, 0.0259, 0.2364, 0.0219, 0.0422, 0.0108, 0.1262, 0.0647, 0.0130, 0.0162, 0.0157, 0.0093, 0.0184, 0.0022, 0.0482, 0.0090, 0.0069, 0.0195, 0.0362], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 5 probability assigned by the nn to the correct character: 0.024598384276032448 log-likelihood: -3.7050745487213135 negative log likelihood: 3.7050745487213135 -------------------------------------------------- bigram example 2: e m (indexes 5, 13) input to the neural network: 5 output probbabilities from the nn: tensor([0.1219, 0.0087, 0.0157, 0.0546, 0.0067, 0.0149, 0.0185, 0.0338, 0.0110, 0.0030, 0.0060, 0.0697, 0.0211, 0.0579, 0.0061, 0.0043, 0.0746, 0.0416, 0.0264, 0.0611, 0.0823, 0.0124, 0.0179, 0.0129, 0.0374, 0.1633, 0.0162], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 13 probability assigned by the nn to the correct character: 0.057898372411727905 log-likelihood: -2.8490660190582275 negative log likelihood: 2.8490660190582275 -------------------------------------------------- bigram example 3: m m (indexes 13, 13) input to the neural network: 13 output probbabilities from the nn: tensor([0.3351, 0.0126, 0.0370, 0.0075, 0.0302, 0.0635, 0.0042, 0.0339, 0.0155, 0.0512, 0.0080, 0.0283, 0.0557, 0.0171, 0.0388, 0.0103, 0.0507, 0.0398, 0.0191, 0.0074, 0.0174, 0.0132, 0.0121, 0.0245, 0.0307, 0.0219, 0.0142], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 13 probability assigned by the nn to the correct character: 0.017136109992861748 log-likelihood: -4.066567420959473 negative log likelihood: 4.066567420959473 -------------------------------------------------- bigram example 4: m a (indexes 13, 1) input to the neural network: 13 output probbabilities from the nn: tensor([0.3351, 0.0126, 0.0370, 0.0075, 0.0302, 0.0635, 0.0042, 0.0339, 0.0155, 0.0512, 0.0080, 0.0283, 0.0557, 0.0171, 0.0388, 0.0103, 0.0507, 0.0398, 0.0191, 0.0074, 0.0174, 0.0132, 0.0121, 0.0245, 0.0307, 0.0219, 0.0142], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 1 probability assigned by the nn to the correct character: 0.012621787376701832 log-likelihood: -4.372330665588379 negative log likelihood: 4.372330665588379 -------------------------------------------------- bigram example 5: a . (indexes 1, 0) input to the neural network: 1 output probbabilities from the nn: tensor([0.0302, 0.0788, 0.0096, 0.0099, 0.0151, 0.1021, 0.0146, 0.0253, 0.0076, 0.0107, 0.0429, 0.0286, 0.0371, 0.0437, 0.0168, 0.0133, 0.0129, 0.0075, 0.0038, 0.0199, 0.0854, 0.0875, 0.1194, 0.1119, 0.0151, 0.0325, 0.0177], grad_fn=\u0026lt;SelectBackward0\u0026gt;) label (actual next character): 0 probability assigned by the nn to the correct character: 0.030220769345760345 log-likelihood: -3.4992258548736572 negative log likelihood: 3.4992258548736572 Average negative log-likelihood, i.e., loss=3.6984527111053467 So how do we calculate the loss efficiently? It turns out that we can pass all these row and column indices to the matrix, and then take log and mean afterwards. The loss for the forward pass is 3.6374.\nloss = -probs[torch.arange(len(xs)), ys].log().mean() print(f\u0026#34;Overall loss: {loss.item()}\u0026#34;) Overall loss: 3.637367010116577 After obtaining the average loss from the forward pass, we need a backward pass to update the weights. To do this, we need to make sure that the parameter requires_grad is set to True for the weight matrix W. Next, we zero out all gradients to avoid the accumulation of gradients across batches. We then call loss.backward() to compute the gradient of the oss with regard to each weight. The gradient of a weight indicates how much that increasing that weight will affect the loss. If it is positive, increasing the weight will increase the loss too. Conversely, increasing the weight will decrease the loss if the gradient is negative. For example, W.grad[0, 0]=0.002339 means that W[0, 0] has a positive effect on the loss.\n# set the gradient to zero W.grad = None # backward pass loss.backward() The next step is to update the weights and recalculate the averge loss.\nlr = 0.1 W.data += -lr * W.grad # forward pass logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) loss = -probs[torch.arange(len(xs)), ys].log().mean() print(f\u0026#34;Overall loss: {loss.item()}\u0026#34;) Overall loss: 3.6365137100219727 The overall loss is now 3.6365, which is slightly lower than before. We can keep doing this gradient descent step until the model performance is good enough. As we train more epochs, the overall loss is getting closer to the actual overall loss, which is 2.454579.\nlr = 50 for i in range(301): logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) loss = -probs[torch.arange(len(xs)), ys].log().mean() if i % 50 == 0: print(f\u0026#34;Epochs: {i}, loss: {loss.item()}\u0026#34;) W.grad = None loss.backward() W.data += -lr * W.grad Epochs: 0, loss: 3.6365137100219727 Epochs: 50, loss: 2.4955990314483643 Epochs: 100, loss: 2.4727954864501953 Epochs: 150, loss: 2.4657769203186035 Epochs: 200, loss: 2.4625258445739746 Epochs: 250, loss: 2.4606406688690186 Epochs: 300, loss: 2.459399700164795 Note that our current neural network only has one hidden layer. We can add more hidden layers to improve the model performance. Additionally, we can add a regularization item (e.g., the mean of the square of all weights) in the loss function to prevent overfitting.\nloss = -probs[torch.arange(len(xs)), ys].log().mean() + 0.01 * (W ** 2).mean() print(loss) tensor(2.4834, grad_fn=\u0026lt;AddBackward0\u0026gt;) In this case, the optimization has two components\u0026ndash;average negative log-likelihood and mean of the square of weights. The regularization item works like a force to squeeze the weights and make them close to zeros as much as possible. The last step is to sample characters from the neural network model.\ng = torch.Generator().manual_seed(420) nn_generated_words = [] for _ in range(5): out = [] idx = 0 while True: xenc = F.one_hot(torch.tensor([idx]), num_classes=27).float() logits = xenc @ W counts = logits.exp() probs = counts / counts.sum(1, keepdim=True) idx = torch.multinomial(probs, num_samples=1, replacement=True, generator=g).item() if idx == 0: break out.append(itos[idx]) nn_generated_words.append((\u0026#34;\u0026#34;.join(out), calc_likelihood(\u0026#34;\u0026#34;.join(out)).item())) nn_generated_words.sort(key=lambda x: -x[1]) for gw, lh in nn_generated_words: print(f\u0026#34;Likelihood for {gw}: {lh}\u0026#34;) Likelihood for jen: 0.0005491252522915602 Likelihood for jor: 0.0001786774955689907 Likelihood for she: 0.00017446796118747443 Likelihood for tais: 3.90183367926511e-06 Likelihood for anuir: 2.335933579900029e-08 We are using the same seed for the generator. The words generated from the neural network are exactly the same as those generated from the probability table above, which is what we want to see.\n","permalink":"https://gejun.name/natural-language-processing/building-makemore/","summary":"This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.\nIn this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let\u0026rsquo;s prepare the data first.","title":"Bigram Character-level Language Model"}]