<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>transformers on Gejun&#39;s Blog</title>
    <link>https://gejun.name/tags/transformers/</link>
    <description>Recent content in transformers on Gejun&#39;s Blog</description>
    <image>
      <title>Gejun&#39;s Blog</title>
      <url>https://gejun.name</url>
      <link>https://gejun.name</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 15 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gejun.name/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding Transformer Architecture by Building GPT</title>
      <link>https://gejun.name/natural-language-processing/understanding-attention-nlp/</link>
      <pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://gejun.name/natural-language-processing/understanding-attention-nlp/</guid>
      <description>In Part2, we constructed a straightforward MLP model to generate characters based on 32k popular names. In this lecture, Andrej guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model. We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.
Data Preparation Let&amp;rsquo;s first import the necessary libraries and get the data ready.</description>
    </item>
    
  </channel>
</rss>
