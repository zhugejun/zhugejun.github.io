---
title: "Multilayer Perceptron (MLP)"
date: "2023-03-13"
categories: 
  - Neural Network
  - NLP
  - MOOC
  - Perceptron
tags: 
  - n-gram
  - pytorch
  - embedding
format: hugo-md
math: true
jupyter: python3
---


In [Part1]({{< ref "../building-makemore.indexmd" >>}}), we learned how to build a neural network with one hidden layer to generate words.
The model we built performed fairly well as we got the exact words generated based on counting.
However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character.
Suppose there is only one bigram starting with a particular character.
In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters.
This lack of context can lead to poor performance of bigram models.
In this lecture, Andrej shows us how to build a multilayer neural network to improve the model performance.

Unlike the bigram model we built in the last lecture, our new mode is a multilayer perceptron (MLP) that takes the previous 2 characters to predict the probabilities of the next character.
This MLP language model was proposed in the paper [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) by Bengio et al. in 2003.
As always, the official Jupyter Notebook for this part is [here](https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part2_mlp.ipynb).


## Data Preparation

First, we build our vocabulary as we did before.

```{python}
from collections import Counter
import torch
import string
import matplotlib.pyplot as plt
import torch.nn.functional as F

words = open("names.txt", "r").read().splitlines()
chars = string.ascii_lowercase
stoi = {s: i+1 for i, s in enumerate(chars)}
stoi["."] = 0
itos = {i: s for s, i in stoi.items()}
print(stoi, itos)
```

Next, we create the training data.
This time, we use the last 2 characters, instead of 1, to predict the next character, which is a 3-gram or trigram model.
```{python}
block_size = 3
X, y = [], []

for word in words:
  # initialize context
  context = [0] * block_size
  for char in word + ".":
    idx = stoi[char]
    X.append(context)
    y.append(idx)
    # truncate the first char and add the new char
    context = context[1:] + [idx]
X = torch.tensor(X)
y = torch.tensor(y)
print(X.shape, y.shape)
```


## Multilayer Perceptron (MLP)

As stated in the name, our neural network model will have multiple hidden layers.
Besides this, we will also learn a new way to represent characters.

### Feature Vector

![feature vector idea](mlp-feature-vector.png)

The paper proposed that each word would be associated with a feature vector which can be learned as training progresses.
In other words, we use feature vectors to represent words in a language model.
The number of features or the length of the vector is much smaller than the size of the vocabulary.
Since the size of our vocabulary is 27, we will use a vector of length of 2 for now.
This feature vector can be considered as word embedding nowadays.

```{python}
g = torch.Generator().manual_seed(42)
# initialize lookup table
C = torch.randn((27, 2), generator=g)
print(f"Vector representation for character a is: {C[stoi['a']]}")
```
The code above initializes our lookup table with $27\times 2$ random numbers using `torch.randn` function.
As we can see that the vector representation for the character `a` is `[ 0.9007, -2.1055]`.

Next, we are going to replace the indices in matrix `X` with vector representations.
Since multiplying a one-hot encoding vector having a 1 at index `i` with the weight matrix `W` is the same as getting the `ith` row of `W`, we will extract the `ith` row from the embedding matrix directly instead of multiplying one-hot encoding with it.

![matrix multiplication](matrix-multiplication.png)

According to the [tensor indexing documentation](https://pytorch.org/cppdocs/notes/tensor_indexing.html#getter) of PyTorch, we can extract corresponding feature vectors by treating `X` as an indexing matrix.

```{python}
embed = C[X]
print(f"First row of X: {X[0, :]}")
print(f"First row of embedding: {embed[0,:]}")
print(embed.shape)
```
To put it in another way, we transform the matrix `X` of $228146\times 3$ to the embedding matrix `embed` of $228146\times 3 \times 2$ because all the indices have been replaced with a vector of $1\times 2$.

### Model Architecture

![MLP architecture](mlp-architecture.png)

As highlighted in the picture above, we should have a vector for each trigram after extracting its feature from the lookup table.
This way, we can do matrix multiplication like before.
However, we have a $3\times 2$ matrix for each trigram instead.
So we need to concatenate all the rows of the matrix into one vector.
We can use `torch.cat` to concatenate the second dimension together, but PyTorch has a more efficient way, the `view` function([doc](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)), to do so.
See this [blog post](http://blog.ezyang.com/2019/05/pytorch-internals/) for more details about tensor and PyTorch internals.

```{python}
print(embed[0, :])
print(embed.view(-1, 6)[0, :])
```

### Building Model

Next, we are going to initialize the weights and biases of our first and second hidden layers.
Since the input dimension of our first layer is 6 and the number of neurons is 100, we initialize the weight matrix of shape $6\times 100$ and the bias vector of length 100.
The same rule applies to the second layer.
The second layer's input dimension is the first layer's output dimension, 100.
Because the output of the second layer is the probability of all 27 characters, we initialize the weight matrix of shape $100\times 27$ with the bias vector of length 27.

```{python}
# 1st hidden layer
W1 = torch.randn((6, 100))
b1 = torch.randn(100)

# output for 1st layer
h = embed.view(-1, 6) @ W1 + b1

# 2nd hidden layer
W2 = torch.randn((100, 27))
b2 = torch.randn(27)

# output for 2nd layer
logits = h @ W2 + b2
```

### Making Predictions

The next step is our first forward pass to obtain the probabilities of the next characters. 

```{python}
# softmax
counts = logits.exp()
probs = counts / counts.sum(1, keepdims=True)
loss = -probs[torch.arange(X.shape[0]), y].log().mean()
print(f"Overall loss: {loss:.6f}")
```

However, as Andrej mentioned in the video, there is a potential issue with calculating the softmax function traditionally, as we saw above.
If the output logits contain a large value, such as 100, applying the exponential function can result in `nan` values. 
Therefore, a better way to calculate the loss is to use the built-in `cross_entropy` function instead.

```{python}
loss = F.cross_entropy(logits, y) 
print(f"Overall loss: {loss:.6f}")
```

## Put Everything Together

Here is the code after we put everything together and enabled backward pass.

```{python}
g = torch.Generator().manual_seed(42)
C = torch.randn((27, 2), generator=g)
W1 = torch.randn((6, 100), generator=g)
b1 = torch.randn(100, generator=g)
W2 = torch.randn((100, 27), generator=g)
b2 = torch.randn(27, generator=g)
parameters = [C, W1, b1, W2, b2]

for p in parameters:
  p.requires_grad = True

print(f"Total parameters: {sum(p.nelement() for p in parameters)}")
```
The total number of learnable parameters of our model is 3482. 
Next, we are going to run the model for 10 epochs and see how loss changes.
Notice that we apply the activation function `tanh` as described in the paper in the code below.

```{python}
for _ in range(10):
  # forward pass
  embed = C[X]
  h = torch.tanh(embed.view(-1, 6) @ W1 + b1)
  logits = h @ W2 + b2
  loss = F.cross_entropy(logits, y)
  print(f"Loss: {loss.item()}")

  # backward pass
  for p in parameters:
    p.grad = None
  loss.backward()

  # update weights
  lr = 0.1
  for p in parameters:
    p.data += -lr * p.grad
```
The model's loss decreases as expected.
However, you will notice that the loss comes out slower if you have a larger model with much more parameters.
Why? Because we are using the whole dataset as a batch to calculate the loss and update the weights accordingly.
In [Stochastic Gradient Descent (SGD)](https://www.wikiwand.com/en/Stochastic_gradient_descent), the model parameters are updated based on the gradient of the loss function with respect to a randomly selected subset of the training data.
Moreover, we can apply this idea to accelerate the training process.

### Applying Mini-batch

We pick 32 as the mini-batch size, and the model runs very fast for 1000 epochs.

```{python}
for i in range(1000):
  # batch_size = 32
  idx = torch.randint(0, X.shape[0], (32, ))

  # forward pass
  embed = C[X[idx]]
  h = torch.tanh(embed.view(-1, 6) @ W1 + b1)
  logits = h @ W2 + b2
  # using the whole dataset as a batch
  loss = F.cross_entropy(logits, y[idx])
  if i % 50 == 0:
    print(f"Loss: {loss.item()}")

  # backward pass
  for p in parameters:
    p.grad = None
  loss.backward()

  # update weights
  lr = 0.1
  for p in parameters:
    p.data += -lr * p.grad

embed = C[X]
h = torch.tanh(embed.view(-1, 6) @ W1 + b1)
logits = h @ W2 + b2
loss = F.cross_entropy(logits, y)
print(f"Overall loss: {loss.item()}")

```

### Learning Rate Selection

So how can we determine a suitable learning rate?
In our previous training processes, we used a fixed learning rate of 0.1, but how can we know that 0.1 is optimal?
Next, we are going to do some experiments to explore how to choose a good learning rate.

```{python}
#| label: fig-loss-vs-lr
#| fig-cap: "A plot for loss on different logarithm of learing rates"
g = torch.Generator().manual_seed(42)
C = torch.randn((27, 2), generator=g)
W1 = torch.randn((6, 100), generator=g)
b1 = torch.randn(100, generator=g)
W2 = torch.randn((100, 27), generator=g)
b2 = torch.randn(27, generator=g)
parameters = [C, W1, b1, W2, b2]

for p in parameters:
  p.requires_grad = True

# logarithm learning rate, base 10
lre = torch.linspace(-3, 0, 1000)
# learning rates
lrs = 10 ** lre

losses = []

for i in range(1000):
  idx = torch.randint(0, X.shape[0], (32, ))
  embed = C[X[idx]]
  h = torch.tanh(embed.view(-1, 6) @ W1 + b1)
  logits = h @ W2 + b2
  loss = F.cross_entropy(logits, y[idx])
  if i % 50 == 0:
    print(f"Loss: {loss.item()}")

  for p in parameters:
    p.grad = None
  loss.backward()

  lr = lrs[i]
  for p in parameters:
    p.data += -lr * p.grad
  losses.append(loss.item())

plt.plot(lre, losses)
```
According to the plot in @fig-loss-vs-lr, the optimal logarithmic learning rate is around -1.0, which makes the learning rate 0.1.

### Learning Rate Decay

As the training progresses, the loss could encounter a plateau, meaning that it stops decreasing even though the training process is still ongoing.
To overcome this, learning rate decay can be applied, which decreases the learning rate over time as the training progresses.
The model can escape from plateaus and continue improving its performance.

```{python}
g = torch.Generator().manual_seed(42)
C = torch.randn((27, 2), generator=g)
W1 = torch.randn((6, 100), generator=g)
b1 = torch.randn(100, generator=g)
W2 = torch.randn((100, 27), generator=g)
b2 = torch.randn(27, generator=g)
parameters = [C, W1, b1, W2, b2]

for p in parameters:
  p.requires_grad = True

losses = []
epochs = 20000
for i in range(epochs):
  idx = torch.randint(0, X.shape[0], (32, ))
  embed = C[X[idx]]
  h = torch.tanh(embed.view(-1, 6) @ W1 + b1)
  logits = h @ W2 + b2
  loss = F.cross_entropy(logits, y[idx])

  for p in parameters:
    p.grad = None
  loss.backward()

  # learning rate decay
  lr = 0.1 if i < epochs // 2 else 0.001

  for p in parameters:
    p.data += -lr * p.grad
  losses.append(loss.item())

plt.plot(range(epochs), losses)
```


### Train, Validation, and Test

Evaluating the model performance on unseen data is important to make sure it generalizes well.
It is common practice to split the training data into three parts: 80% for training, 10% for validation, and 10% for testing.
The validation set could also be used for early stopping, which means stopping the training process when the performance on the validation set starts to degrade, preventing the model from overfitting to the training set.


```{python}
#| label: fig-loss-plot
#| fig-cap: Plot for training and validation loss
def build_dataset(words, block_size=3):
  X, Y = [], []

  for w in words:
    context = [0] * block_size
    for char in w + ".":
      idx = stoi[char]
      X.append(context)
      Y.append(idx)
      context = context[1:] + [idx]
  X = torch.tensor(X)
  Y = torch.tensor(Y)
  print(X.shape, Y.shape)
  return X, Y


import random
random.seed(42)
random.shuffle(words)
n1 = int(0.8*len(words))
n2 = int(0.9*len(words))

X_tr, y_tr = build_dataset(words[:n1])
X_va, y_va = build_dataset(words[n1:n2])
X_te, y_te = build_dataset(words[n2:])

g = torch.Generator().manual_seed(42)
C = torch.randn((27, 2), generator=g)
W1 = torch.randn((6, 100), generator=g)
b1 = torch.randn(100, generator=g)
W2 = torch.randn((100, 27), generator=g)
b2 = torch.randn(27, generator=g)
parameters = [C, W1, b1, W2, b2]

for p in parameters:
  p.requires_grad = True

tr_losses = []
va_losses = []

epochs = 20000
for i in range(epochs):
  idx = torch.randint(0, X_tr.shape[0], (32, ))
  embed = C[X_tr[idx]]
  h = torch.tanh(embed.view(-1, 6) @ W1 + b1)
  logits = h @ W2 + b2
  loss = F.cross_entropy(logits, y_tr[idx])

  for p in parameters:
    p.grad = None
  loss.backward()

  # learning rate decay
  lr = 0.1 if i < epochs // 2 else 0.01

  for p in parameters:
    p.data += -lr * p.grad
  tr_losses.append(loss.item())

  val_embed = C[X_va]
  val_h = torch.tanh(val_embed.view(-1, 6) @ W1 + b1)
  val_logits = val_h @ W2 + b2
  val_loss = F.cross_entropy(val_logits, y_va)
  va_losses.append(val_loss.item())

plt.plot(range(epochs), tr_losses, label='Training Loss')
plt.plot(range(epochs), va_losses, label='Validation Loss')
 
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.legend(loc='best')
plt.show()
```

As shown in @fig-loss-plot, there is a tiny drop in the validation loss at 10000 epochs, which indicates that our training did encounter a plateau and learning rate decay works very well.
Let's check the loss of the testing data.

```{python}
test_embed = C[X_te]
test_h = torch.tanh(test_embed.view(-1, 6) @ W1 + b1)
test_logits = test_h @ W2 + b2
test_loss = F.cross_entropy(test_logits, y_te)
print(f"Loss on validation data: {val_loss:.6f}")
print(f"Loss on testing data: {test_loss:.6f}")
```
The losses on validation and testing data are close, indicating we are not overfitting.



## Visualization of Embedding

Let's visualize our embedding matrix.

```{python}
#| label: fig-embedding
#| fig-cap: Visualization of 2D embedding matrix
plt.figure(figsize=(8,8))
plt.scatter(C[:, 0].data, C[:, 1].data, s = 200)
for i in range(C.shape[0]):
  plt.text(C[i, 0].item(), C[i, 1].item(), itos[i], ha="center", va="center", color="white")
plt.grid('minor')
plt.show()
```
As depicted in @fig-embedding, the vowels are closely grouped in the left bottom corner of the plot, while the `.` is situated far away in the top right corner.

## Word Generation

The last thing we want to do is word generation.

```{python}
g = torch.Generator().manual_seed(420)

for _ in range(20):
  out = []
  context = [0] * block_size
  while True:
    embed = C[torch.tensor([context])]
    h = torch.tanh(embed.view(1, -1) @ W1 + b1)
    logits = h @ W2 + b2
    probs = F.softmax(logits, dim=1)
    idx = torch.multinomial(probs, num_samples=1, generator=g).item()
    context = context[1:] + [idx]
    if idx == 0:
      break
    out.append(idx)
  print(''.join(itos[i] for i in out))
```

The words generated by the multilayer perceptron model make more sense than those from our last model.
Still, there are many other ways to improve model performance.
For example, train more epochs with learning rate decay, increase the batch size to make the training more stable, and add more data.