<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Gejun&#39;s Blog</title>
    <link>https://gejun.name/</link>
    <description>Recent content on Gejun&#39;s Blog</description>
    <image>
      <title>Gejun&#39;s Blog</title>
      <url>https://gejun.name</url>
      <link>https://gejun.name</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 15 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://gejun.name/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding Transformer Architecture by Building GPT</title>
      <link>https://gejun.name/natural-language-processing/understanding-attention-nlp/</link>
      <pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://gejun.name/natural-language-processing/understanding-attention-nlp/</guid>
      <description>In Part2, we constructed a straightforward MLP model to generate characters based on 32k popular names. In this lecture, Andrej guides us on gradually incorporating the transformer architecture to improve the performance of our bigram model. We will start by refactoring our previous model and then add code from the transformer architecture piece by piece to see how it helps our model.
Data Preparation Let&amp;rsquo;s first import the necessary libraries and get the data ready.</description>
    </item>
    
    <item>
      <title>Multilayer Perceptron (MLP)</title>
      <link>https://gejun.name/natural-language-processing/building-makemore-mlp/</link>
      <pubDate>Mon, 13 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://gejun.name/natural-language-processing/building-makemore-mlp/</guid>
      <description>In Part1, we learned how to build a neural network with one hidden layer to generate words. The model we built performed fairly well as we got the exact words generated based on counting. However, the bigram model suffers from the limitation that it assumes that each character only depends on its previous character. Suppose there is only one bigram starting with a particular character. In that case, the model will always generate the following character in that bigram, regardless of the context or the probability of other characters.</description>
    </item>
    
    <item>
      <title>Bigram Character-level Language Model</title>
      <link>https://gejun.name/natural-language-processing/building-makemore/</link>
      <pubDate>Sat, 04 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://gejun.name/natural-language-processing/building-makemore/</guid>
      <description>This is a series of learning notes for the excellent online course Neural Networks: Zero to Hero created by Andrej Karpathy. The official Jupyter Notebook for this lecture is here.
In this lecture, Andrej shows us two different approaches to generating characters. The first approach involves sampling characters based on a probability distribution, while the second uses a neural network built from scratch. Before we can generate characters using either approach, let&amp;rsquo;s prepare the data first.</description>
    </item>
    
    <item>
      <title>Back to Basics - Data Science</title>
      <link>https://gejun.name/datasci-simplified/back-to-basics/</link>
      <pubDate>Sat, 16 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://gejun.name/datasci-simplified/back-to-basics/</guid>
      <description>Statistics Introduction to Probability: Probability Distributions: Bayes Theorem: Central Limit Theorem: Code | Slides | Video Confidence Interval: Code | Slides | Video Hypothesis Testing: Code | Slides | Video p-value: Type I and Type II Errors: Power of a Test: t-test: ANOVA: Chi-Square Test: Linear Regression: Machine Learning Logistic Regression: Decision Trees: Random Forests: Gradient Boosting: Support Vector Machines: K-Nearest Neighbors: K-Means Clustering: Hierarchical Clustering: Principal Component Analysis: Singular Value Decomposition: Deep Learning Neural Networks: Convolutional Neural Networks: Natural Language Processing Word Embeddings: Word2Vec: GloVe: FastText: BERT: LeetCode (Grind75) Two Sum R tidyverse: ggplot2: dplyr: tidyr: purrr: readr: tibble: stringr: Python numpy: pandas: matplotlib: seaborn: scikit-learn: scipy: decorator: </description>
    </item>
    
    
    
  </channel>
</rss>
